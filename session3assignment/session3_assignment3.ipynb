{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "session3_assignment3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "aNyZv-Ec52ot",
        "xTtqsBF2G6M-",
        "PARmzH0pP9g5",
        "UpZ7pO2FXKUN",
        "u6SgpOTgg2lZ",
        "P1LMk-F83SDH",
        "xrl6d5VaPJPy"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "# **Import Libraries and modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load all required modules\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add\n",
        "from keras.layers import Convolution2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load mnist data into train and test set\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "edc94132-f84e-46b6-d9ba-9209d8574d36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "#print train datatset shape plot one image with matplotlib\n",
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])\n",
        "\n",
        "#total train dataset has 60k images each of size 28*28"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f312c891cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reshaping the train and test data \n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-e0Uh7Z66Mb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aea87718-3280-483e-8b35-2bdbad9419bc"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V2R10_v76ud",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "18c40da8-86b1-4feb-9189-f048faea43ba"
      },
      "source": [
        "#pixel values before normalising\n",
        "X_train[0]"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  3],\n",
              "        [ 18],\n",
              "        [ 18],\n",
              "        [ 18],\n",
              "        [126],\n",
              "        [136],\n",
              "        [175],\n",
              "        [ 26],\n",
              "        [166],\n",
              "        [255],\n",
              "        [247],\n",
              "        [127],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 30],\n",
              "        [ 36],\n",
              "        [ 94],\n",
              "        [154],\n",
              "        [170],\n",
              "        [253],\n",
              "        [253],\n",
              "        [253],\n",
              "        [253],\n",
              "        [253],\n",
              "        [225],\n",
              "        [172],\n",
              "        [253],\n",
              "        [242],\n",
              "        [195],\n",
              "        [ 64],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 49],\n",
              "        [238],\n",
              "        [253],\n",
              "        [253],\n",
              "        [253],\n",
              "        [253],\n",
              "        [253],\n",
              "        [253],\n",
              "        [253],\n",
              "        [253],\n",
              "        [251],\n",
              "        [ 93],\n",
              "        [ 82],\n",
              "        [ 82],\n",
              "        [ 56],\n",
              "        [ 39],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 18],\n",
              "        [219],\n",
              "        [253],\n",
              "        [253],\n",
              "        [253],\n",
              "        [253],\n",
              "        [253],\n",
              "        [198],\n",
              "        [182],\n",
              "        [247],\n",
              "        [241],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 80],\n",
              "        [156],\n",
              "        [107],\n",
              "        [253],\n",
              "        [253],\n",
              "        [205],\n",
              "        [ 11],\n",
              "        [  0],\n",
              "        [ 43],\n",
              "        [154],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 14],\n",
              "        [  1],\n",
              "        [154],\n",
              "        [253],\n",
              "        [ 90],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [139],\n",
              "        [253],\n",
              "        [190],\n",
              "        [  2],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 11],\n",
              "        [190],\n",
              "        [253],\n",
              "        [ 70],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 35],\n",
              "        [241],\n",
              "        [225],\n",
              "        [160],\n",
              "        [108],\n",
              "        [  1],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 81],\n",
              "        [240],\n",
              "        [253],\n",
              "        [253],\n",
              "        [119],\n",
              "        [ 25],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 45],\n",
              "        [186],\n",
              "        [253],\n",
              "        [253],\n",
              "        [150],\n",
              "        [ 27],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 16],\n",
              "        [ 93],\n",
              "        [252],\n",
              "        [253],\n",
              "        [187],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [249],\n",
              "        [253],\n",
              "        [249],\n",
              "        [ 64],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 46],\n",
              "        [130],\n",
              "        [183],\n",
              "        [253],\n",
              "        [253],\n",
              "        [207],\n",
              "        [  2],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 39],\n",
              "        [148],\n",
              "        [229],\n",
              "        [253],\n",
              "        [253],\n",
              "        [253],\n",
              "        [250],\n",
              "        [182],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 24],\n",
              "        [114],\n",
              "        [221],\n",
              "        [253],\n",
              "        [253],\n",
              "        [253],\n",
              "        [253],\n",
              "        [201],\n",
              "        [ 78],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 23],\n",
              "        [ 66],\n",
              "        [213],\n",
              "        [253],\n",
              "        [253],\n",
              "        [253],\n",
              "        [253],\n",
              "        [198],\n",
              "        [ 81],\n",
              "        [  2],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 18],\n",
              "        [171],\n",
              "        [219],\n",
              "        [253],\n",
              "        [253],\n",
              "        [253],\n",
              "        [253],\n",
              "        [195],\n",
              "        [ 80],\n",
              "        [  9],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 55],\n",
              "        [172],\n",
              "        [226],\n",
              "        [253],\n",
              "        [253],\n",
              "        [253],\n",
              "        [253],\n",
              "        [244],\n",
              "        [133],\n",
              "        [ 11],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [136],\n",
              "        [253],\n",
              "        [253],\n",
              "        [253],\n",
              "        [212],\n",
              "        [135],\n",
              "        [132],\n",
              "        [ 16],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#normalizing the pixel values to range within  0 and 1\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM4NBKC77t4m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58f839fb-f39a-4d5e-9b25-dcd96269f5bf"
      },
      "source": [
        "#pixel values after normalizing\n",
        "X_train[0]"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.01176471],\n",
              "        [0.07058824],\n",
              "        [0.07058824],\n",
              "        [0.07058824],\n",
              "        [0.49411765],\n",
              "        [0.53333336],\n",
              "        [0.6862745 ],\n",
              "        [0.10196079],\n",
              "        [0.6509804 ],\n",
              "        [1.        ],\n",
              "        [0.96862745],\n",
              "        [0.49803922],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.11764706],\n",
              "        [0.14117648],\n",
              "        [0.36862746],\n",
              "        [0.6039216 ],\n",
              "        [0.6666667 ],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.88235295],\n",
              "        [0.6745098 ],\n",
              "        [0.99215686],\n",
              "        [0.9490196 ],\n",
              "        [0.7647059 ],\n",
              "        [0.2509804 ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.19215687],\n",
              "        [0.93333334],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.9843137 ],\n",
              "        [0.3647059 ],\n",
              "        [0.32156864],\n",
              "        [0.32156864],\n",
              "        [0.21960784],\n",
              "        [0.15294118],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.07058824],\n",
              "        [0.85882354],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.7764706 ],\n",
              "        [0.7137255 ],\n",
              "        [0.96862745],\n",
              "        [0.94509804],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.3137255 ],\n",
              "        [0.6117647 ],\n",
              "        [0.41960785],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.8039216 ],\n",
              "        [0.04313726],\n",
              "        [0.        ],\n",
              "        [0.16862746],\n",
              "        [0.6039216 ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.05490196],\n",
              "        [0.00392157],\n",
              "        [0.6039216 ],\n",
              "        [0.99215686],\n",
              "        [0.3529412 ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.54509807],\n",
              "        [0.99215686],\n",
              "        [0.74509805],\n",
              "        [0.00784314],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.04313726],\n",
              "        [0.74509805],\n",
              "        [0.99215686],\n",
              "        [0.27450982],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.13725491],\n",
              "        [0.94509804],\n",
              "        [0.88235295],\n",
              "        [0.627451  ],\n",
              "        [0.42352942],\n",
              "        [0.00392157],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.31764707],\n",
              "        [0.9411765 ],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.46666667],\n",
              "        [0.09803922],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.1764706 ],\n",
              "        [0.7294118 ],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.5882353 ],\n",
              "        [0.10588235],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.0627451 ],\n",
              "        [0.3647059 ],\n",
              "        [0.9882353 ],\n",
              "        [0.99215686],\n",
              "        [0.73333335],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.9764706 ],\n",
              "        [0.99215686],\n",
              "        [0.9764706 ],\n",
              "        [0.2509804 ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.18039216],\n",
              "        [0.50980395],\n",
              "        [0.7176471 ],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.8117647 ],\n",
              "        [0.00784314],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.15294118],\n",
              "        [0.5803922 ],\n",
              "        [0.8980392 ],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.98039216],\n",
              "        [0.7137255 ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.09411765],\n",
              "        [0.44705883],\n",
              "        [0.8666667 ],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.7882353 ],\n",
              "        [0.30588236],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.09019608],\n",
              "        [0.25882354],\n",
              "        [0.8352941 ],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.7764706 ],\n",
              "        [0.31764707],\n",
              "        [0.00784314],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.07058824],\n",
              "        [0.67058825],\n",
              "        [0.85882354],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.7647059 ],\n",
              "        [0.3137255 ],\n",
              "        [0.03529412],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.21568628],\n",
              "        [0.6745098 ],\n",
              "        [0.8862745 ],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.95686275],\n",
              "        [0.52156866],\n",
              "        [0.04313726],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.53333336],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.99215686],\n",
              "        [0.83137256],\n",
              "        [0.5294118 ],\n",
              "        [0.5176471 ],\n",
              "        [0.0627451 ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "3e06d119-57ba-4421-caf4-c7e177bf9d98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#y labels has a range of labels from 0 -9 total 10 classes\n",
        "y_train[:10]"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "b28c9b05-e508-4e92-de6d-0917b24aa398",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "Y_train[:10]\n"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKqT73Q9JJB",
        "colab_type": "code",
        "outputId": "6616fd6d-6c60-4003-a531-e6a2e71aac3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#given model architecture \n",
        "\n",
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        "\n",
        " \n",
        "model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(Convolution2D(10, 1, activation='relu'))\n",
        "model.add(Convolution2D(10, 26))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzdAYg1k9K7Z",
        "colab_type": "code",
        "outputId": "c360a3cc-0d00-4ec2-dc94-209c1ec7cd9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "#provides model architecture information- input output shape,total parameters.\n",
        "model.summary()"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_228 (Conv2D)          (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_229 (Conv2D)          (None, 26, 26, 10)        330       \n",
            "_________________________________________________________________\n",
            "conv2d_230 (Conv2D)          (None, 1, 1, 10)          67610     \n",
            "_________________________________________________________________\n",
            "flatten_40 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_40 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 68,260\n",
            "Trainable params: 68,260\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp6SuGrL9M3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#compile model which is defined in previous step.\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xWoKhPY9Of5",
        "colab_type": "code",
        "outputId": "36c3cbbf-33a7-414b-de57-d833b8984916",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "\n",
        "#train or fit model to our train dataset.\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=10, verbose=1)"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 19s 321us/step - loss: 0.2019 - acc: 0.9421\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 15s 254us/step - loss: 0.0745 - acc: 0.9776\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 15s 254us/step - loss: 0.0526 - acc: 0.9836\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 15s 257us/step - loss: 0.0408 - acc: 0.9867\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 15s 253us/step - loss: 0.0332 - acc: 0.9897\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 15s 252us/step - loss: 0.0259 - acc: 0.9918\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 15s 252us/step - loss: 0.0207 - acc: 0.9929\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 15s 251us/step - loss: 0.0160 - acc: 0.9947\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 15s 252us/step - loss: 0.0146 - acc: 0.9955\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 15s 252us/step - loss: 0.0113 - acc: 0.9963\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f312f5376d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtsH-lLk-eLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#calculate score or evaluate model for test dataset- which is not included in training.\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkX8JMv79q9r",
        "colab_type": "code",
        "outputId": "45bbbd0a-4809-4e9c-fefc-6fc9c873fa2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.07671759347507978, 0.9836]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCWoJkwE9suh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym7iCFBm9uBs",
        "colab_type": "code",
        "outputId": "60c7acfb-cbc7-4f24-f10b-26fdf6721424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "#compare predicted value of model and actual value of test data.\n",
        "print(y_pred[:9])\n",
        "print(\"\\n\")\n",
        "print(y_test[:9])"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.05105472e-20 1.26059141e-18 8.71992975e-15 7.74211060e-08\n",
            "  7.88577370e-21 2.02974929e-15 2.82287103e-29 9.99999881e-01\n",
            "  9.58769435e-12 9.35670378e-12]\n",
            " [1.43926425e-11 2.65163239e-12 1.00000000e+00 1.19437432e-10\n",
            "  1.93505628e-24 6.44575198e-17 1.33063296e-10 2.29968650e-27\n",
            "  2.81707464e-13 1.58789894e-19]\n",
            " [1.02118465e-08 9.99933243e-01 1.24122522e-07 1.24204994e-10\n",
            "  2.23379448e-05 1.44993934e-10 4.60213534e-10 6.67897268e-07\n",
            "  4.36133269e-05 2.15696216e-09]\n",
            " [1.00000000e+00 4.80402660e-22 7.21920135e-10 5.88246250e-17\n",
            "  6.48965907e-20 3.26088237e-14 4.94658654e-11 1.82254004e-15\n",
            "  9.66074483e-14 7.38513966e-15]\n",
            " [2.35781413e-14 2.88176015e-18 2.29288617e-14 1.97773243e-12\n",
            "  9.99999762e-01 2.40913534e-16 7.19915748e-16 2.10156701e-11\n",
            "  5.45482060e-09 2.32624060e-07]\n",
            " [2.32289014e-11 9.99987125e-01 3.58060381e-09 7.96920342e-12\n",
            "  4.16633429e-06 1.02394534e-11 3.08109831e-12 6.45764158e-06\n",
            "  2.22429412e-06 3.31421557e-09]\n",
            " [1.62187863e-21 1.31116071e-12 9.29008086e-12 1.02128340e-15\n",
            "  9.99869347e-01 5.02568681e-11 1.30354003e-20 2.77723533e-09\n",
            "  1.29794120e-04 7.85164104e-07]\n",
            " [2.59206157e-23 5.09292296e-11 9.04367564e-13 4.19685779e-11\n",
            "  2.58181353e-06 7.71961538e-07 2.15193314e-20 2.64971183e-12\n",
            "  2.12713189e-07 9.99996424e-01]\n",
            " [8.21852308e-10 9.72276412e-21 2.58354552e-16 5.42240688e-12\n",
            "  6.43940493e-15 9.71696794e-01 2.82985587e-02 1.06635100e-20\n",
            "  4.64262484e-06 4.08529703e-11]]\n",
            "\n",
            "\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT--y98_dr2T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "80df4ff2-d805-4d9c-be81-b0678111d245"
      },
      "source": [
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "\n",
        "layer_dict"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation_40': <keras.layers.core.Activation at 0x7f312c7ccf60>,\n",
              " 'conv2d_228': <keras.layers.convolutional.Conv2D at 0x7f3137e32f28>,\n",
              " 'conv2d_229': <keras.layers.convolutional.Conv2D at 0x7f312c81cc88>,\n",
              " 'conv2d_230': <keras.layers.convolutional.Conv2D at 0x7f312c7b98d0>,\n",
              " 'flatten_40': <keras.layers.core.Flatten at 0x7f312c7d0208>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GY4Upv4dsUR",
        "colab_type": "code",
        "outputId": "25deac10-e326-4eac-d03c-eda277463dbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        }
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "%matplotlib inline\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    #x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n",
        "                      layer_name = 'conv2d_228'):\n",
        "    layer_output = layer_dict[layer_name].output\n",
        "    img_ascs = list()\n",
        "    for filter_index in range(layer_output.shape[3]):\n",
        "        # build a loss function that maximizes the activation\n",
        "        # of the nth filter of the layer considered\n",
        "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "        # compute the gradient of the input picture wrt this loss\n",
        "        grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "        # normalization trick: we normalize the gradient\n",
        "        grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "\n",
        "        # this function returns the loss and grads given the input picture\n",
        "        iterate = K.function([model.input], [loss, grads])\n",
        "\n",
        "        # step size for gradient ascent\n",
        "        step = 5.\n",
        "\n",
        "        img_asc = np.array(img)\n",
        "        # run gradient ascent for 20 steps\n",
        "        for i in range(20):\n",
        "            loss_value, grads_value = iterate([img_asc])\n",
        "            img_asc += grads_value * step\n",
        "\n",
        "        img_asc = img_asc[0]\n",
        "        img_ascs.append(deprocess_image(img_asc).reshape((28, 28)))\n",
        "        \n",
        "    if layer_output.shape[3] >= 35:\n",
        "        plot_x, plot_y = 6, 6\n",
        "    elif layer_output.shape[3] >= 23:\n",
        "        plot_x, plot_y = 4, 6\n",
        "    elif layer_output.shape[3] >= 11:\n",
        "        plot_x, plot_y = 2, 6\n",
        "    else:\n",
        "        plot_x, plot_y = 1, 2\n",
        "    fig, ax = plt.subplots(plot_x, plot_y, figsize = (12, 12))\n",
        "    ax[0, 0].imshow(img.reshape((28, 28)), cmap = 'gray')\n",
        "    ax[0, 0].set_title('Input image')\n",
        "    fig.suptitle('Input image and %s filters' % (layer_name,))\n",
        "    fig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\n",
        "    for (x, y) in [(i, j) for i in range(plot_x) for j in range(plot_y)]:\n",
        "        if x == 0 and y == 0:\n",
        "            continue\n",
        "        ax[x, y].imshow(img_ascs[x * plot_y + y - 1], cmap = 'gray')\n",
        "        ax[x, y].set_title('filter %d' % (x * plot_y + y - 1))\n",
        "\n",
        "vis_img_in_filter()"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwoAAAM0CAYAAAAbSNX8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xm8FNWd///3R0GQHRQRUEFR3HdN\n3BiNuyRmm29MTDRqFjNZJvtmfibDJJloMlm+k8lqopKvJm6JJhpDjHvcxQV3RSAiyCYIyCYBOb8/\nqrr81KnbRd97u2/3vff1fDx4cPqe6upT3Z+u7tPnc05ZCEEAAAAA4G3R7AYAAAAAaD10FAAAAAAU\n0FEAAAAAUEBHAQAAAEABHQUAAAAABXQUAAAAABTQUQCAFmdmT5nZMc1uR1cys2Bmuza7HR1hZmeb\n2d3NbodnZu8ys3lmttrMDvQxZWZTzOzyJjcRQAuiowAAbTCzF8zs+C54nM1+SQsh7B1CuKPRbUHb\nzOwwM7vZzF4xs5fN7BozG93JfW5nZleY2QIzW2lm95jZm139W83sbjNbYWaLzOzXZjbY1Y8ws6vM\nbJmZLTWz35rZkJKH/L6kT4UQBoUQHq0WU2Y2Pu2k9enM8QHoGegoAABQbrikiySNlzRO0ipJl3Zy\nn4MkTZd0sKQRkn4j6UYzG5TWD5X0bUljJO0paayk/3b3/3barp0lTZA0StKUkscbJ+mpTrZ5s+hg\nAD0LHQUA2IxKKomZfd/MlpvZP8zsFFd/h5ldYGYPmtmrZvYnMxuR1h1jZvOj/b1gZseb2cmSvibp\nvWlKyGNVHj8b3UhHIK4xs8vNbJWZPWFmE83sPDNbkqaXnOjue46ZPZNuO8fMPhbt+8tmtjD9Zfsj\nPuXHzPqlx/yimS02s1+Y2dZV2jjBzG6LfuEeFh3DF83s8fQX9KvMrL+r/5Jrx4c283qMMLNL022X\nm9kfXd1HzWxW+uv/9WY2xtUFM/s3M3s+/aX+p5bol97ex2070szWmdl2IYRpIYRrQgivhhDWSvqJ\npCPdttukj/WqmT2o5It7qRDCnBDCD0MIC0MIr4cQLpK0laTd0/rfhRD+GkJYG0JYLulX/jGVdBD+\nmLZppaTrJO3dxnPVz8xWS9pS0mNmNtu9Hm2NmP09/X9FGpOHp9t/KI2j5WZ2k5mNi57XT5rZ85Ke\nT5/TH6Xx+Goao/u08VgAWhwdBQCozZslPSdpW0nfk3SxmZmr/6CkD0kaLWmjpB9vbochhL9K+o6k\nq9KUkP1rbMupki5T8ovyo5JuUnI+Hyvpm5J+6bZdIultkoZIOkfSj8zsIElKOyqfl3S8pF0lHRM9\nzoWSJko6IK0fK+kbVdpkki7QG7+A76jiL9ynSTpZyZfc/SSd7drxRUknSNotbU+ZyyQNUPLFeDtJ\nP0r3c2zahtOUvA5zJV0Z3fdtkg5NH/80SSeFENZLulbS6VFb7wwhLGnj8f9F+V/nfyrptfQxP5T+\naxczO0BJR2FWlU3aesy3mdlwMxsu6V8lTYvvFEJYH0KojFLsH0LYXCfmX9L/h6UxeZ+ZvUNJh/bd\nkkZKukvSFdH93qnkPbKXpBPT/UxUMjJymqRlm3lcAC2IjgIA1GZuCOFXIYTXlaSJjFaS7lFxWQjh\nyRDCGklfl3SamW3ZoLbcFUK4KYSwUdI1Sr68XRhC2KDki/H4yq/5IYQbQwizQ+JOSX+TNCndz2mS\nLg0hPJX+Uj6l8gBpJ+hcSZ8LIbwSQlilpFPzvrYaFEKYFUK4Of1i+rKkH0o6OtrsxyGEBSGEVyTd\noKQD4ttRef6mqApL5gacIunfQgjLQwgb0uOSpA9IuiSE8Ej65f88SYeb2Xi3iwtDCCtCCC9Kut21\n4XfRsb0//Vv8+Psp6Sx9Kb29pZIv6d8IIawJITypJD5qZsncgssk/Wc6OhDXnyDpLOU7aY8o6Vgs\nS/+9Luln7Xncdvg3SReEEJ5JY+47kg7wowpp/SshhHWSNkgaLGkPSZbeb2GD2gaggegoAEBtFlUK\n6ZdqKckzr5jnynMl9VUy+tAIi115naSlaQemcjtrm5mdYmb3p6k4KyRNdu0aE7Xbl0cq+dX+4TQt\nZ4Wkv6Z/LzCzUWZ2pZm9ZGavSrpcxeNf5Mpr9cbzF7djbluPkdpR0itpOk5sjL9vCGG1ki/RY2to\nw+2SBpjZm9OOxQFK0nkyaUrWNEmfCSHclf55pKQ+7Wh/TprKdYOk+0MIF7RRf5iSDsv/CSHMdFVX\nS5qp5Av5EEmzlTznjTBO0v+4OHhFyQiSf16z4w8h3KYkPeunkpaY2UVWPtEaQIuiowAA9bGjK++k\n5FfVpZLWKPnCLSn7Bdp/2Q6NapCZ9ZP0ByUr3owKIQyT9BclX/IkaaGkHdxd/DEsVdLp2DuEMCz9\nN9SlscS+o+RY9g0hDJF0hnuczVmo4vNXzTxJI/z8B2eBki+1kiQzGyhpG0kvba4BaUfraiXpR6dL\n+nM6ilLZ1zhJt0j6VgjhMnfXl5WkmtXa/kz6+vxR0nxJH2uj/kBJ10v6UAjh1qj6AEm/TEcxVkv6\nhZJOYGe1FY/zJH3MxcGwEMLWIYR7q90vhPDjEMLBSlKRJiodgQHQvdBRAID6OMPM9jKzAUrmCfw+\n/fI5U1J/S5a77CvpfEn93P0WK0kVasT5eKv0sV6WtNGSCdgnuvqrJZ1jZnum7f56pSKEsEnJBNof\nmdl2kmRmY83spCqPNVjSakkrzWys2vfF8GpJZ7vn7z+qbZimsEyT9LM0P7+vmVXy6q9Ij+eA9Ev4\ndyQ9EEJ4ocZ2/E7Se5WkMGVpR+nx3CbpJyGEX0TteV3J/IYpZjbAzPZSkiZUKo2F3yvpjJ2VPt++\nfh8lIzj/HkK4oY1dTJf0ETPbOh2VOFfS4zUeZ5mXJW2StIv72y8knWdme6dtG2pm76m2AzM7NB2Z\n6auko/xauk8A3QwdBQCoj8skTVWS2tJf0qclKc05/4SkXyv5ZXuNkl+QK65J/19mZo/Us0HpL+Kf\nVvJFfLmSvPvrXf00JZOub1cyifb+tGp9+v9XKn9P04luUboqTxv+U9JBklZKulHJl+da2zlN0v9V\n8mV8Vvp/mTOVjNg8q2Sy9mfT/dyipLPzByWjFBNUZU5FlXY8oOT1GaP8xOCPKPniPCVdCWh1upJQ\nxaeUpDAtUhIDtSydeoSSidUn6o0VhlabWWX+yBeUjDxd7Or8ZOYPKVmudb6SuNpFNXRQNidNq/sv\nSfekqUaHhRCuk/RdSVemcfCkknki1QxR0slcriQNa5nyS7sC6CYshIaNegNAr2Bmd0i6PITw62a3\npTPMbE8lXwL7pZNWAQC9GCMKANCLmdm70rX2hyv51fgGOgkAAImOAgD0dh9Tkr4zW8kSmx9vbnN6\nDjOb5FOVqqQtAUDLIvUIAAAAQAEjCgAAAAAK6CgAAAAAKKCjAAAAAKCAjgIAAACAAjoKAAAAAAro\nKAAAAAAooKMAAAAAoICOAgAAAIACOgoAAAAACugoAAAAACigowAAAACggI4CAAAAgAI6CgAAAAAK\n6CgAAAAAKKCjAAAAAKCAjgIAAACAAjoKAAAAAAroKAAAAAAooKMAAAAAoICOAgAAAIACOgoAAAAA\nCugoAAAAACigowAAAACggI4CAAAAgAI6CgAAAAAK6CgAAAAAKKCjAAAAAKCAjgIAAACAAjoKAAAA\nAAroKAAAAAAooKMAAAAAoICOAgAAAIACOgoAAAAACugoAAAAACigowAAAACggI4CAAAAgAI6CgAA\nAAAK6CgAAAAAKKCjAAAAAKCAjgIAAACAAjoKAAAAAAroKAAAAAAooKMAAAAAoICOAgAAAIACOgoA\nAAAACugoAAAAACigowAAAACggI4CAAAAgAI6CgAAAAAK6CgAAAAAKKCjAAAAAKCAjgIAAACAAjoK\nAAAAAAroKAAAAAAooKMAAAAAoICOAgAAAIACOgoAAAAACugoAAAAACigowAAAACggI4CAAAAgAI6\nCgAAAAAK6CgAAAAAKKCjAAAAAKCAjgIAAACAAjoKAAAAAAroKAAAAAAooKMAAAAAoICOAgAAAIAC\nOgoAAAAACugoAAAAACigowAAAACggI4CAAAAgAI6CgAAAAAK6CgAAAAAKKCjAAAAAKCAjgIAAACA\nAjoKAAAAAAroKAAAAAAooKMAAAAAoICOAgAAAIACOgoAAAAACugoAAAAACigowAAAACggI4CAAAA\ngAI6CjUys6fM7JhmtwPtZ2a7m9kMM1tlZp82s1+Y2dfTumPMbH6z24jGIgZADIAYADHQfn2a3YAy\nZvaCpI+EEG5p8ONMkbRrCOGMatuEEPZuZBvQUF+WdHsI4YDNbdiImDOzEZIulnSipKWSzgsh/K5e\n+0dNmh0Dn5J0tqR9JV0RQji7XvtGzZoWA2bWT9LPJB0vaYSk2UrOA9PqsX/UrNnngcslHSdpoKRF\nkr4XQvh1vfaPmjQ1Bty+d5P0hKTfl333bAWMKKA3GCfpqUY/iCXaek/9VNI/JY2S9AFJPzczOp5d\nq9kxsEDStyVd0ug2oKpmxkAfSfMkHS1pqKTzJV1tZuMb3R7kNPs8cIGk8SGEIZLeLunbZnZwo9uD\nnGbHQMVPJU1vdDvqodt0FMzsbDO728y+b2bLzewfZnaKq7/DzC4wswfN7FUz+1P6S26bw0lm9oKZ\nHW9mJ0v6mqT3mtlqM3usyuO/YGbHp+UpZnaNmV2eDl89YWYTzew8M1tiZvPM7ER333PM7Jl02zlm\n9rFo3182s4VmtsDMPmJmwcx2Tev6pcf8opktTofJtq7X89rTmdltkt4i6Sfp6zvRzKaa2bfb2PYy\nSTtJuiHd9svp3w8zs3vNbIWZPWYuBS2Nu/8ys3skrZW0S7TPgZL+VdLXQwirQwh3S7pe0pkNOmRE\nmh0DkhRCuDaE8EdJyxpzlCjT7BgIIawJIUwJIbwQQtgUQvizpH9I4ktiF2l2DEhSCOGpEML6ys30\n34R6Hyva1goxkG73PkkrJN1a94NsgG7TUUi9WdJzkraV9D1JF5uZufoPSvqQpNGSNkr68eZ2GEL4\nq6TvSLoqhDAohLB/jW05VdJlkoZLelTSTUqez7GSvinpl27bJZLeJmmIpHMk/cjMDpKktKPyeSVD\n0rtKOiZ6nAslTZR0QFo/VtI3amxjrxdCOFbSXZI+lb6+M0u2PVPSi5JOTbf9npmNlXSjkl+DR0j6\noqQ/mNlId9czJZ0rabCkudFuJ0raGD3uY5IYUegiLRADaLJWiwEzG6Xk3NDwXzaRaJUYMLOfmdla\nSc9KWijpL50/OtSiFWLAzIYo+Y74+TodVsN1t47C3BDCr0IIr0v6jZIOwShXf1kI4ckQwhpJX5d0\nmplt2aC23BVCuCmEsFHSNZJGSrowhLBB0pWSxpvZMEkKIdwYQpgdEndK+pukSel+TpN0afpLw1pJ\nUyoPkHaCzpX0uRDCKyGEVUo6Ne9r0DGh6AxJfwkh/CX9JfBmSQ9Jmuy2mZq+fhvT198bJOnV6G8r\nlZxE0D10NgbQ/dUtBsysr6TfSvpNCOHZxjYbdVSXGAghfELJ+X+SpGslrW9rO7SkesTAtyRdHELo\nNpOmu1tHYVGlkH6plpIvYhXzXHmupL5KRh8aYbErr5O0NO3AVG5nbTOzU8zsfjN7xcxWKAmqSrvG\nRO325ZGSBkh6OB3mWiHpr+nf0TXGSXpP5flPX4OjlHRSK+a1fVdJ0molI0neEEmr6ttMNFBnYwDd\nX11iwJKc5cuUzFn6VENaikap23kghPB6moa6g6SP17+paJBOxYCZHaAke+RHjW1mfbX0qkcdsKMr\n7yRpg5JVZtYo+cItSUpHGfyX7dCoBlmy2sUflKRF/SmEsMHM/iipkjK1UMnJosIfw1IlnY69Qwgv\nNaqNyIljYZ6SkaqPtuM+3kxJfcxstxDC8+nf9hcpB62s3jGA7qfuMZCOEF+sZBR8MiNPLa8rzgN9\nxByFVlbvGDhG0nhJL6ZZ84MkbWlme4UQDupEOxuqu40obM4ZZraXmQ1QkgP2+/RX/pmS+pvZW9Nh\n3/Ml9XP3W6wkVagRz8dW6WO9LGmjJROwT3T1V0s6x8z2TNv99UpFCGGTpF8pmdOwnSSZ2VgzO6kB\n7URisfITkC6XdKqZnWRmW5pZf0smx+9Q5f45aRrctZK+aWYDzexISe9Q8qsiWlNdY0CSzKyPmfWX\ntKWSD4b+ZtbTfqjpSeoeA5J+LmlPJTnP6za3MZqurjFgZtuZ2fvMbFB6/5Mkna5uMqG1l6r3eeAi\nJR3DA9J/v1Ay56Glv9P1tI7CZZKmKklR6i/p05IUQlgp6ROSfi3pJSUjDD4/7Jr0/2Vm9kg9G5TO\nK/i0kg7BcknvV7LqTaV+mpJJ17dLmiXp/rSqkrf4lcrfzexVSbdI2r2ebUTOBZLOT4cVvxhCmKfk\ni/3XlHT25kn6ktr33vmEpK2VTGq/QtLHQwiMKLSuRsTA+UpGB7+qJM91Xfo3tKa6xoCZjZP0MSVf\nDhalq6isNrMPNKb5qIN6nweCkjSj+Uq+C3xf0mdDCNeX3gvNVNcYCCGsDSEsqvxTkpr8Wgjh5Qa1\nvy4shJ4xYm5md0i6PHTzi5eY2Z6SnpTUL50oDQAAAHS5njai0C2Z2bssuV7CcEnflXQDnQQAAAA0\nEx2F1vAxJWkpsyW9LlZBAAAAQJN1qqNgZieb2XNmNsvMvlqvRnVECOGY7pp2FEI4OYQwNIQwIoTw\nrhDCwma3qVatFANoDmIAEnEAYgDEQE/U4TkK6RKjMyWdoGRyznRJp4cQnq5f89DKiAEQA5CIAxAD\nIAZ6qs4sz/cmSbNCCHMkycyuVDIbvGpAmFnPmDndTYUQbPNbtUu7Y2DgwIFh2LBh7X6g1157rWpd\nuh6xJKl///5Vt6vHxP2NG9+YOhLvz7ejb9++Nbej2rFtvfXWNe+jFitWrNCaNWuaHgMDBgwIQ4cO\nlSRtscUWcV3VB/LPb3z8a9euzcp+n2Xx4PcX73PDhvwS91tu+cYF3uM2e5s2bappu7K2+Hb444r3\n2ZFjW7JkiVauXFnvGJDaGQcDBgzo0HkAnbdixQqtXbu2JWKgch5Ys2ZNri6Oe3TMwIEDq95euHDh\n0hBCvS/e2q4YGDx4cNh22+Tas/H5rF+/fm3dBe30+uuv52777xuzZs2qKQY601EYq/wV6OZLenO8\nkZmdK+ncTjwOWle7Y2Do0KH62Mc+1u4Hev7557NyHPhbbbVVVt5zzz2zsv/SJhW//HXE0qVLq+7f\nf4kbPXq0qonb8eyzz7a53f7775+77R+vI8fyy1/+st33qUG7Y2DIkCH68Ic/LCn/2knSIYccUvWB\nfOcrPv5HHnljVWPfwfLxEPNf/iXpn//8Z1b2r7OU/4AdNOiNi8HHsej3UdbRi1U7Nn9c8T7Lji3u\nqFb2+ZnPfKbmNrXTZuOgHucBdF6DzgNSO2PAnwfuv/9+v5lmzJjRqDb2Koceemju9mGHHZaVp0yZ\nMrcBD9muGNhmm200ZcoUSdLEiRNzO4pvo2NWrFiRuz1z5sysPHny5JpioOEX/AkhXKTkIhOMKPRS\nPgbGjh3boRi49957s3L8S+348eOz8imnnJKV41/q587t/Hlxzpw5Vev8iW348OG5Ov/l77777svV\n3XnnnVl58ODBWfkd73hHbjt/PPU4lq7kY2Do0KHhoYcekiQtXrw4t138vHm+E+GfJ0n61a9+lZVX\nrlyZleOOiDdmzJiqdQ888EDuto85/+HrOwaSNHv27KwcdxQqv5y1pdqx+eOSaj+2SZMm5W6PGjVK\nUvmITaP5GBgzZgyfBb2Qj4GRI0eGSoc8/tGlbEQWtRs7dmzudnt+vGgUHwPjx48PlVH6+IebbbbZ\npsvb1hPFIzNLlixp9z46M5n5JUk7uts7pH9D70EMgBiARByAGAAx0CN1pqMwXdJuZrazmW0l6X1y\nVxxGr0AMgBiARByAGAAx0CN1OPUohLDRzD4l6SZJW0q6JITwVN1ahpZHDIAYgEQcgBgAMdBTdWqO\nQgjhL5L+Uqe2oBtqVAzEE3yXL1+elceNG5er+8hHPpKVTzzxxKz83HPP5barR17/6tWrs3JlxY6K\nk08+OSvHObYvvPBCVvZzEiRp3bp1WfnAAw/Myn6+hZSfH9FKcxTaGwOvvfZaNjndPy+StGDBgqr3\n8zms8Yo5Pl78Pt785sK86kw8Z8BPUvYTviTp1Vdfzcr77LNPVo7nCfhJ9/FKLgcffHDVtlQ7tvh9\nUOuxxSuIVOZ+9OnTuGlpfB6gozEwcmR+4ZWyFb1Qu4MOOih3u2wOWL20JwbWrl2rhx9+WJL0t7/9\nLVdXNocMtYsnhXdk7gdXZgYAAABQQEcBAAAAQEHDl0cFOiJOz/HiJd/8UpDr16/PymVpLO3h19T3\ny5yWXSzslVdeydVNmzYtKy9btixXt+uuu2bls846KyvH6Uvxmv29jU/zKUtNqPV5ilPHKkuISsXl\nGv2ypEOGDGnzPlI+hSiOgTLVji1eMtCnQFWG7CvKUpvQGl566Y0FYGbNmpWre/rp2i5eW3bxP59a\ntsMOO+TqKimb/hwJoMhfWDW+DsH8+fPbrIuXY/efIfEy2n7/8eeVv+3LcbqtTynafffd2ziK+mFE\nAQAAAEABHQUAAAAABXQUAAAAABQwRwEt48knn8zKtebxS/kl33yer99fZ8S5xBVxXqDPD45z3G+6\n6aasHOcV+zkXxx57bFaOcx59fnNv5J+3OHff53zG+aDVxHNA/KXu4zxuPzfFv7Zbb711bju/pN/s\n2bNraoeUjx0/RyGed+DnPTz66KO5uu40R2Hjxo1atGiRpOJSv/497J93qXz+SbXc/XhJWH+7rC6e\nf+Lfp7vsskvVdniVY6x48MEHs7JfSlcqxnQ1Pm7jc4mf6zJ48OBcXSWu4vugNcVLL3v+HOTPfd3N\n66+/rlWrVkmSHn/88Vydf6/E5+NqefxS/n3k5w365a/jOl+W8uf1svelf+7jc5Vvc/yZ5G/Hx+br\n/LHF34kqz1tbbfTfl+qBMwYAAACAAjoKAAAAAApIPWqy4447Liv/9re/zcpHH310brv4KsM9UVl6\nzuTJk7OyT8+R8ldLbkR6jr8qtF8a8/jjj89t54eD/XKaUn5Zy3hY0F9ZuloalSTNmDGjPc3ucXxM\nxGlD/rmvdXnUeLjWp534NCQpPxx81113ZeU4PWW77bZrs02bUy0dZsKECbnt7r777qy8ePHimvff\najZs2KCXX35ZUv79JeWH1OPXsuw5rZaSE6dw+HSBstSj+MraPjXCp0WEEHLb+TbPmzcvV+evvP72\nt789V7fbbrtl5R133DErx2lOe+65Z1aO0x387crzW1FJhbvtttvUU3T39JzDDz88d9tfbd3HSvw+\nmD59elZ+4IEHcnWPPPJIPZvYUGaWvefi97ZferTW9JyYj484vcjXxXE0fvz4rHzSSSfl6vxy7D6d\nKf7M8Pssq4s/h/xt/xxcfvnlue2mTp2alf3ngiQ988wzWflNb3pTrq4jKaqMKAAAAAAooKMAAAAA\noICOAgAAAICCbjFH4V/+5V+y8jbbbJOru+6667q6OXV16KGHZmWfd9gbleXxn3feeVnZLx8pSXfc\ncUdWbkQev1+m1M9R8PMJJGnJkiVZ+cYbb8zV+ZzpOF/Rz3XwuZhxfnNvV7asZUfEua1+SbyTTz45\nV7d06dKs7PPT3/Wud1XdR61zJXqjIUOGZHE/bty4XN0ee+yRlePn0N/2cxmk/Lwg/97xS8pK+bzf\nMvHypQsWLGhzH3H+u8+19jnnUn7uQVy38847Z+V4Hpbnz0dxbrWvi5+fSjvjORXdTVneuY8PP1+j\nVecozJkzJ3fbzLKyb/N+++2X287HYryMdncycODALIf+tNNOy9XttddeWTmOc/86r127Nle3bt26\nrFy2RKl/f8TnBF8Xf177fZYto+rvF7ffx21c5/fjj/Ooo47KbeeXbb/vvvtydX7+mv9eIuXndNaK\nEQUAAAAABXQUAAAAABR0i9SjY445Jiv7JeSk7pd6FC/76Yeb/RC8H4KENHTo0KwcpwQ0OmXLpxIM\nHDgwK5ddsdEv9SrlhyF32mmnXJ0/Nj8U3dtT0Rrt2Wefzd0eMWJEVj7hhBNydVdddVVW9mlIZTEQ\nL+nXEfFwuR+m9ssndjd9+vTRyJEjJUnDhg3L1cVD/dX496KkbH9SMZ3J88P58ZKMtV7x1YuX6vXb\nlcWHj7f48XwqwcKFC3PbteeK322pR1y2ijglx6cb+ZSUxx57rMva1B7x0sCeX+Y0XvLUp7bGaVX+\nfdDqQghZ3MfvFX9eKFsGN74Cea1pZtVSlCTp4Ycfzso33HBDru6hhx7Kyv78EZ+rfTvKUijjOp9e\n//73vz8rn3HGGbntLrjggqrt9+lFcV18PqkFIwoAAAAACugoAAAAACjoFqlHH/zgB7NyPLu7uxk9\nenTu9kc/+tGs7K+8F6dF9ERPPvlk7nbZykBxylZX8kODEydOzMpxm/wQn1/BScqv4nTOOedUfazu\nvIJFd+NXDpHyQ8djxozJ1fkhZv8adTQu46F0/9iPPvpoVvbD3FJ+NZ/TTz+9Q4/dClatWqVbbrlF\nUnHlF39F8jgdwa8yMnbs2FydT7nwq8nFqUF+NZX4Su5+VaL4vejTXn274v2XxYRPjakcf4W/kvLc\nuXOzcpxa0duUXb3YrxgnSZdeemlWfuKJJxrbsDqIY8WnNfqr6/pUGCkfRz5mu5u1a9dmx+avNCzV\nfsXlshRBfx73V1veHL+PbbfdNld35JFH1rSPWle9i1Ol/Ip+2223XZt/j+vitCeftuVTrNq6XQtG\nFAAAAAAU0FEAAAAAUEBHAQAAAEBBt5ij0Mz89Hr79a9/XbUuXvazp2vPEqLN5HMNd9hhh6wcXxHS\nH08cs/7YTjrppFwdV2NuDT4vtWw5Ph8PZctf+jzi2CGHHJK77fOMr7jiiqwcnxN8fmmcO9udbNiw\nIcvJj6+cXDZPx9fFubZ+CUw/zyO+QrHP7Y2Xp/TzHOIrwPvn28+PiJej9PERz4Px8zHi9u+9995Z\neffdd8/K8Xmmt/HzNSTpuecp2KEAAAAgAElEQVSey8oXX3xxrm7GjBlZ2c8daaXPEx8vfilMKf85\n4cvDhw/Pbedjc9SoUfVuYpcxs+wcGuf0+/dtXOfz+uP8fP9+8fOdypZhjucZ+fPzqaeemqvzc2TK\nrhLu6+LPibLPl1tvvTUrX3TRRVn5W9/6Vm47v894TqefyxVfAf6AAw6o+tjV9Jxv4AAAAADqZrMd\nBTO7xMyWmNmT7m8jzOxmM3s+/X942T7QvREDkIgDEAMgBkAM9Da1pB5NlfQTSf/P/e2rkm4NIVxo\nZl9Nb3+lXo2Kr7bYnYfWYv4qvLGbb765C1vSLlPVgBgoW0L04IMP7mBT6y+EkJX9kF6cOvC73/0u\nK8dDgX7oe8iQIbm6WbNmZeV4GbwWM1VdeC6odQnRevFDuXHqmH/sstSjslQkL77CvI8rv0RiC5qq\nOsTA1ltvnS1Tuu++++bqypYV9Cla8ZWN99lnn6zsU7bi9A6/LPMuu+ySq/OpCnHq0YoVK7KyTyuL\nU8x8ulF8ReAXXnghKx9xxBG5Ot9mn17jl02VknSNinjJRB9/8edmZanIOFWjA6aqC88D8fvBf27E\nqZo+Na1VU/N8G+MYPuigg7Lyueeem5XjzwWfYhVftbmLTFUdYqB///7ac889JUnHHHNMrq7yd6m4\nRLA/R8ycOTNX9/vf/z4r33333Vk5vkKxP5dMmDChaht9KpBU+xL9/nwRL3fvUxDjlCX/HvbpV35/\n8Xbx547//hGnP7ZnmdiKzY4ohBD+LumV6M/vkPSbtPwbSe9s9yOj2yAGIBEHIAZADIAY6G06Okdh\nVAhhYVpeJKnqT/5mdq6ZPWRmD1XbBt1Sh2JgzZo1XdM6dJWa4sDHgL94GXqEdsdAb5+g2wO1Owa4\nuGSP0+4Y4PtA99DpycwhycsIJfUXhRAOCSEcUm0bdG/tiYGBAwd2YcvQlcriwMdAT1rFDHm1xkA8\n3I6eo9YYiFO70HPUGgN8H+geOro86mIzGx1CWGhmoyUtqWejJk+enLvt83e7I58ruvPOO1fd7qWX\nXuqK5tRLQ2Mgnqfif4UuW3ayEfzyjT7neNy4cbntfA5knAdY65yLbvhre8PioNYlROulLOfTK8uh\n9+JccD/nIq5rxJyLLlTXGKjk0kvFOQRvectbsnJ8HvCjFP4LSPx6bb/99lXr/GPHy6r6fOcHHnig\nzb9L+TkFb3rTm3J1/rMt/kXdx8SLL76YleOOtZ+XEM9R8LfjL+KVJSD9HIc6qmsM+PdKfB44/PDD\ns7JfRlaS7r///qzs8/rjuSLN5D8n4tx7HxO+vGjRotx2LToi1+4YCCFUPZ/65yaeZ+Tr4s9a/57z\n+46fM18Xz4d75plnsvITTzyRq1uy5I3DKpuH4Occxcfo7xcf25lnnpmVzz777Krb+ThavXp1rq5s\nadl4rkMtOvrT3vWSzkrLZ0n6Uwf3g+6LGIBEHIAYADEAYqDHqmV51Csk3SdpdzObb2YflnShpBPM\n7HlJx6e30UMRA5CIAxADIAZADPQ2m009CiGcXqXquDq3JRMPJ3pPPfVUox62Yb7//e9n5XjJOr+0\nVzzU3SqaEQPHH3987natVy/2Sx/G4mH6WvlhvXvvvTcr+yFCKZ8aEw/v+1SqOL2ou0zq6+o46Ool\nRH2KR5x65K8E6uvi186nEMUT9XwKhY8VKZ9K5VNQ4iHxAw88sPoBdIF6xUDfvn2zNJ84vejoo4/O\nynHajU/58UuNStLs2bOzsk/j9MsPS/klUNtj8eLFWdlfyTVO8fHLnsbpiUcddVRWHj16dK5ujz32\nyMq1plfGaQX+dvweqZw3O7sMd1ecB3yqZpyK7J/TOOXCp4XEy2E2i0+VkvJXyo0/5/wS25///Oez\n8t/+9rfcdn4p8bKlPRulXjGwbt26LLXn0ksvzdX5c2t8Pp40aVJWPuOMM3J18bLDFX5JWSl/Ve+7\n7rqrHa1uW/xe9K9LvPSrP//HV4z2yzz7crx//zkR78Mv3V6P9wGzCgEAAAAU0FEAAAAAUEBHAQAA\nAEBBR5dHbZrp06c3uwmSpCFDhuRun3zyyVk5zpk78cQTq+7nW9/6VlbuyLJVPVU8D8Gvux5fzt7P\nS7jllluy8qBBg3LbxXnhtfI51Mcee2xWjpdw9Xm1fnk1STrppJOycpwzWDbnArWptoRyHCtl+f+n\nn/5G2m2cD+pj6W1ve1tW9nnlUn4Olc8TlfJzLuL2+nzyhx5649qUcWz4NnZnffr00ciRIyVJI0aM\nyNX96U9vLJby97//PVf35z//OSuXze2pNp9AKs4pqJWfX+Dz5P2cCil/vvA5xlJ+jlq8XKO/7Y8t\njkU/xymu83Mb4jkylbpkifvWEi8j65fBXblyZa5uypQpWfmGG27I1fn3S1fm8ZfNQ4g//5ctW5aV\n42VP58+fn5X9a+mX9JWK567uysyyuYPxHEI/Lyx+r/z+97/PyldddVWurtZrtPj3Tke/e/nP/He+\nM38h6uOOe2O6xl577ZWr88d233335er8fDU/j6Jfv34dauPEiRNzt+NlVmvBiAIAAACAAjoKAAAA\nAAq6XepRPExdq/333z8rx0tX+iXKdthhh1ydH+L7wAc+kJXjZft8moFfwk/Kp5rEw2txagQSPoVI\nkt7znvdk5TiF6Ic//GFW9sO18bD/OeecU/Xxal2O8IQTTsjKcQrR3Llzs3J85V2fqhYv19gq6XSt\nJn4OfcpIfLVWvzSmTwfadtttc9v5Yd44pcGne8RL9fmUgNNOOy0rx8O48T5r5Y/Vx1Gc2hQfT3e1\natUq3X777ZKKy5zGVzr2fBpRvMywfw9XSxOS8qlB8bmkLG2omnjJa39eiGP4j3/8Y1aO06qmTZuW\nlX1qQtnVl+NlI/1n23bbbZerqyxHGy8p2gr8sqaS9L//+79Z+fHHH8/V+XSd+HPYp+iUpef4VCGf\nJiR1bAniShpdhU8vWrBgQa7OH2ucWuj3c8opp2Tlww47LLfdI4880u42tqKBAwdmaWf+vCpJ++67\nb1aOX8sHH3wwK//sZz/L1fmlZH16kU8TkvJLlvrHkqQ999wzK5ct1+/596yUTxWK06C9OA3Vf5b5\nZVvj58C/9+O0JL/PeNn9Aw44oGpbqmFEAQAAAEABHQUAAAAABS2ZehSvFuJXafjFL36Rq/va175W\n0z79CjVx6lHZ7Hp/Fc9LLrkkK/uVSSTpzjvvzMp+xQ0pn7YQDzM9++yzm217bxSncPgUgXgY1r9m\nfqWLONXIp5jFw++1rjzkX1ufKiDlh0P90CU6Jk7L8++dsvQAn54Sr/ziVyXyK1FJ+RSVa6+9Nlfn\n42Xw4MFZOU5/8Wkh7VmZpNpKRz1llaPYxo0b9fLLL0sqrkL06U9/OivHK8aMHz8+K8erGXVEe9KG\nqrnyyitzt2+77basfP311+fq/OoqcXz4uPIpNWWpR2VXm49Xf6k8X/HnXyuopEVV7L333lnZp51K\n+XS/sWPH5urK0rI8n+Lj04Skjl3J1q/UJeVTzG688cZcnY/bOHXKfxfxaTNxGlxHV+5qNZs2bco+\nv+MUGX/8cSz7led8ipbUsdS6jqYN+e+q8QpcfsWisqvB+/e9lI8//90m/m7q4zs+D/i6OPXNn0Nr\nxYgCAAAAgAI6CgAAAAAK6CgAAAAAKGjJOQqf+MQncrf9coFHHHFEh/b54osvZmWfPyjlr6J7//33\nd2j/3rnnnpu77fMh58yZ0+n99xRl+bVx7nd82/N5nj7/019FOa7zV3OW8vnwZY/l/eAHP8jd9nmN\nkydPztX5fZZdURZvmDlzZu62z1kdOHBgrm758uVZ2c9H8lc8lvJLK7797W/P1fk5EGVX6vSvZTyf\nyuedx0ub+pz3Rx99NFf3/PPPt7nPnrIcamzQoEGaNGmSpOKcI59DG1952OfexvndHZmzEM8h8Mvn\n/vWvf83V+dfFtyue3+TbGC+/6uecxLnVflt/bPH5yD922VWbK3NAKiq5yjfffLNaTZxH7XPG4/dA\nfJXiavcrO4/7x4uXZvWvZ7zUuf9+MGPGjKwcz3Pw8yiGDx+eq/Ofe3EbfX66X+43fi3juOqu1q9f\nny0H6uf4ScUrFnv+St5+mVNJGj16dLvbET/WH/7wh6wcx4CfA+HjLf4s8J8h8fwCPy8hPg+cffbZ\nWdnP3Ynf6z5Wyh47vl9HrkLNiAIAAACAAjoKAAAAAApaMvUo9t3vfrfZTWiX4447rmqdH9Lq7eJU\nAT8kG6cVlKm2TFi8pJpP76jH1ZBfffXV3G2/NOt5552Xq/PDhLUuxYo8f8XKMj71KF6q2Kd9+atl\nS/kh2nhpRX/bpwvEqYR+WPqzn/1srs6nC1xxxRXVD6AX6Nu3b3bl4HjY319ZNR7298PmZaklZUub\n+rp4GL7sisg+BcYvpfsf//Efue186pS/QrRUvtyhj00fi2WpR2VXlI/3X9nWLzfeKl566aXcbX81\n5h/+8Ie5uo4sX9pRcUrRsGHDsrJfojRe3vXUU0/NyvHyrv78FKc6+3j3z0G8fGdv59OU4pSljojP\n9/49Fl+12df597pPh5LyV3SOl8Uv+6yJl0utiM9pfh/xe8LHaT3eL4woAAAAACigowAAAACggI4C\nAAAAgIJuMUehJ7nuuuua3YSW5XP19tprr1ydz9ONc+78XAefNxrn75bl89ZbvCTerFmzsrJfihX1\nFy8H51Wba9Ce+/nt/LwXSVqzZk1WPvroo3N1Pk81Xra1t1m9enWWn71w4cJc3QsvvFDTPsrytsve\n60OHDs3KJ598cq7u8MMPz8rxEpR+XkKtyzVfeeWVubrbbrstK8dLs1Zb+jWeK+Fvl9WNGjUqVzd2\n7FhJ+Xk0raLs9arMZanwSxw3WrwUq5+n4uPDx40kvfWtb83KPm4k6e67787K06ZNy9X94x//yMp+\nuW00ll/OVpImTpyYlf08BCk/99GX/dLYcV38ncXPVb344otzdX4Oo9+H/24j5ZcIj+c5+PsdfPDB\nuTp/bLViRAEAAABAAR0FAAAAAAWkHqFLvfbaa1m6Rjxc5ofW4rSNVatWZeU777wzV+eH6f2QWzNT\nj9A8HUkhim/HS/f6YWW/3cqVK6s+VjwUjTds2rQpS4GJ0zv233//rBynIPqlA8uuxFz23PuUnLJz\nQnx+8qlCDz30UFa+5ZZbctvVetXmeP/+ePw5LT6WsiVc/e346uWV/ZuZWp1vY3yl3Y5cebfR5s6d\nm7t9wQUXZOX58+fn6p599tmsPGjQoFydfy+04nH2VPHVuf1tnyrWUfFSxTNnzszKTzzxRK6u2pXX\n4xSoCRMmZOX4XOLFcRQvG18LRhQAAAAAFGy2o2BmO5rZ7Wb2tJk9ZWafSf8+wsxuNrPn0/+Hb25f\n6J6IARADIAZADIAY6H1qGVHYKOkLIYS9JB0m6ZNmtpekr0q6NYSwm6Rb09vomYgBEAMgBkAMgBjo\nZTY7RyGEsFDSwrS8ysyekTRW0jskHZNu9htJd0j6SkNa2c35fMt4aar4Eu6tqJ4xsHr1at17772S\nivmZfrm5uG7OnDlZ+Ze//GWuzufcHXnkkVk5zj9et25dWdNQojudB8rmKPjc73g7Hy9xzmfZ3IZq\n++9p6hkD/fv31x577CFJ2nXXXXN1I0aMyMplubdlcxTK6vycgsceeyxXd9ddd1W9n1/GtWz/funU\no446KldXWaJUKi6bWW/Vnruytm9OdzoPdKV4ueNly5Zl5VdeeSVX55d79bEuSSNHjmxA6+qLGGi/\nSZMm5W5/9rOfzcrxHKR//vOfWdnPbYg/d/zyufEcCD+nM15mN962Fu36VDOz8ZIOlPSApFFpwEjS\nIkmjqtznXDN7yMweaqse3UtnY6Bs/XF0D8QAOhsD/noT6J46GwOvvfZal7QTjdPZGODHu+6h5o6C\nmQ2S9AdJnw0hvOrrQghBUmjrfiGEi0IIh4QQDulUS9F09YiBnvyLa29ADKAeMRCvyIPupR4xEF9A\nCt1LPWLAX4QSraum5VHNrK+SgPhtCOHa9M+LzWx0CGGhmY2WtKT6Hnq35D2T6K5fkuoVA5s2bcqG\nxXbaaadc3Xve856sHF/J0A+Xvfpq7pyUW+7wve99b1aOr0AaXwG2kR5//PHcbZ8eFQ/9x8faqrrj\neSAervXLR/oh3njb+DWqlrIU778sVaYnaEQM3H777bnbPlUjvgqvf77LrqxdNmr18ssvZ+WypUL9\neVuSDjnkjd+6/DKWY8aMqbqPnqg7nge6mr/Sb1l8xOmxZVcbbyXNjgH/mRmfx/13BT9iEX/O+vvF\ndf51iF8jf57xo2LxOcfvP77a/IIFC7JyfAV1/9gzZszIyg8++GBuO5/eFn9e+U54fNXwfffdV+1V\ny6pHJuliSc+EEH7oqq6XdFZaPkvSn9r96OgWiAEQAyAGQAyAGOh9ahlROFLSmZKeMLNK9+Zrki6U\ndLWZfVjSXEmnNaaJaAHEAIgBEAMgBkAM9DK1rHp0t6Rq47PH1bc5PV88DDR16tTmNKQd6h0DlfSM\neDa+X7Eonpnvh+ri9C2f7+yv3Priiy/mtps1a1Z7m9ph8dVafVqVX91J6tp2dVR3Og+UpZ341KB4\nyNrfjmOz1lWPak096o4piPWMgfXr12dXaI9XhfFD6mVXTo5fP8/HQJwLP3ny5Ky833775er8VaFj\n1VYLiv/uY6A9dT6u/HGXpT6UpV/5FCvpjatEx7HdHt3pPNAqys4J8evXHVKPWiEGfAzH8Vxt3kPZ\neTuu898/4hTmFStWtLld/J3Fn598ClF8O0578ulSfp9lK/HFz4FfSSlOuY6/f9Si+31aAQAAAGg4\nOgoAAAAACugoAAAAACioaXlUdE7ZEny9jZll+dlxjq7Ps4uXE/vzn/+cleNcPb/0qM/nbWa+Z5wH\n6K/IGudFd4c5Ct1JrXMIynK/46tldmSOQpxD73PlDzrooFzdpZdeWnWfPVG/fv202267SSpendYv\nF1iWl1s2z8M/1/F5xs9pas/ytn4/ZXMNfLviOPLbxu338ejvF1+YrNY5CvFF7Sr7j5d9RecdfPDB\nuduHHnpo1Tr/Osevn18C85FHHmmz3BvF7yP/mT9o0KBc3bBhw7Kyn7Po7yPl5zKU5fjH5wS/ra+L\n2+i3i+dN+GW6Y9WWfq11LoOUvzJzfI7zdbViRAEAAABAAR0FAAAAAAWkHjXAtGnTcrf90pi9nZll\nw+/xcF9Z2tCjjz6aleNhvMMOO6zNfcTDcV3p+OOPz932w4aVZQrRGGWpQT4lJU4N8jE3ZMiQXJ2P\nubL9++Hn+EqaQ4cOzcoHHnhg1X30BltssUWWChCnBPjnME7PKUv5qWblypW5208//XRWjpdmjbet\nt7J0qbJlfeupbFlZdMzcuXNzt5988smsfNFFF+Xq/OscvxaLFy/OysOHD8/KcVpLb1O2lLVfrlSS\n5s+f3+79x98pfPqSv8p2XFctzSmui/fv06Xi1Cb/+eLL8XPgv1P4NKS4Lv686sj3D0YUAAAAABTQ\nUQAAAABQQEcBAAAAQAFzFBpg6tSppbd7u0pOXpxj7OcXxEtX+iUCR48enat75zvfmZX9vIQlS5Z0\nvrEl4iXO/G2fjy5Jzz//fFaePn16Q9vV2/l8zTjv2+ebLl++PFe37bbbZuV4eVuf1+ljM87/9HMb\nZs+enas76qijsrJfAlTK566XLdHZU2zYsCHLx/Z52T1dV81DQNeKP2v87Xj+Qq18Tnp8PkJ9xTn+\n/vbChQu7ujl1NXHixNzteM5FLRhRAAAAAFBARwEAAABAAalH6FL9+/fXrrvuKknaZZddcnV+Sa/4\naqQTJkzIyttvv32uzqdtLFq0KCvPmTOn8w0usf/+++duV45LKl5FMb7dm22xxRaF5eIqHn744U7v\nf+nSpW2Wpfww8vXXX5+r8/EYXxncL6N52223ZeUFCxZU3S4+Fh/DcbpNvFRwtX3UKh5urqRZNXPJ\nYMAbOHBgdgXj3XffPVcX3+5uyq6e7W+XLZdbj3TEOM3kueee69B+GuX111/PljeNl5PuyBWEURRf\nGXzSpEnt3gcjCgAAAAAK6CgAAAAAKKCjAAAAAKCAOQroUgMGDNCBBx4oSdpuu+1ydT4vPF5ubp99\n9snKcd6lX7oyXvKykY499tjc7TFjxmTlOBe0I5eV76kGDhyY5U3uscceuTq/RGlH+fkPcW7v8OHD\ns/JWW21VdR/xErx+2VOfOzxs2LDcdkcccUSb7ZDy8yVWr16dq/N5pHvvvXeb7W2PeG7GQw89JEla\ns2ZNh/YH1FufPn2y93v8Pu0tc7rKlsv1dRs3buzQ/v2cPam4DGgrqLz28TEuW7asGc3pceLnsTIn\npD0YUQAAAABQQEcBAAAAQIGFELruwcxeljRX0raSlm5m867Qm9oxLoQwssGPsVnEQFXEQPP0pnYQ\nA23rTe0gBtrW29rR9DggBqpqqRjo0o5C9qBmD4UQDunyB6YdLaNVjpl2NE+rHDPtaJ5WOWba0Tyt\ncsy0o3la5ZhpR9tIPQIAAABQQEcBAAAAQEGzOgoXNelxY7SjeVrlmGlH87TKMdOO5mmVY6YdzdMq\nx0w7mqdVjpl2tKEpcxQAAAAAtDZSjwAAAAAU0FEAAAAAUNClHQUzO9nMnjOzWWb21S583EvMbImZ\nPen+NsLMbjaz59P/h3dBO3Y0s9vN7Gkze8rMPtOstjQLMUAMNCsG0sduehwQA8QAMUAMEAMJvhO0\nfhx0WUfBzLaU9FNJp0jaS9LpZrZXFz38VEknR3/7qqRbQwi7Sbo1vd1oGyV9IYSwl6TDJH0yfQ6a\n0ZYuRwxIIgaaGQNSa8QBMUAMEAPEQK+OAanpcTBVzY8BqTvEQQihS/5JOlzSTe72eZLO68LHHy/p\nSXf7OUmj0/JoSc91VVtcG/4k6YRWaAsxQAz0hhhoxTggBogBYoAY6G0x0Apx0Gox0Kpx0JWpR2Ml\nzXO356d/a5ZRIYSFaXmRpFFd+eBmNl7SgZIeaHZbuhAx4BADkpofA1ITn3tiQBIxMF7EADHQ+2JA\nar044DtBG5jMLCkkXbYuWyfWzAZJ+oOkz4YQXm1mW5AgBiB17XNPDLQmYgDEAPhO8Iau7Ci8JGlH\nd3uH9G/NstjMRktS+v+SrnhQM+urJBh+G0K4tpltaQJiQMSAWisGpCY898QAMUAMEAO9PAak1osD\nvhO0oSs7CtMl7WZmO5vZVpLeJ+n6Lnz82PWSzkrLZynJC2soMzNJF0t6JoTww2a2pUmIAWKg1WJA\n6uLnnhggBogBYoAYkNR6ccB3grZ08SSNyZJmSpot6f/rwse9QtJCSRuU5MB9WNI2SmaSPy/pFkkj\nuqAdRykZPnpc0oz03+RmtKVZ/4gBYqBZMdAqcUAMEAPEADFADDQ3DlohBrpLHFjaUAAAAADIMJkZ\nAAAAQAEdBQAAAAAFdBQAAAAAFNBRAAAAAFBARwEAAABAAR0FAAAAAAV0FAAAAAAU0FEAAAAAUEBH\nAQAAAEABHQUAAAAABXQUAAAAABTQUQAAAABQQEcBAAAAQAEdBQAAAAAFdBQAAAAAFNBRAAAAAFBA\nRwEAAABAAR0FAAAAAAV0FAAAAAAU0FEAAAAAUEBHAQAAAEABHQUAAAAABXQUAAAAABTQUQAAAABQ\nQEcBAAAAQAEdBQAAAAAFdBQAAAAAFNBRAAAAAFBARwEAAABAAR0FAAAAAAV0FAAAAAAU0FEAAAAA\nUEBHAQAAAEABHQUAAAAABXQUAAAAABTQUQAAAABQQEcBAAAAQAEdBQAAAAAFdBQAAAAAFNBRAAAA\nAFBARwEAAABAAR0FAAAAAAV0FAAAAAAU0FEAAAAAUEBHAQAAAEABHQUAAAAABXQUAAAAABTQUQAA\nAABQQEcBAAAAQAEdBQAAAAAFdBQAAAAAFNBRAAAAAFBARwEAAABAAR0FAAAAAAV0FAAAAAAU0FEA\nAAAAUEBHAQAAAEABHQUAAAAABXQUAAAAABTQUQAAAABQQEcBAAAAQAEdBQAAAAAFdBQAAAAAFNBR\nAAAAAFBARwEAAABAAR0FAAAAAAU9vqNgZrub2QwzW2VmnzazX5jZ19O6Y8xsfrPbiMYiBkAMgBgA\nMdD78Jp3Xp9mN6ALfFnS7SGEAza3oZm9IOkjIYRb6vXgZnaHpMMkbUz/9FIIYfd67R81aWoMpPt9\nn6T/kLSTpEWSzg4h3FXPx0CpZp8HVkd/2lrSz0II/16vx8BmNTsGxkv6maTDJa2X9HtJnw0hbCy5\nG+qr2TGwp6SfSjpY0suSvhRCuK5e+0ebmv2af0rS2ZL2lXRFCOHsqP44JTGxk6QHlHw3mFuvx6+H\nHj+iIGmcpKca/SCWqPZ8fiqEMCj9Ryeh6zU1BszsBEnflXSOpMGS/kXSnEa3BzlNjQH3/h8kaXtJ\n6yRd0+j2IKfZnwU/k7RE0mhJB0g6WtInGt0e5DQtBsysj6Q/SfqzpBGSzpV0uZlNbHR7erlmv+8X\nSPq2pEvauM+2kq6V9HUlMfGQpKsa2c6O6NEdBTO7TdJbJP3EzFab2UQzm2pm325j28uU9OhuSLf9\ncvr3w8zsXjNbYWaPmdkx7j53mNl/mdk9ktZK2qVLDgw1a5EY+E9J3wwh3B9C2BRCeCmE8FIDDhdt\naJEY8P5VyRdGRpS6SIvEwM6Srg4hvBZCWCTpr5L2rvvBok0tEAN7SBoj6UchhNdDCLdJukfSmY04\nXrTEa64QwrUhhD9KWtZGE98t6akQwjUhhNckTZG0v5nt0emDr6Me3VEIIRyr5MO48ov+zJJtz5T0\noqRT022/Z2ZjJd2opDc4QtIXJf3BzEa6u56p5JeBwZKqDRddYGZLzeweH2RovGbHgJltKekQSSPN\nbJaZzTezn5jZ1nU8TJRodgy04SxJ/y+EEDp8UGiXFomB/yvpfWY2IN3fKUo6C+gCLRIDMZO0T4cO\nCJvVoq+5t7ekx1wb1pZszlYAACAASURBVEiarRb7AaFHdxTq4AxJfwkh/CX9JfhmJUNDk902U0MI\nT4UQNoYQNrSxj68o6WWOlXSRkt7qhIa3HPXS2RgYJamvpP8jaZKSlIMDJZ3fBW1HfdTjPCBJMrNx\nSlJOftPYJqPO6hEDf1fyBeBVSfPT+/+x0Q1H3XQ2Bp5TMpL4JTPra2YnKjkXDOiS1qMj6nbur2KQ\npJXR31Yq6XS0DDoK5cZJek865LTCzFZIOkpJjmnFvLIdhBAeCCGsCiGsDyH8RslQ4+Sy+6CldDYG\n1qX//28IYWEIYamkH4oY6E46fR5wzpR0dwjhH/VuJBqqUzFgSe7yX5XkIw+UtK2k4UrmLqF76FQM\npF8i3ynprUoWtPiCpKuVdBrRmup57m/LaklDor8NkbSqE/usu96w6lF7xKkA8yRdFkL4aDvuU8tj\nWDvvg65T1xgIISy3ZPm1UMv2aAmNPA98UNKFHWoVulK9Y2CEkvznn4QQ1ktab2aXKklp+HKnWopG\nqft5IITwuJJRBEmSmd0rRhdbSVd8B/SeUpKKKkkys4GSJqgLJl+3ByMKeYuVn4xyuaRTzewkM9vS\nzPpbsu7uDrXszMyGpfftb2Z9zOwDSla8IS+1ddU1BlKXSvp3M9vOzIZL+pySlS/QmhoRAzKzI5Sk\nILLaUeurawykI4n/kPTx9LNgmJIvCI/XveWol7qfB8xsv/R+A8zsi0p+mZ5a32ajExrxmvcxs/6S\ntpRU2UflR/rrJO1jZv+abvMNSY+HEJ6t0/HUBR2FvAsknZ8OMX0xhDBP0jskfU3JmsfzJH1JtT9v\nfZX8YvSypKWS/l3SO8sm1KDp6h0DkvQtSdMlzZT0jKRHJf1XXVuNempEDEjJF8NrQwgtNayMNjUi\nBt4t6eT0/rMkbVDyowFaUyNi4ExJC5XMVThO0gnpCBNaQyNe8/OVpCB/Vcmch3Xp3xRCeFnJKnj/\nJWm5pDdLel99DqV+jIU3AAAAAMQYUQAAAABQQEcBAAAAQAEdBQAAAAAFneoomNnJZvZcesXZr9ar\nUeg+iAEQA5CIAxADIAZ6og5PZjazLZWs4nKCkguGTJd0egjh6ZL7MHO6iUIIdb1+Q0diYMCAAWHo\n0KGSpPXr84s9bLFF5we4/D769u2bq9u4cWObZUkya71LW8TPhz+e119/PVf3z3/+s83tJGmrrbaS\nJC1fvlyrV69uegwMHDgwDBs2rJZ9525vueWWWblPnz6l21Zs2JC/UKZ/3irPS1v7989n2f3ifWza\ntKnqPuK2NMOKFSu0Zs2augd7e+Ngq622CltvvbUkaeDAgbk6/5lU9vlU62dXvF2t+6/18eL3qY/F\nsro4Zut93OvWrcvdXrNmjaQkRuv9WSC1PwYGDx4cRo4cKan4PorPYegYfz6S8uekWbNmLQ0hjKzn\n47U3BoYPHx7Gjh0rqXq8onPi7zqV864kzZ8/v6YY6MwF194kaVYIYY4kmdmVSpaRqvoFAT1Ou2Ng\n6NChOueccyRJc+bMydX169ev0w0aMGBAVh4zZkyu7pVXXsnKixcvztW14geTPxZJ2nHHHbOyPxZJ\nmj//jYt7jho1Kle38847S5J+8IMf1LuJUgdiYNiwYfr4xz++2R3Hr0mlgylJI0aMyNX52PEfji+/\n/HJuu5UrV2bl0aNH5+p852XBggVV7+dfhzjG/IfdvHn5C3YuWrRIzfbzn/+8UbtuVxxsvfXWOvLI\nIyVJhx56aK7O/4AQd4i9uC7+UlRtO7//ss5b/EXef8nyHYD4ferjtn///lXr4vj2bSlrlz+eascs\nSY8/nr9Ew/Tp0yVJa9eurXqfTmpXDIwcOVLf/OY3JUnjxo3L1cXvK3TM6tWrc7f9OenUU0+d24CH\nbFcMjB07VldffbUk6YknnsjVPfzwww1oXu8Tfwbut99+Wfnzn/98TTHQmY7CWOUvXT1fyRqwOWZ2\nrqRzO/E4aF3tjoF+/frprrvukiS98MILue0OOOCADjXC/yJ5zDHHZOX4y/Tll1+eleMP9x12aNe1\ns9qt7MvDihUrsrL/VXufffbJbee/2E6bNi1Xt2TJkqz8hS98IVdX+RD2vyTUUbtjYNiwYdlxxq9D\n2Zf8l156KSsPGZK/6v0222yTlffaa6+sfNBBB+W2u/nmm7NyJQ4rJk+enJUPO+ywXJ1/vm+99das\nXPmy29bjjR8/Pld3zz33ZOW4I9IDbDYOfAwMHTo0e45PO+203I58pyz+Rd5/gS77wuvjKP5l0n95\neu2113J1/v0XP7Z/PL+d78BK0qBBg7Ly4MGDc3X+dtxR8G3x7Y87OrUe9z/+8Y8271fWueikdsXA\ndtttl71vJ0yYkNtRZaQBnePPmVLxM7cB2hUDo0ePzuJ7xowZuR29+OKLjWxnr7F8+fLc7aOPPrrK\nltU1fDJzCOGiEMIhIYRDGv1YaE0+Blrxl3s0no+BONUEvQMxAB8DcWcfvYOPgeHDhze7OahBZzoK\nL0na0d3eIf0beg9iAMQAJOIAxACIgR6pMx2F6ZJ2M7OdzWwrJZedvr4+zUI3QQyAGIBEHIAYADHQ\nI3V4jkIIYaOZfUrSTZK2lHRJCOGpurUMLa8jMbBhwwYtXLiwzbo4J79Wb3rTm7Kyzy3/3ve+l9vO\n5wAffPDBubp4Ml1nxfMQJk6cmJXj3OG5c9+YT+Tbce65+ak9Dz74YFZ+9tlnc3W77rprVj7uuONy\ndZWJvnGb6qEjMRBCyFZiiOdN+LkGy5Yty9X53PU419bPZ9hpp52y8plnnpnbzsdKPEfhf/7nf7Ly\nV77ylVzdpEmTsnLZPId3v/vdWfnDH/5wrs7nht555525uu4+Z6G9cbBp06bs/RinI/p5K/GE4rK5\nPtXmL/j5BPE+4zr/2HG7/Epb8Qotnl/BJ96/n/cQL97g5y/4c1U8J8Efdzzp2W8bz49o4NwESe2P\ngfXr12vmzJmSpGeeeSZXF99Gx5RNpm+E9sbAunXr9OSTT0qSrr8+35+IV+tBx8QLA3RkPmZnJjMr\nhPAXSX/pzD7QvREDIAYgEQcgBkAM9ERcmRkAAABAQadGFIBmiIfU/ZKos2bNyspXXHFFbju/vGi9\nU41ilWsXVGy77bZZ+Y477sjV+Ws6vP/978/K8TDxVVddlZXjJRPjx2tlGzduLKQVVZx00klZ2aca\nSflh9Dgdw6dz+TSeH//4x7ntPve5z2Xlr341f9HQ73znO1n529/+dq5uypQpWbmy9rsknX/++bnt\nrrnmmqwcp3p88pOfzMpxetjtt9+elf01MXojH9vx8rl+6dF4adNVq1Ztdn9S9eshSOXXOfD8Rc/i\nFImyaz94taaFlF1vodoxoz7i19IvrevPT/H5qOyaGD7+yi7sWKuy9Lbtt98+V1e5uBlal//cKLte\niz8HlaVC1gMjCgAAAAAK6CgAAAAAKKCjAAAAAKCAOQrodo488sjcbZ/De8kll2TlOIe50Xn8Puc4\nfiyfN//II4/k6kaOHJmV/TyKpUuX5ra75557svLQoUNzdY2ec1FPmzZt0po1ayRJc+bMydWdffbZ\nWbk9efx77713Vt5xxzeu9xMvQ/rf//3fWTleAvVrX/taVi6bo+DLF154YW67L33pS1n5yiuvzNX5\nfNNPfOITubpTTjklK990001Z+cUXX1Rv1p7lHastjxrniPvXwS95GtfF8xd8jrCvq8RyW48X78OL\n57D4Y/XzsOL891deeaXqPruTTZs2ZbnVcZzffffdue28WvP4fe5+/Dr4pWnjHP+yeSo+BvzSzvEy\nz76NcXv93IZ58+bl6l56qbZrk/kleOPH9p8Nfk6PlJ8r1535mPDPRaw7LrHq5x74+Is/81esWNFl\nbWJEAQAAAEABHQUAAAAABaQeNdlb3vKWrOyX8/RLfkrFK/H2Nn7Y7YgjjsjVzZgxIytfeumlWTlO\n/9l9990b1Lri48VLmd1///1Z2Q8txu3yV3CePXt2bjs/3Nroq6w2kk89uu+++3J1P//5z7PyOeec\nk6vzz03ZEqJDhgzJyv5qyFI+fem73/1urs6nIvn0Ikn6xje+kZV9ipJPZZKkH/zgB1nZL8UqSZdf\nfnlWjpdF9ClLfonYadOm5bbrbUunlqXnxGkhPtWw7ArO/rmP0zb8/cpSUsra6FNN4rSneJ+eT4fx\n5484hbJM2bKclfSbVjl3vP7661kaTnyldX87Tt0pSz3yz68vx+kp/nX3qZ+SNHz48Kw8YcKEXJ0/\nx48fP76mfcQpI779cXqpTytbtGhRVo5j2Keaxlfe9ccdp6k9/fTT6gnKUvr8ldd9amGrpiGVvR93\n2WWXrBx/ZvgUtrL0q3pgRAEAAABAAR0FAAAAAAV0FAAAAAAUdIs5CkcddVRWjnMBr7vuuq5uTl29\n+c1vzsrTp09vYktaW7wkqueXRPUaPSchNmrUqKy8fv36XN1TTz2VlX0usiS9+93vzso+X/GZZ57J\nbedzW1slz7gj+vTpo+23315Scbm+u+66Kyv7OQmSdMghh2TlONe71hxNPyfo1ltvzdX5JVHjOQrf\n/OY3s/L555+flf3cAik/Z+H/Z+/O4+SsqvyPf68BDJIICZEQIJCwSMISdmSXxUBAEFdeImBwRBx+\n6MgoCogwjCuiw+Ao4xCQAOqIgMgiEQgYtiCQBDLsOwSCgQQIe1gC9/dHV9+ce56uSnV3LU93f96v\nV1651ffpqlvVp56u28+55/7iF7/I+o466qjUPvvss7M+m5N97LHHdjleqXaJ2P7C5hX78o72tn+P\n2T5bUtTnM9v79+/FWnFkY87mjPtYrLWmwOZP++dm10DY97fPT6/2PKViHnOZDR48WOPHj5ckfexj\nH8v6fvKTn6S2f31rrfOw50h7nH9dqh3n+3z5UntOvvTSS1Pbl3e160P8Y9vyzXadgyRtvvnmqW1L\n5NqvS/naCb/OwZ5LHn300ap9fYn/fWffw349oF2L0J31PWVh48+W/Z43b152XK33QaNxRQEAAABA\nARMFAAAAAAV9IvXIXn736Qh9LfXIXwa35dbWXXfdVg+nz7Al4J577rmsb/r06altU9M6L2s3i08J\nWHvttVPbX4q2OxDbS89S/tzsDs633nprdpwtl+rL9vUlK6ywgoYPHy5J2mqrrbI+Ww7U79psU498\niUSbQrLaaqultr/Ubkun+rQeG0c+9ajabsw+9ciWRD3zzDOzvrPOOiu1bWqFlD/Xr3/966lty636\nMfvUKRs7fYm/hG5Ta3xagT3W91VLM6h1/z7VyMaRT3ew526bJuTHYY/r6XOz8e1/Z9hx+feBfT61\nSkiWwaBBg1LpUJ+C03l+6DzOqvd52depcwfoTrZs6MMPP5z12fQi31etfKnfnfu1117rsu2/zz+3\nLbfcMrUnTZrU5eNK0mWXXZbafvd5qzs7m/cl9nWzKVpS/rPwqXlltHjx4uy2/bnbtv+dZNMYm63c\nZxIAAAAAbcFEAQAAAEBBn0g9mjx5cmr7nVz7Gr+L4hFHHJHadufWgb4Tc1/g0+BWWWWV1P7b3/6W\n9dkUgXHjxmV9No3IphfZCkBSfsm92WlVrdJZ/aiTrYLkK3bYy80vvfRS1fvcbbfdUttfel64cGFq\n2xQlSZo4cWJqX3PNNVlftd2YfWqQTT2yKURSnrJ02mmnZX02jcimJfndnW0lpV133TXrs/FS9jSk\nEEL6efoUHPszq5X+U2vXY5ti4VMT7Pf5NJZqlYc8+32+cpJ9r/vnZtV6bvY+fAzXSqew91H2ymhv\nvvlmSvOZNm1a1nfDDTdU/T6/m3Y19jX0u/Lan5lPx7HpW/6x7LnapgFuttlmVcfo03/sTs22Up6U\nV/N54oknUnvOnDnZcbXSjfoj/z61v2trpR6VlX1v+hg75JBDUtumvvn3xIYbbticwXWBKwoAAAAA\nCpgoAAAAAChgogAAAACgoE+sUSh7mbfuOOecc6r2+ZxslNs666yT3bY7xfqcUluW87DDDsv6qu3G\nvGDBguw4u77F58T2Je973/tSXqbfcds+r7vvvrvqffhSmPfcc09q77///qm90047ZcfddtttqW3L\nFEr5moV99tkn6/vLX/6S2rYkql2vIOVrCPz6gp/+9Kep7dco2LKIBxxwQGpfccUV2XH2sX/84x9n\nfXbNgs9h9s+13UIITS3VaO/bP469Xav0qF9fYN/f1dZD+D5fftWWR/VrG6qtKai1A6tfp9EXykF2\nevfdd/Xyyy9LKpYxtvn53dmF3a5LqLXWwJbRtqVYpXwNwbBhw7I+WwLb3qcvgWof24/X5p1fcMEF\nWZ9dq2HPcbV2p64V3/Z5StLqq6+uvsK+H+zaDSl/Xv5n2xfeAy+88EJqH3744Vnfvvvum9qf+9zn\nUrtWGeZm6z+fwAEAAAA0DBMFAAAAAAXLTT0KIZwraX9JC2OMm1W+NlzSHyWNkfSkpINijIur3Ud3\nTZgwIbvtS4j1Zb6Ul2V3hi2TdsRAWdW6rGt3z7QpRJI0atSo1PbxbEtZ3nvvvQ0ZZzM0Kg7efvtt\nzZs3T1Ixvcim4NQq/+YvL9v7ufDCC1PbllaW8lSnmTNnZn02PceXTrXpTJdffnlqf+Mb38iO++Uv\nf5na3/ve97I+m5bkd3y192/Pf/7ysk2BsiVbpTwVyad0dT7X3qb7NCoGYowpTaQZl9BtSo7fvdiW\nq6z1etS786lPIbK3fcqIfexaKUW12Odm06Gk/Ln6590ojYqBlVdeWZtuuqmkYqqfLTtcK+2mp+x9\nvP3221nf888/n9o2BUrKy1fPmjWr6v3bVCR/H/a97x+72nOrlX7lU29s6pRPNWpUul8rPhPU2uHc\npoT5NNRqO7S3M4XdpxXac4Qt5y1Jf/jDH1Lbls32pfVbqZ5X7jxJk9zXjpd0fYxxI0nXV26j/zpP\nxACIAxADIAZADAwoy50oxBhvkvSi+/KBks6vtM+X9MkGjwslQgxAIg5ADIAYADEw0PT0WszIGGNn\nSZZnJVXNDQohHBlCmB1CmN3Dx0I59SgGmnU5HG1TVxzYGFiyZEnrRodW6HYM9IXKJOiWbsfAK6+8\n0rrRoRW6HQOvvvpq60aHHut1edQYYwwhxBr9UyRNkaRax1n77bdfdttvgd7X2Jz0sWPHVj3O5qr3\nJd2JgcGDB9cVAzbPUpI22GCD1LZ5oq1mS6L60nkXX3xxavsJ0TbbbJPa9rlI+fOxvzz98/TfVza1\n4sDGwMiRI2Nnubtbb701O+7BBx9M7aOPPrrqY40YMSK7bUvHXXXVVVW/78tf/nJq23KiUr5mwb8X\n7ZoFW770yiuvzI6zY95hhx2yPlv+2K6jkKQxY8ak9l577VV1/JZ/nieffHJqH3fccVnfeuutJ6l2\naclGqDcG1lprrXSMzz/uSQlRz77//PfYPG1/nvHlRquxx/nx15sHXqs0q1174D9M2RxsP177ff55\nd44zxrpOwT1WbwxstNFGsfN9ZeNfykuW+tfJlsr0r739udt1Ak899VR23P3335/a9pwjSc8991xX\nQy+wf/DoXHPV6ZFHHulyTJK09dZbp7ZfS7THHnukdq08fPvYfp2D7fOxY8tyNlO9MbD++utXDUb7\nc6+1DmjhwoX+/lO73nVGzebfc7vssktq+88RU6dObcmYuqOnVxSeCyGMkqTK/wuXczz6H2IAEnEA\nYgDEAIiBfqunE4UrJHWWE5ks6fIax6J/IgYgEQcgBkAMgBjot+opj/oHSbtLGhFCmC/p3ySdKumi\nEMKXJc2TdFAjB/XhD3+4at99993XyIdqiZ///Oep7Utj2lJpZc3Xa0cM+JJh1XYvbrVaqWN33HFH\navuUhn/6p39KbZ8SYJ/PzTffnNr+kvv48eO7N9gGa1QcrLDCCim1wF6ClaTZs+tbymR3upbyS/bX\nXHNNal922WXZcfY1tWlIkvTRj340tf3OxjYVyaZFHHjggdlxt99+e2r70q+2nK5NfZCko446KrVt\niVX7vKTaqTe2vPJ///d/Z32dz7W3a4QaFQN2Z2a/46gdY09LYdrv82kL9vF8mlC9JRTt/fvvqXX/\n9rb/WdrnbVNN/HoO+33VSkE2U6NiYMmSJakctN2RWJJmzJiR2v7nZ9OrfOzYEpL2HOxTg/yu9z3R\nuau0lJdUlfLPMP532SGHHJLadid6KT+32J+7/znbtCq/K7RNnbJjlPK0pN5oxWeCWrukW/61t3pa\nErXWrtD1npNsupFP+bRla2+44Yasz6bC+V3D22W5E4UY48FVuupLpEWfRwxAIg5ADIAYADEw0LAz\nMwAAAIACJgoAAAAACnpdHrXVbA5wO/kc9EmTlm1SeOihh2Z9EydOrHo/P/rRj1J78eIe73be7/i1\nHDZH3JfUtGVEm11C1OaZ+xzjZ599NrV9Lrh9Pr70ZmeerpTnzvot230+K5ax5Uv32Wef1P7LX/6S\nHfe73/0utX352SOPPDK1t99++6zvtttuS237c7aPK0mf/OSyPYY23HDDrM/GzqxZs7K+M844I7W/\n/vWvp/YvfvGL7Dhf0rWaxx9/PLv95JNPSiqWUiyDWusmapUQrcXmNPscd7tOwK8hqJULXY0fU601\nCvWuEbF50L4Ean/Zg+Ldd99NOfSd8dnJniN9TriNYVtKV8rz9V98cdl+YD6Pvae565Z9f3/605/O\n+uxnAL+2zOadP/TQQ1nf1Vdfndp+nVRP+NLy9ZbuLQP7XvG5+nadiv05S/l6gJ4+X7u+oNZ9LF26\nNLX9WgbLn4PsOhJfstbej30NGhGzPcUVBQAAAAAFTBQAAAAAFPS51CNbVqo7ttxyy6p9didUu/Ou\nlF+6O/jgZQv9/eXQ119/PbV9WoEtbeYvH9VbDnKgeemll7Lb9nK7f83sa1qrhKhNF/NlTu2lzFps\n2Tu/g6ctPVdvioRUfTfmsu/E3Gz+srn9ma277rpZn9151aYD7b///tlxNnXR7pQsSd///vdT26YE\nSnlqgd/l1bKpYz4+JkyYkNqf//znsz57CfvXv/51an/729/Ojvvxj3+c2j4NycacT4nqjM3uxGUz\nxRjr3gW5XjZFoN6ygj7VyN5Hb0vJdvc+qu3G3I4SqK2w8sora9NNN5WUpwtKeaqmLy1p04t8aUm/\nU3onf/+2ZKmPgc5dzKVi+mc1PhXZxt+dd96Z9dnyx5dfXn2rAfsZw6e/rLzyyqnt06/s7VrlecvO\nnv/95y2bqlZrZ/RaOzPX2sHZvr7+M5v9LGJTinzqkT2n+/uwqXBDhw7N+mw8liVVlCsKAAAAAAqY\nKAAAAAAoKGXqUa00kClTpmS3TzzxxLru01729+zlYV9Rwu4EPXXq1NSeM2dOdpy9BOpTDp5++unU\n9pc57S58WGbmzJnZ7W233Ta1fdqJ3VF38803r3qcTUPxMVbvbs/2OP89Nla6kzZUbTfmdu/E3G5+\n91SbcvDZz34267M7nNdiq5H4ahMXXnhhatsqRJJ09NFHp7avYGHZlJFLL70067v22mtT+1/+5V+y\nPvt8bAycddZZ2XHf/e53U9umIUnSzjvvnNr+/WMrNZWNfy/W2pm5VuqEPbc2Im2op2r9/qpVVanW\nbsz90aBBg1KKnD9f2gphPvXIVrnZd999sz6falgPnzY0YsSI1K43VW/u3LnZbXv+8O9hex7zaUM2\n3u1rMGzYsOy4Wq+Pvd3Tnc3LwKbk+IqQNgb8a1gt3cimGkn5a+PThuzP3af/VPs+/763t336oE2t\n879Pyvje54oCAAAAgAImCgAAAAAKmCgAAAAAKCjlGoWjjjoquz1v3rzU3mGHHXp0n/Y+fEkym2tu\nd2DtKbvDq5TnEz7xxBO9vv+BwOfp7bTTTlWPtWUtLV8Sz+Yd+vUFdrdnv6tttZz0m266Kbtt8493\n3HHHrM/m4PodgavtxjwQdmL2P2f72vuygtYee+xR837qYd+XknTYYYel9kknnZT1TZ48ucvv8znn\n2223XWrbXZqlfJdou/uyJP3Hf/xHattSrD5H+pxzzkntk08+Oeuzt/15sjO+a+0e2mqdz82u65Bq\n/yxt3nJPdz2uxT52I8q31lqT4Mfbl/PJe+Ktt95K50K/xqjWmjFbonrrrbfO+vx5oR73339/dvvi\niy9O7Ysuuijrs6Wsbe76okWLsuNsDv0WW2yR9R144IGp7c8R9j5tPPhYsesgn3nmmazP7mpt10NI\necnOsvElRG1evy097o+tVQLV8sfZc6Ffh2Df+35tg31se77w91FrjLZ8tf8+e59lOV9zRQEAAABA\nARMFAAAAAAXluK6xHKeeemq7h9Ate+65Z9U+XzIRy9i0AlvGUsovofrUoDvuuCO1bbqOT2mwqR/3\n3ntv1vf3v/89tX0KQLUypX6XcPvYPn2uVtrTQDZq1Kjsto0B/16xP7MLLrig14/tSw4edNBBqf3N\nb34z6zv77LNT25bq8zuI2zSGL37xi1nf6NGjU/vYY4/N+k4//fTUtmlIn/jEJ6qO378GNgXvuOOO\n6/KxfSnFdrE7M/sUH5ty4FMn7HvTpyo0Ytdp+9i1ypzax/aPu8Yaa6R2X9oJt6+waUo+ZcmWOK6X\nT+uxMWd36PW37ff5XdK33HLL1LapRlJejtXHt01hss/N/87w6avV+N3t+1I81pv6V+9xPv3HpmH5\n9J9aKT/2/W7vw58H7DnC775sY8enjlW7j3YqxygAAAAAlAoTBQAAAAAFTBQAAAAAFPSJNQr9yWWX\nXdbuIZTWzjvvnNrDhw/P+i655JLUPvHEE7M+m7e87rrrpvbdd9+dHWfL3vl1CHa9gc+br1amtFb5\nUrtFu5SXrPPrIwayESNGZLc///nPp/aNN96Y9fl8296aP39+dtuuQ/jP//zPrM/enjlzZmr7Mc6e\nPTu1ffzZ0qkTJkzI+qZPn57av/71r1P7K1/5SnZcrTULv/vd71J7ypQpWd8XvvAFSY0pIdoo9eTf\n+vHaspM+39reYcwABQAAIABJREFU7unztOcFn7dsb9uSiT7/2OYqN2MdBRprlVVWyW7bNWn+HG9L\nsw4ZMiS1/dofGyv2fCFJ11xzTWr7Ets2377Wehxbstu/D+xYfAlof7tMar03/PoCX7K0Hv57aq0v\n6EkZ2e6sJ7DrJ7uzPqJduKIAAAAAoICJAgAAAICC8l3jwIBl039qpYXY9ANJ2m+//VLbXiqeM2dO\ndpy9lGsvIUvSeuut14MRo9FsKtJnPvOZpj6WL21q039OOOGErM/uevyRj3wktX16Ub18apot1WrL\n+PpL4jYVyca9Z0sGS8tK8DVit+Fms8/55ZdfzvpsmoXf9bjRz82/9q+//npqV9utHX2PLYkr1S6/\nWq9nn302tX1pU9vnU+TsbVtG1abUStJaa62V2j6N1qbA+FQjn2ZVZq3cRbrZZUj9c7E7TfvHLktJ\nVKt8IwIAAADQdkwUAAAAABQsd6IQQhgdQpgRQrg/hHBfCOEbla8PDyFMDyE8Uvl/2PLuC30TMQBi\nAMQAiAEQAwNPPWsUlkr6VozxzhDCUElzQgjTJR0u6foY46khhOMlHS/puOYNte+yOWcbbrhh1nfr\nrbe2ejg90ZIYsDnBPm90wYIFqe3zNffaa68uv+/RRx/NjrP57z5HHMvV784Dq622WnZ74sSJqX35\n5Zdnfd/97ndT+9vf/nZq2zzi3jz2xz/+8dS2ayWmTZuWHWdL5x166KFZ3z777JPavmTnwoULJfW6\nPGdTYsCvLbClA33+tV2XUGv9gi+nWE2tPGhfjtfe/wAuc9rvzgPNYNfKHXDAAVnfTjvtlNo+vi1b\nftW2pTz2X3vttazvlVdeSe1FixZlfU8//XStYdeLGOgmf56pVQK1Wl8r12x4y72iEGNcEGO8s9J+\nVdIDktaWdKCk8yuHnS/pk80aJNqLGAAxAGIAxACIgYGnW2sUQghjJG0l6XZJI2OMnX/mfVZSl3+i\nDSEcGUKYHUKY3VU/+pbexkCZNnxCz/Q2Bmz1GPRNvY2BN954oyXjRPP0Ngb8X8LR9/Q2BuzVQ5RX\n3eVRQwhDJP1J0jExxlfspd0YYwwhdLlVXoxxiqQplfvo/nZ6/YC9TF3rUmPZNSIGBg8eXDUG7Ovk\nTyB2guHTNmw6gi2dandDrjx2l23UrxExsPbaa5fyPGDj6sADD8z6/v73v6f21Vdf3eX3NOqxbQrU\n9ddfnx1nd3X16S9f/OIXU3v77bfP+jrLpTZi189GxMCaa64ZO1OO/KThrbfeSm1fhrTetKR6+bQn\nmybqX1+b1mjTkMpYzrDZGhED6667binPA3anXF+K26by2D94+LSQsWPHpraP09GjR1fts7/nnnzy\nydS+8847s+NmzZqV2u3ambkRMbD++uuXMgYaodYO7fbn7nf1tjFgz4XtVNcZLoSwojoC4vcxxksr\nX34uhDCq0j9K0sLmDBFlQAyAGAAxAGIAxMDAUk/VoyDpN5IeiDGebrqukDS50p4s6XL/vegfiAEQ\nAyAGQAyAGBh46rkOvbOkwyTdE0KYW/nadyWdKumiEMKXJc2TdFBzhti/7LDDDtntqVOntmkk3dKS\nGLCX3J577rmsz17aHT58eNXvs7vt+rSCRqWJ9FYfTT8bUOcBHyu2spZNlfGpCY1+bJuGJOWpSLY6\nkpRf3v70pz+d9XWmO/jL3N3UsBiIMab3p69wZlM/bFvKx19vWpK/7F8rJcD+bH1KR7X1VbXSl/qh\nPnke8HFkY8WvmXr++edT21cNsjvq2vv0sWGrC912221Z31lnnVX1++x92lhfeeWVs+NsCpH/fWh3\nX27Szsx9MgZazZ5n/M/Z7qzt04sef/zx1Lafe9p5XlnuRCHGeIukarXm9qrydfQjxACIARADIAZA\nDAw8/fpPHwAAAAB6hokCAAAAgILe18rDcvXznNWGsbm+8+fPz/psWcd11lkn67P5yM8++2xq+3KJ\nPpcTqJfNF7btF154oer3+BK848ePT+3dd98967O7iNvY/+AHP5gdZ9dKzJgxI+uz6xf8Y3/0ox+V\nVJ5zUYwx5WP7HP9a6wSsetcv+PuvpdaxNs+4O/eJ8rG/T3ze/tprr53aY8aMyfq23nrr1Lb5//79\nVu18sTz17jNkf+f598HixYtTu0k7M6MLvpyyXV9gd+qWpNVXXz21//a3v2V9/udZBuX4rQEAAACg\nVJgoAAAAACgg9agJ7O6pkvS5z32uTSPpW2zKgd9V2ZaMXG+99bI+e3nVpoIMHTo0O64nO7cCPbVg\nwYLs9r333pvadndWSZowYUJq33///antywQ/9dRTqb3HHntkfTfffHNqz507N+sbN26cpHKmzNTa\nmdmz46+VllRLtRKUXr27w9dKnfKlkHs6ZnSfTbMbOXJk1mfTi/x7cc0110xtn/o3ZMiQ1LYlS335\n0p6mHtnYfO2111Lbvu+l/Bzx4IMPZn32nOFTooi/5vHnsY022ii1995776zPlra2Jd2lPG2yLKXU\nuaIAAAAAoICJAgAAAIACJgoAAAAAClij0ARTp06teRtds+sSXnzxxazP5pj6EnLPP/98atu8VJtH\n3Ap2TcTYsWOzPlv+0j+3J554our3odx8Xqpdh3DBBRdkfZdccklq77jjjlnfQQcdlNo2p9muXZCW\nrTWQpMcffzzrs++ROXPmZH3PPPOMpPKU3nvvvff05ptvSiq+hvb97ftq5ez68oTVvl6rBKU9tnN8\nXT12rTHWWqNgz09+XHatQ1nK2PZl9vW1JUP9bfueBXpq2LBh2W37u3zatGlZ37XXXpvavmx7CNU2\nvW4fzkYAAAAACpgoAAAAACgg9QgtNWjQoJQSNGrUqKzPlnLbdtttsz67s2FZd1jeeeedU9uXVnz4\n4YdT+4EHHsj6tt9++9TeddddU9vu5Nsdfufqzp1+Y4w9uj9U52N4yy23TG1fmtCWSz377LOzPrvL\n8qqrrpraG2ywQXac3SnWlmqU8t0+N9lkk6yvs+RoWWLgrbfe0iOPPCJJ+stf/pL12dQdn4JTK/Wo\nWulHfym/1mtgj7U7q3r2Z+vLLve0DLN9bjbtyadA2RQln0pmxzxv3rwux2XLbrbTe++9pyVLlkjK\n0y+l4q7j6JkRI0Zkt8uW2vrOO++kcq6+hO0WW2zRjiH1mC2rK+UleG05W0k64IADUtunSDc67XCr\nrbbKbm+zzTbdvg+uKAAAAAAoYKIAAAAAoICJAgAAAIAC1iigpYYMGZLy8H1Oos379Xn2fYEte+rX\nIdjbG264YdZn1zb0dF2C5cv9debG+1xn9J7PAf7MZz5T9VgbHzNnzsz6VlllldS2ZR0fe+yx7Lin\nn346tT/0oQ9lfb6Uapm9+eabevDBByVJTz31VJtH0zzNLnNarSSsVFwj07lGwZdzbZelS5em94SP\nc79mAT3j1/T43z3tFkLQCit0fAy167skafTo0e0YUsMsWrQotf252t9uJr92wpdnrwdXFAAAAAAU\nMFEAAAAAUBBaWS5v2223jbNnz27Z42GZEMKcGOO2yz+yuYiB9iEGQAyAGIBUjjggBtqr3hho6UQh\nhLBI0jxJIyQ937IHrm4gjWO9GGPrEuOqIAaqIgbaZyCNgxjo2kAaBzHQtYE2jrbHATFQValioKUT\nhfSgIcxu90yWcbRXWZ4z42ifsjxnxtE+ZXnOjKN9yvKcGUf7lOU5M46usUYBAAAAQAETBQAAAAAF\n7ZooTGnT43qMo33K8pwZR/uU5TkzjvYpy3NmHO1TlufMONqnLM+ZcXShLWsUAAAAAJQbqUcAAAAA\nClo6UQghTAohPBRCeDSEcHwLH/fcEMLCEMK95mvDQwjTQwiPVP4f1oJxjA4hzAgh3B9CuC+E8I12\njaVdiAFioF0xUHnstscBMUAMEAPEADHQgc8E5Y+Dlk0UQgiDJJ0paV9Jm0g6OISwSYse/jxJk9zX\njpd0fYxxI0nXV24321JJ34oxbiJpB0lHV16Ddoyl5YgBScRAO2NAKkccEAPEADFADAzoGJDaHgfn\nqf0xIPWFOIgxtuSfpB0lXWNunyDphBY+/hhJ95rbD0kaVWmPkvRQq8ZixnC5pIllGAsxQAwMhBgo\nYxwQA8QAMUAMDLQYKEMclC0GyhoHrUw9WlvS0+b2/MrX2mVkjHFBpf2spJGtfPAQwhhJW0m6vd1j\naSFiwCAGJLU/BqQ2vvbEgCRiYIyIAWJg4MWAVL444DNBF1jMLCl2TNlaVv4phDBE0p8kHRNjfKWd\nY0EHYgBSa197YqCciAEQA+AzwTKtnCg8I2m0ub1O5Wvt8lwIYZQkVf5f2IoHDSGsqI5g+H2M8dJ2\njqUNiAERAypXDEhteO2JAWKAGCAGBngMSOWLAz4TdKGVE4VZkjYKIYwNIawk6fOSrmjh43tXSJpc\naU9WR15YU4UQgqTfSHogxnh6O8fSJsQAMVC2GJBa/NoTA8QAMUAMEAOSyhcHfCboSosXaewn6WFJ\nj0k6sYWP+wdJCyS9o44cuC9LWl0dK8kfkXSdpOEtGMcu6rh8dLekuZV/+7VjLO36RwwQA+2KgbLE\nATFADBADxAAx0N44KEMM9JU4YGdmAAAAAAUsZgYAAABQwEQBAAAAQAETBQAAAAAFTBQAAAAAFDBR\nAAAAAFDARAEAAABAARMFAAAAAAVMFAAAAAAUMFEAAAAAUMBEAQAAAEABEwUAAAAABUwUAAAAABQw\nUQAAAABQwEQBAAAAQAETBQAAAAAFTBQAAAAAFDBRAAAAAFDARAEAAABAARMFAAAAAAVMFAAAAAAU\nMFEAAAAAUMBEAQAAAEABEwUAAAAABUwUAAAAABQwUQAAAABQwEQBAAAAQAETBQAAAAAFTBQAAAAA\nFDBRAAAAAFDARAEAAABAARMFAAAAAAVMFAAAAAAUMFEAAAAAUMBEAQAAAEABEwUAAAAABUwUAAAA\nABQwUQAAAABQwEQBAAAAQAETBQAAAAAFTBQAAAAAFDBRAAAAAFDARAEAAABAARMFAAAAAAVMFAAA\nAAAUMFEAAAAAUMBEAQAAAEABEwUAAAAABUwUAAAAABQwUQAAAABQwEQBAAAAQAETBQAAAAAFTBQA\nAAAAFDBRAAAAAFDARAEAAABAARMFAAAAAAVMFAAAAAAUMFEAAAAAUMBEAQAAAEABEwUAAAAABUwU\nAAAAABQwUQAAAABQwEQBAAAAQAETBQAAAAAFTBQAAAAAFDBRAAAAAFDARAEAAABAARMFAAAAAAX9\nbqIQQtg4hDA3hPBqCOFfQgj/E0I4qdK3ewhhfrvHiOYiBkAMgBgAMQBioPf63URB0nckzYgxDo0x\n/leM8Z9jjD/o6sAQwpMhhI818sFDCF8LIcwOIbwVQjjP9a0UQrik8rgxhLB7Ix8bSZljYIcQwvQQ\nwoshhEUhhItDCKMa+fiQVO4Y2KTSt7jy77oQwiaNfHxIKnEMuONOrvw+aOjjQ1KJYyCEMKbyc3/N\n/DupkY8PSSWOgUr/B0II/x1CeD6E8HII4aZGPn4j9MeJwnqS7mv2g4QOXb1+/5D0Q0nnVvnWWyQd\nKunZZo0NpY6BYZKmSBqjjnG+Kmlqs8Y4gJU5Bv4h6bOShksaIekKSRc2bZADV5ljoPN7N5D0OUkL\nmjS8ga70MSBptRjjkMq/Lj/AolfKHgNT1PG7YHzl/39tzgh7rl9NFEIIf5O0h6RfVWbnHw4hnBdC\n+GEXx/5W0rqSrqwc+53K13cIIdwaQngphPB/9q/+IYQbQgg/CiHMlPSGpPX9/cYYL40xXibphS76\n3o4xnhFjvEXSuw162jD6QAz8NcZ4cYzxlRjjG5J+JWnnxjx7SH0iBl6KMT4ZY4ySgjrOBRs25MlD\nUvljwDhT0nGS3u7F00UX+lAMoEnKHgMhhHGSPiHpyBjjohjjuzHGOY159o3TryYKMcY9Jd0s6WuV\n2fnDNY49TNJTkg6oHHtaCGFtSVepY/Y3XNKxkv4UQviQ+dbDJB0paaikeU16KuihPhgDu6kFf+0Y\nSPpKDIQQXpL0pqRfSvpxT+4DXesLMRBC+Jykt2KM07r7vVi+vhADFfNCCPNDCFNDCCN6eB/oQh+I\nge0r3/PvoSP16J4Qwme6eR9N168mCg1wqKRpMcZpMcb3YozTJc2WtJ855rwY430xxqUxxnfaM0w0\nUctiIIQwQdLJkr7duyGjwVoSAzHG1SStKulrku7q9ajRSE2NgRDCUHVMDr/RuCGjwZp9Hnhe0nbq\nSI3ZRh0fNH/fiIGjYZodA+tI2kzSy5LWUsfvgvNDCOMbMfhGYaKQW0/S5yqXmF6q/MVvF0l2senT\n7RkaWqQlMRBC2FDSXyV9I8Z4c2/vDw3VsvNAjPF1Sf8j6YIQwhqNuE80RLNj4BRJv40xPtmL+0Bz\nNTUGYoyvxRhnVz5gPqeOD4l7VyaRKIdmnweWSHpH0g8rqek3Spohae9e3GfDrdDuAbRZdLefVsfJ\n+yvd+B70bS2PgRDCepKuk/SDGONve3NfaIh2nwfeJ+kDktaWtLCB94v6tToG9pK0Tgjh/1Vuf0jS\nRSGEn8YYf9qL+0XPtfs80Hlf/AG3fVodA3c3+P6aYqAH5HPKF5/8TtIBIYR9QgiDQgiDQ0ed3XXq\nvcMQwgohhMGSBknqvI8VTP/7K/2StFKlPzTiyaBHWhoDlZzHv0n6VYzxfxr4PNBzrY6BiSGErSr3\n/UFJp0taLOmBxj0ldFOrfxfspY6Ugy0r//4h6avqWNyM9mj1eeAjoaPG//tCCKtL+i9JN8QYX27g\nc0L3tPo8cJM61kWcUDluZ3Usvr6mMU+nMQb6ROEnkr5XuaR0bIzxaUkHSvqupEXqmE1+W917nb6n\njstJx6sjv21J5WudHqp8bW11BMMSdVzeQnu0OgaOUMeJ6JRg6mc35qmgh1odA6tJ+oM68lIfk7SB\npEkxxjcb8FzQMy2NgRjjCzHGZzv/qaPy1eIYI+eC9mn1eWB9SVero0T2vZLeknRwA54Heq7V54F3\nKve/nzp+H5wt6Ysxxgcb83QaI3RU6AMAAACAZQb6FQUAAAAAXWCiAAAAAKCAiQIAAACAgl5NFEII\nk0IID4UQHg0hHN+oQaHvIAZADEAiDkAMgBjoj3q8mDmEMEjSw5ImSpovaZakg2OM91f7nhVXXDEO\nHtxRGXSFFfItHFhU3RivvZYXzXj33XdTO8bY0DKsPYmBD3zgA3G11VaTJL333ntZ36uvvtrI4Q1Y\n/r30vvd1/D3grbfe0jvvvNP2GHj/+98fV1llleXe99tvv53dtueMzufUacUVV0ztpUuXpnZZzyv1\nPjf7vKTeP7fXX39db731VsPLMXc3DgYPHhyHDBkiqfizXLJkSaOHNyB1vr6dOs+3r7zyipYsWdL2\nGFhhhRWij+9ONs59fKy00kqp/c47+Ua49j1h79vfx5tvLisw5quT2/eif4/ZY2u9/+xxtaqf2+fp\n79N+3/vf//6q3+fPJfa5rrzyylmf/Z376quvPh9j/FDVwfVAd2MghFDOE/TAUVcM9GbDte0lPRpj\nfFySQggXqqPMU9UPCIMHD9a2224rSRo2bFjWZz/Qouduuumm7PZLL73UzIfrdgysttpqOuKIIyRJ\nb7zxRtZ34403Nm2gA4n/5fOBD3xAkvR///d/zXi4bsfAKqusookTJy73jp955pns9vDhw1Pb/wJc\na621UvuFF15I7bfeemu5j9MOtZ6b/VCwzjp5ue7ePrfp06d3+3vq1K04GDJkiPbff39JxZ/lvffe\n26wxDii77rprdrvzj0h//OMfm/WQ3YqBFVdcUWPGjOnyjp5//vnU7jx/dbLf89xzz2V9duKw5ppr\npraPsQcfXFZ9ctCgQVnfhz607HOT/1xij631mcUe5/8oavvs+1nKJ8l2zGPHjs2Os6/Pk08+mfV9\n8IMfTO3NN98863vllVdSe8aMGfOqPoGe6/bvA7RVXTHQm4nC2sq3rp4v6SP+oBDCkZKOlDre8Gus\nsYYk6fHHH8+O23LLLXsxFFSz9dZbS8pPjA3U7RgYMWKEJkyYICn/q44k3XDDDc0Y44Azd+7c7Hbn\nB0//l6cG6dF5oB7+F+AjjzyS2v58sdlmm6W2/dBd1g+d/vxX7bnZ5yJJq6++emrfc889TRpdjyw3\nDmwMDBkyJP3Fd4sttsjuyH7A6yn7IcvHW+dV7eU9VueVz674v9Zb/q+/lv/LtmX/2ltrEmivGvsx\n2nPoJz7xiazvggsukFT84NpA3YoBOw77wVeSNtlkk9T+9Kc/nfXNmTMntf0fHDv/ECnlVx6uvPLK\n7LjO30GS9LGPfSzrq/XXevsXfzsp8VfHLf8HsQceWLavop/A2Odtz2n2OUvS/PnzU/uzn/1s1ft4\n8cUXs77rrruu6jgbpFsxgL6h6YuZY4xTYozbxhi3rXUCRf9lY8D+tQMDB+cB2BiwH9YxcNgYaOKE\nBSVmY6DdY0F9ejNReEbSaHN7ncrXMHAQAyAGIBEHIAZADPRLvZkozJK0UQhhbAhhJUmfl3RFY4aF\nPoIYADEAiTgAMQBioF/q8bW/GOPSEMLXJF0jaZCkc2OM99X6nlVXXTUtYHviiSeyvu9973s9HQoM\nXwVi/fXXlyQ9++yzDX+snsTASiutpPXWW0+SNHTo0KzP56JW45+jTWXZeOONU7tWZRyfA2zzRmvd\n/zbbbFN1XDYn1ue2zpo1q+r3NZpfZNdZTcrmtTZKT2KgXn4h/kYbbZTaX/jCF7K+vfbaK7Ufe+yx\n1C7rGgW7qFDKFyva5/bxj388O86uNSrTGoXuxsGKK66oUaNGSZImTZqU9R177LGp7d9v9n1aS608\n89dffz21ff64ve2LAtjb9hzhzxc2X93nrte7ANaeu2othvVVg+xYfPGCzoWz/jk3SndjYOnSpVq8\neLGk4qLb73znO6l9yy23ZH3/+Mc/Unvy5MlZn12I3LkmQyquKTn++GVVO/2CYvsz8z8v+3uj1s/Z\nriO5++67q45xu+22y/rs+oL771+2/tevMTzssMNSe6eddsr67ILoGTNmZH3NTvts5u8DtE+vkgRj\njNMkTWvQWNAHEQMgBiARByAGQAz0R+zMDAAAAKCAsgMoDXuZ11dFGTlyZGrPnDmz6vfZy+o+tWnf\nffdNbZsmJOWX+i+99NKsr1rZQn8Z114G9ykBdoMxXxb2tttuE5bPp4JYNl58XfSyeOqpp1Lbl7W0\nJTw70wWlYvpVf9xv5uabb85uz5u3rLS3TROS8rSNWmpthmXvw7+HbZpPrTSNWpt52bKWnk2x8d9n\n02Nsapr/mduUFF/61b52W221Vdb3y1/+suq42mGllVZK+4R86Utfyvpuv/321P7DH/6Q9X31q19N\nbb+/wPnnn5/atuTqKaeckh1ny4b6c3W9+5PYn5//Hvte97+H9thjj9T28WxTsm266uGHH54dZ1OU\n/GPXKsEL9AQRBQAAAKCAiQIAAACAAiYKAAAAAApYo4DSsGXvbJ62JG266aapfdddd2V9neU/Jeny\nyy9Pbb8W4NFHH03tQw89NOuz+bwLFizI+m666abUPuuss1Lbl0WcO3duats1FZJ00kknpbZff2Hz\njO19+tztgaAnefxSnstf1jx+G382f1qS9txzz9S2ederr756dtwNN9zQnMG1UWe55E5f+cpXUnu/\n/fbL+n7zm9+k9sorr5z12VK4Nu/c56BbPr/b3vbrI+plY9iXJLZxaktoSsXzSSf/PO1ahmHDhmV9\nNj/djqOMhgwZot12201S8Zx+5plnpvYhhxyS9dlyuuecc07W9/DDD6f2L37xi9T25cHtWpRaa59q\nsWvX/Fqi4cOHp/a4ceOyPhuP9neeJD3++OOp/alPfSq1/VoMGyt+TYJdW+PPhdViDKiFKwoAAAAA\nCpgoAAAAACgg9ajN6i1l5nd+7I/sZXpfNs5eyu28XN3J7tZqy5Deeeed2XHXXnttavtLsv/0T/+U\n2hMnTsz6xo8fn9q77757av/tb3+rev/+Eu/TTz+d2qNHj876TjvttC7HNRBTj3qSniPlKTplTc+x\n6Qk2lU5S2rFektZcc83U9jsRT58+vUmjax9b0lPKS1z6nWXtbbs7bZm8/PLLqe1Tx+xO07vuumvW\nZ3cIPvDAA1N70aJF2XF253GfemRLO/sde8uWivT222+nMf35z3/O+vbee+/UPvnkk7O+KVOmpLbf\nfdqmLD333HOpXWsH656y9+HL1K699tpVH8unKVk2rapz53Ip321Zyj8P+M8QtrSu3wEe5WfL59rS\nwPazh1TcrbuZuKIAAAAAoICJAgAAAIACJgoAAAAACvrEGoVaefwDIXd/oHjxxRdT25a5k/Ic0+22\n2y7rs3nctizp1ltvXfWxfB67LVW4zjrrZH1HH310am+wwQapbdcuSHkOvc8lt2Vbfb7sF7/4xdS2\naywGolp5/EcddVRqDxo0KOu77777Urssefw+J9yuu/ElL+26FZuT7st39kdrrbVWdtvmp++1115Z\nn8279+uYpk2b1oTRdd/mm2+e2v5c8sEPfjC1bRlLSbr++utTe86cOal93XXXZcfZ34f+tbPnlrL/\nbnz11VfTc7Y/c0k6++yzU3vq1KlZn31/+z6bk29f32aXBV1jjTWqjsM/tl2D5EtA29K39r3v78Oe\nP/waFrs2w8eYP2/2Vbvssktq+/OAX+/S13zkIx9J7VmzZrVxJMtwRQEAAABAARMFAAAAAAV9IvUI\nA4O91PrQQw9lfTYlxV6+r8XvjmxTkXyJupkzZ3bZlvLyjXYX2WOOOSY7brPNNkvtDTfcMOuzZT/t\nTtJS9RQBWx5PytMYynJJspU22mij1B4xYkTW99e//rXVw1ku+zOX8nKv++yzT9ZX7bmV8Xk1mt+d\n1pYd9iWIbVqFT8m56KKLUnuHHXZIbbtjcyvYUsj33HNP1vfEE0+ktk+vXHfddVP7lFNOSW2byiTl\n5zH7PZJR6P8SAAAgAElEQVR0+umnp/YBBxyQ9f3yl79c3tBbavDgwdp4440lSd///vezPrsD92WX\nXZb12XQjf+60qVfNTtuzaUK+DKktZ2rf21KeDuTLntpUJHucTxmy6Ub+d1mt9Lb+wpYQ/fCHP5z1\n9bXUI59ab0t/+/d3u3BFAQAAAEABEwUAAAAABaQetRg7MVdndyX26Tn2Eu0KK/QsbG0qkk9Lspd8\nbfUlKa+QZCuT+JSJ448/PrX95VB7mXr77bfP+mw6hd2Vdty4cdlxO+64o6rpq6lI3akMZH9GPp3k\nmmuuacLoesenBNgqTvvuu2/WV+25lfF5NZpPrbE7Fvv0mTvuuCO1ly5dmvXZ6me2OplPVbTv76FD\nh2Z9PhWkJ958882qff/6r/+a2n5nVZs2dPXVV6f2lVdemR1nd2z3u1rb8ftzaNmsttpqaQfqc889\nN+u76667UtvuxCzl5wVfDaje36/18ik/NrXJvr9t1TwpP3f7+7BpSn7nbrvDs/19uHjx4uw4m1Y1\nfPjwqn3+9bH32ZdNnjw5tf/+97+3cSS95yuXHXHEEan9u9/9LrVbuROzxxUFAAAAAAVMFAAAAAAU\nMFEAAAAAUMAaBUC11y+suuqqqX3bbbelts0jlvKSqPPnz8/6bD61zUGU8lzXSy65JLU32WST7Ljd\ndtsttf06B+viiy+u2lc23Skh2hfYNRd+x1CbW213Ypbycoe1ctz7I59/bdf++J2ZP/GJT6S23+Hc\n7mq+zTbbpPYVV1yRHff222+ntl9HMmbMmNT2+eM2F9z+bP36oEmTJqX2GWeckfXZ0oe//vWvsz67\ne/uZZ56Z2n7nXVse9LHHHsv67O399tsv6zv55JNVJi+//HI6h/r1Jvb5+xx/uw6hGTsN2/v3JVar\n7Zrud+C26+h86VR7/35nZsvef61Y9zHsH8/qLzszN3otSjudc845Vfv878d26T+vNgAAAICGYaIA\nAAAAoGC5qUchhHMl7S9pYYxxs8rXhkv6o6Qxkp6UdFCMcXG1++iu/nRZqT9oVQzYy6L+UqtlUwf8\nsXZH1pdffjk77u677+7RuGz5Mnv/vgSqvUxoS6pKedk7z6ahjB8/PrUXLlyYHfeDH/wgtX1awcSJ\nE6vef6M0Iw66U0K0L7AxYNOopDyVyu/W2ld2Y25GDPjdY+372afpfexjH0ttX/rRsu9Tf/ne7mzs\nS/BOnz49tX3549/+9repbcth+t9X9vawYcOyvttvvz21t91226zPpjPZErlf//rXs+PsDtR+F3lb\navmRRx5RMzQqBmKMKeXoxBNPzPpef/311PblbRvBlh/3Pz8bV/78FEJIbbtrrh+jTaXycWq/r9bv\nBcuP0Y7Ll/S1KXO+zHqjdqtu9efCCRMmZLd9enBf5ks0W/Z81E71fCI/T9Ik97XjJV0fY9xI0vWV\n2+i/zhMxAOIAxACIARADA8pyJwoxxpskvei+fKCkzp2hzpf0yQaPCyVCDEAiDkAMgBgAMTDQ9DTH\nZ2SMcUGl/aykqteBQghHhhBmhxBml32nSHRLj2LAX8pFn1dXHNgYaNTlb5RGt2PgjTfeaN3o0Ard\njoFaqWPok7odA60bGnqj1+VRY4wxhBBr9E+RNEWSxo4dW/W4/qy/r7noTgyMHz8+HefXGtj8zzXX\nXDPrsx8s7LbmUp7P+qUvfanqOO3PYe7cuVWPq8WuV/Bbr1sPPPBAdrvWJNnms9qyjqNGjcqO+9nP\nfpbavsydfQ3apVYc2BgYPnx4nzsPVCt76vPrba6yLbUp5WsufFlEm5N+zTXX9Gqs7VRvDKy11lrp\nmCFDhmTH2RLBhxxySNZnX+/rrrsu67N5zDfffHNq/+lPf8qOO/vss1Pbn5vtJNbmkkvSsccem9p2\nbYNfS/Pwww+n9i9+8Yusz66XuOuuu7I++9636zTGjRuXHWfzzu+4446s75RTTklt/9wuvPBCtUK9\nMTBy5MjY+Zz9H5Dsz7nWGhZfVrVe9j78Hy7sbb/2wJYlrbW+wJYo9e/1WmsuVlpppdS2v/N8yVM7\nybJle6V8ncqLL+Z/9LdlW5up3hio9bnB8mvyBg8e3NshtpVdY2FLJnt+nVS79PQT7HMhhFGSVPl/\n4XKOR/9DDEAiDkAMgBgAMdBv9XSicIWkyZX2ZEmX1zgW/RMxAIk4ADEAYgDEQL9VT3nUP0jaXdKI\nEMJ8Sf8m6VRJF4UQvixpnqSDmjnIgcKXMiuLZsWAL19qL7v6FCJ7Gd3vtGovQ9oye0cffXR2XLVL\n+54va/nEE09UPbZedow+P9umUtl0o69+9avZcTblxZd6ffDBB3s9xuUp07kgxupXrGvtjuzTGOpl\nd0u2JS5tOVcpTwPYbLPNevRYZdaMGPBpFTatZ++998767r///tS2KUpSXmbwox/9aGp/8pP5msrD\nDjus6lhsvPj36Zw5c1Lbvt98WoQtnfrkk09mfXbHYV/21J6T7HnshRdeyI6z6SQ+z//3v/99am++\n+eZqhkbFQIwxjd8/D5ue49+z9vfku+++W/X+6/196n/O9rF9elG184ePYVsW15fytKl2fow2Hdfe\np0+xsnE6f/78rM/e9mlP8+bN63L83dXq3wW+FLl13333NephWubnP/95avv4sKmLZVnXu9yJQozx\n4CpdezV4LCgpYgAScQBiAMQAiIGBpn+vsgUAAADQI0wUAAAAABS0plbWANDfS6A2g83Fk/KydLvu\numvVvkmT8g0hb7311tS++OKLU9vmlUt5vuZGG22U9dkShxtuuGHWZ/NUa+VD2jxbm6MqSQcdtCxd\n08fKpZdemto2r9jnPtvbvqSazWWcOXNm1TH2ZXbtwZIlS7I+m+tbbT2BVFxTUC9bVtCuHdluu+2y\n4x566KHU9usjRo8endqLFi3K+nysDmR2nY5f52HLnto8X0naf//9U9uWC7Z5+5J01llnpbY9d0j5\n+ghbslaSFixY0OVxO+ywQ3bct7/97dTeYIMNsj5b7tCuV5Ck3/72t6ltS7NutdVW2XE2B92vQ7Df\n598jZRNjTO9bXyLXrgXw6xBqrS+zav1OtvHhz9W2BKpXrayqL2Vtn0+tNRZ+jHZ9i30sPya7TsWX\nll1nnXVSuyzlNZvJn+PbZdVVV81u288phx56aNY3ceLEqvfzox/9KLUXL17coNH1Dp9uAQAAABQw\nUQAAAABQMGBSj0gNKh+7g6SUX0K1l/ml/JL6F77whar3aS/V2VQVSbrppptS2+/GaVOKTjjhhKxv\n5513Tm1bysyXtrOpCj4d4VOf+lSXY5TyVCc75h/84AfZceutt15q2/QGqfh8+iOb1uNTwGwJSZsm\n5NO3bIqZTQWS8rK4tdI2Nt5449T2ZR0feeSRLtuS9PGPfzy1R4wYkfX99a9/rfp4A5n/OVx99dWp\n7Xc2tu9b+z7yqR+1yiTbMozHHXdc1bHYlCi/Q7t9f/u0p6lTp6a23elZkg444IDUtucZn1507bXX\nprZPj7KlnP3u9mXzvve9L/1serpjsC8bas/J9ufu36c2VcinPdXLxo4fvx2XPzfbzyK1PpfYPp9e\nZFOUfHlN+9hlLbneSKuvvnqPvm/LLbes2rfXXsuKN9lULikvdX7wwcuKPw0aNCg7zqa3zZo1K+uz\nqaY+BmbPnl1r2G3Bp2cAAAAABUwUAAAAABT0+dSjMqYU+ct9tcY4EC4N9oTfedimavi0nnqrYNhK\nEdOmTcv6rrrqqtS2lxal/DKkHYf/udqqB/6yv02FsDvIStK4ceNS215GvuGGG7Ljrr/++i6Pk/L0\nqP7KXsr1MWDTgeqt9uJ3Xf3IRz6S2j7+LJvGcMYZZ2R9NtXk8MMPz/ps1S2fMnLNNdcsf8ADkH1f\neptsskl226YK2XQPf5zl04bsLr233XZb1mertJ1++ump7c8XtgKZr6r0wx/+MLX9rrl2/LYqm9/d\n2fLnAV9pq+w6U3b870F7TvcpHfb951N+bDWqWmxaUq3fzz61yR5rx+HTSe3z8b+f7M7PtVKu7M7M\nPnXKxk6ttNMyfj7qiVrPccqUKdltu6t5LRMmTKjaZytt+Z27bdqrTSW0O7dL+e9vn2b99NNPp7b/\nPWR3fS+L/hFFAAAAABqKiQIAAACAAiYKAAAAAApKuUahOzn+zX7snugveYHN5nM3bS6g3aXU86Ul\nffmyarbffvvUtuVWvXPOOSe7fcstt3R5nM9ftbmGu+22W11j8uOyz8U/T7sDpS8NWW9ubtnUeq/4\nHVntsbXyzu+88866Htvn/f7lL39J7XrXDNhdtaV852dbihXV+Tz+PffcM7X9OcLuZux3V7c5wvff\nf39qV3v/SsV1CHa9gY8B+x6z79n1118/O87mj9c6N/nfNTYn/ZhjjkntBx54IDsuxpjafudqmzPt\ndwsuY+5zNTYn3een2/UFvrRpT35/+zVN9ufgS+tadtdj/7vAxopfB2PPY2+//XbVx7b3adc1+DH7\nNRx+LP3BUUcdld2eN29eavv1avWy93H55ZdnffY9588RPXHkkUdmt+1aIlvSuKz4RAsAAACggIkC\nAAAAgIJSph55lBDtn3x5Mpu64y/3+Uu09fCXYO3Oxt/5zneyvi996Uup7dMdbr755i7vf+7cudlt\ne5nallSV8vJo559/ftZny6PuuOOOXT6WlF9i9pfE/S7AfYW/bB5CSG2femR3UralUiVp/vz5qX32\n2Wc3cohNYdNHBjpfItK+VxYtWpT1/exnP6t6P3Y3d5v6sdVWW1X9Hv9etzHmd2jfaaedUtum8fi0\ngn//939P7a9//etZn32v+7S1SZMmpbZ9Da644orsOHtemzFjRtZnd+wt+zkhhJDe/z4F0ZektOyx\n9e5I7+/ffp9N95Hy2PHfZ19T+1r7FDObKuTP1fbzjH+e9nxox1Frd2f/+cimnNlSvVJeIrwvO/XU\nU9s9hG6x6ZSePw+UEVcUAAAAABQwUQAAAABQwEQBAAAAQEGfWKOA/smuGZDyvH6fm/zUU091+/4f\nffTR7LZdUzB8+PCs7+ijj07tb33rW1nffvvtl9qzZs1KbZ9Db/NB/foLm6d6ySWXZH221Kctq+pz\nq23pRp976tdL9BU+B7hW7v6HP/zh1PY/v+nTpzd2YGgZXz5y2rRpqe3L/p522mmp7fO77fvIlo+0\npQilPN/bl1h96aWXUtuXaLbrGWwZUj9+e+6yz0WSTj755C6Pk/J1WDbHfdiwYdlxG2ywQdX7sKVZ\nfQlhu46nDGKM6Rzq1xzZ5+xjwK4NqLVGwf68/HF2XYK//9VXXz21Z8+enfXZdTD29fVlWu353v+e\n8OtiLLtex5ZvrlVG2q+PsGvqrrvuuqzPxgfK4bLLLmv3EJaLKwoAAAAACpgoAAAAACgg9QilYVOR\nfFrStttu2+37sztnStLMmTNT26cE2MvDPvXIpgDZy8EXX3xx1cf24x86dGhqjx8/Put75plnUtuW\nf7QpT5K06667pnaty9e1xlU2/pK6fV7+kr29LO9TrerdSRnl51N5rCuvvDK1bQlRKU8nsakmvkyy\nve1LS9rbPjYvuuii1Lapb88//3x23LXXXpvaBx54YNZn4/vYY4/N+mwa0cEHH5zadpdmKX8f1Eq9\n6Um6ZivFGFOKmN8l3b72NtVIyn/OvryyfW1sGpkvgWpTu3zqjt3J25fptumxNo2nVmpQM9j3iN/Z\n/LjjjkvtbbbZpmVjQv/FFQUAAAAABUwUAAAAABQsd6IQQhgdQpgRQrg/hHBfCOEbla8PDyFMDyE8\nUvl/2PLuC30TMQBiAMQAiAEQAwNPPWsUlkr6VozxzhDCUElzQgjTJR0u6foY46khhOMlHS/puBr3\ng76rT8aAz3XeeeedU9vnntrcZ5+3fNRRR6X2xhtv3OtxjRs3Lrttc2mvvvrq1H7yySez42zf1772\ntayvEeNajqbEgM8xrpf/GaEl2n4esPnqvkTuZpttlto2Z7zWeh5fHtXmsu+9995Z32OPPZbac+bM\nSe1ddtklO84+tj13SHkuvj8/ffOb30xtW4bz2WefzY6zZWF96dQWaGgMdL7/a5VA9SVg7c/TlyW1\n61ZeeOGF1Pav9dixY1P7hhtuyPp+85vfpPZnPvOZrM+W2q0VV43m11HMmDEjtb/xjW9kfXvuuWdq\n2/eEVFzP0ENtPw/0dfYcseGGG2Z9DfoZNdRyryjEGBfEGO+stF+V9ICktSUdKOn8ymHnS/pkswaJ\n9iIGQAyAGAAxAGJg4OnWGoUQwhhJW0m6XdLIGGPnnz2elTSyyvccGUKYHUKY7TeJQt/T2xiwfz1H\n39TbGKhVqQV9Q29jwG+Whr6ntzFgN8VD39TbGGjJINFrdZdHDSEMkfQnScfEGF8JIaS+GGMMIXS5\npWqMcYqkKZI0duzY6tuuovQaEQPjx49vWwzYy8877LBD1mcvI19//fVZny1n6r+vJ0aPHp3dtuk3\nL774YmrbHTalPBXJ7uAsFS9fNksjYmD48OHpGF9W0N725VHtBMPuYovWakQMrLXWWj06D9RKtenJ\nHyFsaWIpL3Vq0xGlPOXnzTffTG2bEijlOyfbkqpSXsb3vvvuq/p9NsXF7kguSeuuu25qP/LII2qH\nRsTAyJEjY2c5U78zs/1Z+nOETTd64oknqn7fmDFjUtumGknSjTfemNonnHBC1vf5z38+tbfeeuuu\nnkZL2HQjO15JOvroo1N70qRJWZ99Pr/61a+yPl8mtjcaEQPVjunvbOpsT9NvW6muKwohhBXVERC/\njzFeWvnycyGEUZX+UZIWNmeIKANiAMQAiAEQAyAGBpZ6qh4FSb+R9ECM8XTTdYWkyZX2ZEmXN354\nKANiAMQAiAEQAyAGBp56Uo92lnSYpHtCCJ3boX5X0qmSLgohfFnSPEkHNWeIKIF+FwO+Coa9/Gd3\n95SkRx99NLV9GkCjx2Lb/pKkHccDDzyQ9Y0aNarh43KaEgO1djT1lY1sKlKMA/KKdbs1JQa22GKL\n7PYll1yS2ptvvnnWZysPNYJfK7H66quntt/V1qZtHHDAAantU55sZSNfrcbuCOx3UL/rrrtSe6ed\ndkptnxrTZg2LgRhjSif0KUT25+BfQ3usf+1tFSubbuTTOE888cTUtlWCJOmgg5YN3a+n+sAHPtDF\nM+k5f463Y77qqqtS+4gjjsiOmzhxYmqffPLJWZ9Nhbv99tuzPl8lqof63eeBdvLpzFOnTm3TSKpb\n7kQhxniLpFCle68qX0c/QgyAGAAxAGIAxMDAw87MAAAAAAqYKAAAAAAoqLs8KtCfjRy5rOSzLasn\nSSNGjEhtu06gFpunLOW5obvuumvW5/NnO/kyqnanUX//dkfSvqRWeVS/RsGWRPXPv4z8c2M36a7N\nnp2XU7ex7NcQ2PdiI9g1A96dd96Z3bY7JP/1r39NbVsqVZIWL16c2n73XhsTl1+er/W0uxHb85Et\nmezH0ejXo5XeeeedtOuyL3tr11z5dSl2p+ZNNtkk67NlZe0Ot6ecckp2nF2X8M///M9Zny3V6neM\nrpf9ufv3vT13+XO8LZ97+OGHp7ZdEyNJ3//+91P7iiuuyPp+9rOfpbZf44NyqLU2r4z61mgBAAAA\ntAQTBQAAAAAFpB4Bji/HZ3ecfPnll1N78ODB2XE2ZeLXv/511nfSSSeltk89smwaki9lZy9T+zJ9\n8+fPr3qffYm9TO93Zq52XF9RK61qIBs3blx225YlPeaYY7I++77yZQVteVFbJtOn5dm0niVLlmR9\ntiynTXGRpOHDh3d5nE9PsWkn9jhJWm211aqOy5b9tLs02+cl5c/79NNPz/rWWGON1PZpW2UTQkjl\nQX25alv+2aZaSXk5XfszkfLX6kc/+lFq77LLLtlxttyoj4H3v//9dY2/FltW1d/fxhtvnNp//vOf\ns77PfvazqX3ggQem9k9+8pPsOLtr+GmnnZb12fiwcSQV0/zQGjalTJI+97nPtWkkPcMVBQAAAAAF\nTBQAAAAAFDBRAAAAAFDAGoUmIP+4bxs6dGhdx2266abZbZtL+8c//jHre/XVV1P705/+dNZnc1Gr\nlUqV8nznFVbI37o+z7av6MxR7mSfl1+jYHOu21ke9ZlnnkltW45RyvPV11xzzaxvwYIFqV32/PFW\nevDBB7PbM2fO7LIt5SVyb7zxxqxv6dKlqW3XFXkxxtS2ueT+Pmxbys/r1dpS7bU19j79cXZNkl3L\nYMumSsW1UZZdX7XVVltVPa4MBg0alMrTPv7441mfPV9OmDAh67PrEu6+++6s76c//Wlq21x9W2pU\nyt9/fr1XvWqVuLT378tt23UJ/nfBpz71qdT++c9/ntqXXnppdpwtj+rXvH3pS19K7f/93//N+nwp\nX7TG1KlTa94uO64oAAAAAChgogAAAACgoKWpR6+99ppuuukmSdLcuXOzvq233rqVQ+m3XnnllXYP\noaYYY0ob8buW2t0y+4Ldd989u73XXnulti+tePXVV6f2Sy+9lPXZMonNeA06U6L8Dp5lYEtVStJK\nK61U9Vi7M7VNQWk1m67i00LsTqg+pcHuPjt9+vQmja5vGDJkSEqZ8OUjTz311NT2pYrt6+3LB9v3\nld1x2Z8T7eP59J9apTH9+apTrR24u1Nq06at2d2BfXqUTc/z929v2xKa9vtuu+22usfUTDHG9Prb\n5ytJ66+/fmr753/LLbek9lVXXZX17b333qltdzP2qX72dfJpjPbnWe8Ouv44Gyvnn39+1mdL2k6e\nPDnrO+yww1L73HPPTe3f//732XH2ee6xxx5Zn/1dc88992R9/j0D1IMrCgAAAAAKmCgAAAAAKGCi\nAAAAAKCgpWsU3nnnHS1cuFBSccv2zq+jsTrXgpSlHOOSJUtSSbsRI0ZkfT4fuex8XurYsWO7bEt5\nvPvc9fvuuy+1R40a1etxbbHFFl0+9nXXXdfr+260tddeO7ttSwn6MrV23ceTTz7ZzGHVZEui+tKE\ndo2Jf89deOGFqT3Q1yjYtUp+DcEdd9zRjiH1O7YcryStssoqkoplX9vl7bffTu9jfy596qmnqn7f\no48+mto+596uD7n44otTu9baJ19q2t5HtXUpni/zbEvw+ni25V19ad0zzjgjtW3ZYFsu13+fLbcq\nSf/4xz9S278+7Vzbhb6LKwoAAAAACpgoAAAAACgI9hJZ0x8shEWS5kkaIen5lj1wdQNpHOvFGD+0\n/MOaixioihhon4E0DmKgawNpHMRA1wbaONoeB8RAVaWKgZZOFNKDhjA7xrhtyx+YcZRGWZ4z42if\nsjxnxtE+ZXnOjKN9yvKcGUf7lOU5M46ukXoEAAAAoICJAgAAAICCdk0UprTpcT3G0T5lec6Mo33K\n8pwZR/uU5TkzjvYpy3NmHO1TlufMOLrQljUKAAAAAMqN1CMAAAAABUwUAAAAABS0dKIQQpgUQngo\nhPBoCOH4Fj7uuSGEhSGEe83XhocQpocQHqn8P6wF4xgdQpgRQrg/hHBfCOEb7RpLuxADxEC7YqDy\n2G2PA2KAGCAGiAFioAOfCcofBy2bKIQQBkk6U9K+kjaRdHAIYZMWPfx5kia5rx0v6foY40aSrq/c\nbralkr4VY9xE0g6Sjq68Bu0YS8sRA5KIgXbGgFSOOCAGiAFigBgY0DEgtT0OzlP7Y0DqC3EQY2zJ\nP0k7SrrG3D5B0gktfPwxku41tx+SNKrSHiXpoVaNxYzhckkTyzAWYoAYGAgxUMY4IAaIAWKAGBho\nMVCGOChbDJQ1DlqZerS2pKfN7fmVr7XLyBjjgkr7WUkjW/ngIYQxkraSdHu7x9JCxIBBDEhqfwxI\nbXztiQFJxMAYEQPEwMCLAal8ccBngi6wmFlS7JiytaxObAhhiKQ/STomxvhKO8eCDsQApNa+9sRA\nOREDIAbAZ4JlWjlReEbSaHN7ncrX2uW5EMIoSar8v7AVDxpCWFEdwfD7GOOl7RxLGxADIgZUrhiQ\n2vDaEwPEADFADAzwGJDKFwd8JuhCKycKsyRtFEIYG0JYSdLnJV3Rwsf3rpA0udKerI68sKYKIQRJ\nv5H0QIzx9HaOpU2IAWKgbDEgtfi1JwaIAWKAGCAGJJUvDvhM0JUWL9LYT9LDkh6TdGILH/cPkhZI\nekcdOXBflrS6OlaSPyLpOknDWzCOXdRx+ehuSXMr//Zrx1ja9Y8YIAbaFQNliQNigBggBogBYqC9\ncVCGGOgrcRAqAwUAAACAhMXMAAAAAAqYKAAAAAAoYKIAAAAAoICJAgAAAIACJgoAAAAACpgoAAAA\nAChgogAAAACggIkCAAAAgAImCgAAAAAKmCgAAAAAKGCiAAAAAKCAiQIAAACAAiYKAAAAAAqYKAAA\nAAAoYKIAAAAAoICJAgAAAIACJgoAAAAACpgoAAAAAChgogAAAACggIkCAAAAgAImCgAAAAAKmCgA\nAAAAKGCiAAAAAKCAiQIAAACAAiYKAAAAAAqYKAAAAAAoYKIAAAAAoICJAgAAAIACJgoAAAAACpgo\nAAAAAChgogAAAACggIkCAAAAgAImCgAAAAAKmCgAAAAAKGCiAAAAAKCAiQIAAACAAiYKAAAAAAqY\nKAAAAAAoYKIAAAAAoICJAgAAAIACJgoAAAAACpgoAAAAAChgogAAAACggIkCAAAAgAImCgAAAAAK\nmCgAAAAAKGCiAAAAAKCAiQIAAACAAiYKAAAAAAqYKAAAAAAoYKIAAAAAoICJAgAAAIACJgoAAAAA\nCpgoAAAAAChgogAAAACggIkCAAAAgAImCgAAAAAKmCgAAAAAKGCiAAAAAKCAiQIAAACAAiYKAAAA\nAAqYKAAAAAAoYKIAAAAAoICJAgAAAIACJgoAAAAACpgoAAAAACjodxOFEMLGIYS5IYRXQwj/EkL4\nnxDCSZW+3UMI89s9RjQXMQBiAMQAiAEQA73X7yYKkr4jaUaMcWiM8b9ijP8cY/xBVweGEJ4MIXys\nkclIARMAACAASURBVA8eQvhaCGF2COGtEMJ5ru+QEMJr5t8bIYQYQtimkWNAeWOg0n9QCOGByonr\n/hDCJxv5+JBU/hg4IoTwaOU8cHUIYa1GPj4ktTEGQgjvDyH8JoQwr/I+nxtC2Ncds1cI4cHK74EZ\nIYT1GvX4SEobAyGElUIIl1QeN4YQdm/UYyNT5hjYIYQwPYTwYghhUQjh4hDCqEY9fqP0x4nCepLu\na/aDhA5dvX7/kPRDSef6jhjj72OMQzr/Sfp/kh6XdGdzRzvglDYGQghrS/qdpG9K+qCkb0v63xDC\nGs0c6wBU5hjYXdKPJR0oabikJyT9oYnDHKjaGQMrSHpa0kclrSrpe5IuCiGMqXzPCEmXSjpJHTEw\nW9Ifmz3WAai0MVBxi6RDJT3b7DEOYGWOgWGSpkgaUxnnq5KmNnus3RZj7Df/JP1N0ruS3pT0mqQP\nSzpP0g8r/btLml9p/1bSe5KWVI79TuXrO0i6VdJLkv5P0u7m/m+Q9CNJMyvft2GNsfxQ0nnLGe8M\nSf/W7tetP/0rewxI+oikhe5riyTt2O7Xrr/86wMx8HNJZ5rba0mKkjZo92vXX/6VKQbM99wt6TOV\n9pGSbjV9q1TuZ1y7X7v+8q/sMeC+Pt/eN/8GXgxU+raW9Gq7Xzf/r19dUYgx7inpZklfix1/tX+4\nxrGHSXpK0gGVY0+r/LX3KnX8ch8u6VhJfwohfMh862HqOMkPlTSvp2OtXGbeTdIFPb0PFPWBGJgt\n6YEQwidCCIMqaUdvqePkgQboAzEgSaGL9mY9uB90oWwxEEIYqY4PKZ1/2dxUHR86OsfwuqTHKl9H\nA/SBGECT9cEY2K1GX9v0q4lCAxwqaVqMcVqM8b0Y43R1fLDbzxxzXozxvhjj0v/f3pmHSVFeXfy8\n7AiC4EIQUVHciYL7romicd/iGhQ1Lh/RT4lGxX1L3GJMNBp3BVE07uAWxRVXRA1GEBFcUUFRgQCf\nikB9f8zUy7m3u8uemZ7unuH8nsfHW9yq6urqW293TZ333CRJfmzAax0B4MUkST5qyAGLktOoNZAk\nySLU3ByOQM0NwggAx9f+UBDVQWOPA/8CcFAIYcMQQnsA56HmicIyJTl6UQpKVgMhhNYA7gIwLEmS\n92r/uSOAOW7VOaj5sSGqg8auAVH9lK0GQggboua74LTSvoWGoxsFy2oADgwhzE7/A7AtAJ5cMq1E\nr3UEgGEl2pcoHY1aA7UTpa5AzSPPNqjRLt4SQuhb/0MWJaZRayBJkqcBnA/gAQAf1/43FzXyA1Ed\nlKQGajXLwwEsAHAipeahZo4S0wk1dSCqg8auAVH9lKUGQgi9ATwB4OQkSV4syZGXkFaVPoAKk7jl\naQCGJ0lybB22qTMhhG1Qo0u+v6H7Eg2m3DXQF8CYJEneqF0eF0IYC2BnAOMbsF9Rf8o+DiRJch2A\n6wAghLA2aia5TWjIPkWDKHkNhBACgFsBdAOwu/tr40QAA2ndDgDWRBXKDpYiyl0Dovooew3UytCf\nBnBxkiTD637Ijc/S/kThSwBr0PKdAPYKIexaqx9vF2p8dlcpdochhFYhhHYAWgJI9+FvyAYCeCBJ\nEv31qPKUuwbGAdgufYIQQugHYDtojkIlKWsN1MZ9al0yVkWN68XVSZLMKuF7EnWj5DUA4HoA66FG\n8/ydyz0EoE8I4YDaOjkPwH8kS6ko5a6B1D6zXe1im9rXCH49UTbKWgO1cyCeBXBtkiQ3NPTgG4ul\n/UbhUgDn1D5S+kOSJNNQY1l4FmqcaKahRi9Wl/N0Dmpmvw9Bjb7tu9p/A1DzIwHAQZDsqFooaw0k\nSfICgAsA3B9CmIsa+cklSZI8VZq3I+pBuceBdqiZmzIPwOsAXkWNTaaoHCWtgdq/Eh6PmieIM8KS\n3jm/AYAkSWYCOAA1jimzUOOGdkiJ35OoG2WtgVomo2Zs6AHgydpY/TQqR7lr4BjU3JhcQLl5JX5P\nDSYkSYOVNEIIIYQQQohmxtL+REEIIYQQQgiRB90oCCGEEEIIIXLQjYIQQgghhBAihwbdKIQQfhVC\nmBxCmBpCGFKqgxJNB9WAUA0IQHUgVANCNdAcqfdk5hBCSwDvA+iPmkZB4wAcmiTJu6U7PFHNqAaE\nakAAqgOhGhCqgeZKQxqubQ5gapIkHwJACOEe1NhIFSyIdu3aJcsuW9Ohft486wD1/fffN+BQREqb\nNm3McocOHQAA8+fPxw8//FBqf+Y610DHjh2T5ZdfHgDQooV9oPXNN9+U+PCWThYtWmSWFy9eDAD4\n8ccfsXDhworXQIcOHZLllluuxIchimH27NmYP39+Y/i016kOsmqgbdu2eWMA+O9//xtj/53RuXPn\nvNvxNn67Tp1sc2Tebu5c2+bmhx9+yLtdKY4xazvexm+X9d7atWuHfFRTDXTp0gVA7ndBq1ZLfpr4\ntgK8nNVyoL7tCOqznd+G/wC7cOFCk+M6qiSff/7510mSrFji3dapBtq3b5+k19KPP9pedPqOKA2z\nZtn2PK1bt47xzJkzi6qBhtwo9IBtXf0ZarygDSGE4wAcBwAdO3bEvvvuCwB47bXXzHoTJqgpaSno\n3r27Wd5ii5qPZPTo0Y3xcnWuga5du2LIkJqnkcsss4xZb9gwtZYoBXPmzDHL//d//wcA+Pjjjxvj\n5epcA507d8agQYMa41jET3D99dc31q5/sg6KrYE11ljS72ittdYyuSeeeCLGU6ZMMbnddtst73a8\njd9u5513Nrk111wzxs8//7zJTZ06Nca77LJLSY8xazvexm+X9d7WWWcd5KNaamC55ZbDSSedBABo\n37692dEKK6wQY/5hA9ibCp/jH+z+j2aF8DcpfrkQLVu2zBsDwIIFC2L89ddfm9wHH3xQ1P4bm7PO\nOuuTRthtnWpg2WWXxWGHHQYA+OKLL8yO0t+KomHcd999ZrlHjx4xvvbaa4uqgYbcKBRFkiQ3oabz\nKFZaaaUk/WtI+pfuFB4YRP1Jn9ikpD/Gix38GgOuge7duyeffvopAPulDOT+JVzUj1VXXdUsv//+\n+xU6kiVwDfTo0UPNW5ZCiq2Bb7/9Nsb+5pb/Yu5/JBbazv9Vn7ebOHGiyc2YMSPG22yzjcnxj8F/\n/vOfMV5xRfsHue222y7G+++/v8ndeuutMb7uuutM7rjjjsu7HW/jtzv66KNNbr311ovxY489ZnK9\ne/dGpfE1UOiv6/7HNcNPVPz3Xc+ePWPMn9fs2bPNetOmLfkt65/sdO3aNca9evUqeByMf3rD+/Q3\nEV9++WWMvbJiaYBroGfPnslqq9X0l/O14K8dUT8++ugjs7z22mvH+Nprry1qHw359fg5gJ60vErt\nv4mlB9WAUA0IQHUgVANCNdAsaciNwjgAa4UQeoUQ2qCm/fyo0hyWaCKoBoRqQACqA6EaEKqBZkm9\npUdJkiwMIZwI4EkALQHcliTJxJ/YTDQjVANCNSAA1YFQDQjVQHOlQXMUkiR5HMDjxa7foUOHOLn2\noIMOMrmf//znBbdj7XqxDgJe787b+VzW/gvp5r3ukF0afM4vF7Nd1v69ppL58MMPzXKq1R0zZkzB\nbRpCXWtg8eLFcXKtdzno27dvjA855BCTu/fee2N8+OGHmxxPgua5GMWuBwD77bdfjB966CGT23PP\nPWP86KOPFjzGe+65J8bpe0zZddddY7z66qub3G233RZj/tyPOeYYs96NN95YcP/HH398jP3cjz/8\n4Q8ArDa2lNS1BkTzpC510K5duzgp10+6PeOMM2I8btw4k3vggQdivNlmm5ncAQcckHc7vi4BoF+/\nfjE+5ZRTTG7UqCV//PzZz35mcgceeGCMJ0+eHOPx48eb9XgSpv9+4vGZJ7wCQOoG57fzYzpv58eB\n7777LsY8XwHIHW8bg7rUwI8//hjHpM8/t+oU/t7155Dfs5+n0rFjxxizsYd30Jk5c2aMeV4KYN2u\n/PwTPvfs0sevCwD77LNP3hiwk6z9+/aT35sidamBtm3bxnkgvpb9Zyvqx2effWaWvdNbMagzsxBC\nCCGEECIH3SgIIYQQQgghcmh0e1QmSZL4GNE/TvRNIZj6yIayrDanT59ultk+0j+mKQXF2n6yLRtb\nWPllbwnHsqT58+ebXPo4L226VU2sssoqZvmFF16I8UorrWRy7D3tH1GeeuqpMT755JNj7H2Zeb20\nl0O+1/PWfJxj+c6IESPMeiz/GT58uMmxvMi/9sUXX5x3H97v/Morr4zx2WefbXJ/+ctfYnz11VdD\n5IflF942luUqWXJBJms88jmWLXjZiV8uBu8Tz8teCpHKUPz4UCkWLlwYx3wvrWGrSS+j4+Nn+U/W\ndt4elccStjIF7Od+6aWXmtw555wT4/POOy/GbJUKAL/73e9i7Mexv/3tbzHedtttTe6Xv/xljF98\n8cUY+14JbCV+7LHHmhyPfxtvvLHJpeNJtTQ3bdu2beyZkUqSU1ie6SWTjz++RNXi7XNZesXSNH8u\nWNYycuRIk+Ox20uR0wZxgK0xljIBwHvvvRfjnXbayeR4Oz5esXTiJYEsLeffyZWUYumJghBCCCGE\nECIH3SgIIYQQQgghctCNghBCCCGEECKHss5RmD9/Pt544w0AwO23325yrFP1ukDW4HsKWYVm2Yt6\nbS/vw+8v67WZrPkR/v0wvC5rR71mmXX5hSxhAaBbt25mOdVp/ve//y24TaXw53rOnDkx9vNI+H15\nXe4rr7wSY9YEn3baaWa91157Lcb+M7noooti7HWjb7/9dox5XsUjjzxi1mO71JNOOsnk2KrVHz/b\nsfbu3TvGQ4cONesNGjQoxqyRBoANNtggxr///e9NLrUifumll1ANLF68ONb6u+++a3L8WdZXx8/X\nt7/WGX+dsq7f60H5c/G2nIXwdTRp0qQYP/3005nHUgxZ8yM87du3B5BrC1wpZs2aFa1O/fy0+uj4\nAavl33zzzWN82GGHmfXefPPNvK8F2HkIf/zjH03uwgsvjPH5558f4wEDBpj1uKb//e9/mxyP4xMm\nTDA5nmOx/vrrx5jrBrC1eeaZZ5ocWx/ed999ebcLIaAaYKvsDh06mBxfb17DndrqArnz3Ph7g9//\ngw8+aNZju2o/h4DnR/gxh21W2c6b56ABwN133x1j/v4AgIEDB8Z4xx13NDmeg+PrQzRP/G9Vnk9a\n7Fy5xqY6vjWEEEIIIYQQVYVuFIQQQgghhBA5lFV61KJFiyg38bITfqSXZeXpH52zNCirOzLju0Dv\nsMMOMd56660Lblds92UvV+L36nO8HT9ifvjhh8163BHY24OyjMY/wk1fu1okB8XiO1auuuqqMfYW\nttyt9dxzz42xf6zLj6K9fSR3Cv/rX/9qch999FGM2fqQuzkDVhq0yy67mBxboPocy1AuueSSGHsr\nS3497v4KWBlG2ok55f777weQbUFcblKZjH+PWRaixcpzstZjWcFGG21kcjvvvHOMvZ1iKt3xx1WX\nLux8XF4Sxftk21YvUSoWlvEBS2Qt1dLttFWrVrETMVtOAvWT5wBWuskdbr3VNL/eLbfcYnIsc2EZ\nEmClSCw94msbAC6//PIYczd4wMpOvHUqWxz36NEjxl46xZLKDTfc0ORY4ug7S6fSSLahriTz5s2L\nUsMnn3zS5LhLNY/9gB0zWOIDAJ9++mmM2W6c9wfY70lvN77JJpvE+N577zW5q666Ksbcqdt/v/L4\n4e222dLVvze+PvkYp06datbjrtCi+cK/Fytpcd+0fj0KIYQQQgghyoJuFIQQQgghhBA56EZBCCGE\nEEIIkUNZ5ygss8wy2HTTTQHkaqx53kCWParPFbIKzbIo9Rp/3sf48eNN7oMPPojxjBkz8r7WT1Gs\ntpq1tHvuuafJXXDBBQX3x8fP+mZgiR7y2WefLeoYqgXWGwPWBo9tSAHgsssuizHXFdvoAVaz6ve/\n5ppr5o0BqyO94oorYuz13t27d48xW54Cdt7Ac889Z3JHHXVUjFm/6uuULVz9PAQ+J36ORaql9e+5\nUrRt2zbWOtsUAvbce336mDFjYpw1N6BPnz4x3mabbcx6Wdci55566imTYy30tGnTCu6Pl71V8Trr\nrBPjnj17Fjx+/tx57hZg39v2229vcjzP4YsvvjC5dJ7NyJEjUQ106dIlzrnx+m7W/LMdKmCvbz8e\n/+tf/4oxz0Hy87ZYk+6v4XvuuafgMbMlMc9RGDJkiFmPr2df32z76ecv8Fyrfv36xZhtWQE7jh1w\nwAEmx98FgwcPNrl0/kWSJKgGkiSJY5L/LMeOHRtjnvvl1/XzDfn88rXD4yoAjBgxIsZ33XWXybG1\nrofnhPB8Fj+n6bjjjovxDTfcYHL8ev765s+M69bPseA5Cn4OBP9mEdWPH/+qcT5p9R2REEIIIYQQ\nouLoRkEIIYQQQgiRQ1mlR0mSxEej3hbx22+/jbF/nM+P5bMe9Wd1R2beeecds8x2cWnn6HxkSaCy\nui9ndZRlCzu2xPM2qjNnziz4Wvxe/XlNZQyVtNYqBWyJ2qtXL5MbNWpUjN96662C+2BLSt+pmm31\nvESHbVW//PLLGHv5S2pDCuQ+SmeLQ2+JN2/evBizPa+30GQZhrfNPPTQQ2PMEh1gSddmtnmtJCGE\neO68TS3bkLLMBrAdkf255y7IfN6yrj2WEAG2g6q3aywkN8qycPUSAF72x8+SmhNPPDHGLFcC7Hvz\nFqhZ1qzp+amWcWDevHmxU7qX/3jbWqbYTqUsx/BWkmeccUaM2S4XAE455ZQYs7wIsBIgtjE+66yz\nzHosIfLSN5ZD+vrma7p///4xTuW6KXvssUeMfQ3wWOW/A6+77rq821SKli1bonPnzgByO1jffPPN\nMV5mmWVMbp999onx7NmzTY4lRfw978dSXuYxB7AyH2+XvvLKK8eYuzGzZS0A/OY3v4mxl5Kk4zEA\njBs3zuROP/30GHOteBntuuuuG2MvX+LXY5vg5kSWPKdaxrjmhJ4oCCGEEEIIIXLQjYIQQgghhBAi\nh7JKj+bPnx+lPV7i8+GHHxbczndxLibnH1GzlMc/huR9+EfFLPfYYostitq/P6ZiO0a//PLLMb76\n6qtNjh9ne+kRvx7Ll4Alcg0vtalGWIIzd+5ck2NXHy8L6du3b1H779SpU8HclVdeGWPvKHTppZfG\nmDs4/+pXvzLr8WNj77TCXX/Z0QMAHnvssRjzI1XusgpYCcWZZ55pcuzS46V11cb3338f3bm8uxCf\nt2OPPdbkWKrhJT/s9sVdzbNcfvy1yDIU39Wbu+YyfizhfWZd6z6XSjAAKxnxcjGWR/lurcWQJcUq\nJy1btoyyM+5wCwB/+9vfYuyv9ddffz3GvnZYCrLlllvGmKVGgO3u7Dszcw34c8/jM0vCjj76aLNe\nKqkCbCdmwHZj/vOf/2xyLG1iR6TddtvNrMcd1h999FGTYynf3//+d5NLJV4hBFQDHTp0iA5D7CAE\nWPmkd/DyjmEMn1/+PvHOQDxerLbaaiY3ffr0GJ988skmt/fee8eYx2rvesRSyCz5oJfFcc2xO5KX\nq7LUdMCAASbHvw/8efXuT0IUg54oCCGEEEIIIXLQjYIQQgghhBAiB90oCCGEEEIIIXIo6xyFFi1a\nRD2913B36dKl4HZeB8yw/r/YuQDcQRewFqXcHRmwunbWzddFf5w1P+Lzzz+P8eTJk2PsrRWzzg/v\n35/X9HxXY7c/7pAKWMs31ikDuR1ai8F3NuZzyp2uATsHwts1Dh8+PMZ/+tOfYsyaaMBqYr1F6Suv\nvBJjr8k+8sgjY/z73/8+7zH5Y+b9AbkdSqudVIfPtsiAPW9s9QjYa8dfY6y959hbH2aNJbxP3wm1\n0NyDrA7RWWME2/EC9r2ybt5bXGadA86xDh8A1ltvPQC510SlaN26dZxP5eensbUw6/EBayHqteUr\nrrhijFlLvsIKK5j1WN/NtsuAtdf0tqfcBZnHI7/eDjvsEGPuFAzk2mEyl19+eYx5ToyfN8fjjv9e\n4DlU/L0GLBlP2A68koQQ4veTH+vYCnn8+PEmx3PG1l9/fZPj7z+2IeXvWQB46aWXYsxzSgDb+Tq9\nblIKdWPmTswAMHTo0Biz5S5g5z/xZw7Y+Ya8jxtvvNGsx99JbNkN2G7g3v5XVB/V+NvMU/1HKIQQ\nQgghhCg7P3mjEEK4LYTwVQhhAv1b1xDC6BDClNr/F/5zt2jyqAYEoDoQqgGhGhCqgaWNYqRHQwFc\nC+AO+rchAJ5JkuSyEMKQ2uUz8mxraN26NVZZZRUAhSUy+WDLT7a9A6xUKO36DGR3L/Y53s7btrIt\n6ejRo/Nu4/G5rI7RbHv261//OsbeOq9YaQnbvgFL7NCeffbZorYvwFCUqAYWLVoUJVz+WIcMGRJj\nlgAA9lGxf0z9yCOPxJjlTGuuuaZZ74orroixPx977bVXjAcNGmRybE/Ij3WzLGy9xSrvwz/6507Q\nZ599doy99eF5550X41dffdXkuBNoIzIUJaiDNm3axLr31wo/RmcpV759MPy5HHXUUTHOslL01yLb\nMHqpwp133llwP4XwHUJZEuVr/7nnnsu7nier43xWLh1fS2CNORQlqIFZs2bF68XLZ1jK48dBvm7P\nPfdck7vhhhtizJ8Xd7oGrDSIx1zPP//5T7P8ySefxJjHCy9TY8mPt6Nkm07u1g5YidFVV10VYy9V\nZHne8ccfb3J8zQwePNjkUtlMkiRoIENRghpYvHgx5s+fDyD3embJj7cQ5bEv7TadsuGGG8aYx3Fv\noc02qmyHCljZmrc9ZdkXj9W/+93vzHre2pphqZ3vDs+SNpaR+XHy/vvvjzHbawO2k7X/XvDnsgEM\nRYl+E4jCVEuX6Z98opAkyRgA37p/3gdAavQ8DMC+JT4uUUWoBgSgOhCqAaEaEKqBpY36zlHoliRJ\nehs+A0C3QiuGEI4LIbwRQngj/euBaBbUqwb4r+eiWVBUHXANZD0pEE2SOteAbwQlmjx1rgH9Hmh2\n1LkGmkIjWFGCycxJzXPMgs8ykyS5KUmSTZMk2dTLjUTzoC414N1YRPMhqw64Brh7rGheFFsD3llM\nNB+KrQH9Hmi+FFsD7Copqpf62qN+GULoniTJ9BBCdwBfFbNR69ato35ziy22MDnW4LPdJ5BtM8j2\nillzAbLmKHCOLfb8cbFt39SpUwseU100xjxY8vv2mkRvI1kI/1ea1A6xEbRu9aqBNm3axHkqEydO\nNLmddtopxl6fzxrQgw8+2OT4rxKs/b7ooovMev37948xW5ICdj7D6aefbnJsS8p6Z9aoAtbSkOee\nAHbOhdeUPvTQQzFeY401Yuz1zazP5vkKnlNPPbVgrhGocx2EEKJ953777WdyrDv31z1fO1nWoHPm\nzMm7Tb5lhvf53XffmRw/CXv77bdjzHML8h1Xodfu1auXyZ1zzjkxzppvwmNL1nvj+RbAEjvWsWPH\nFtx3A6hzDbRq1Spq+/2NI1uKDhw40OT+53/+J8Z8rQBWW/7uu+/G2M9VOu2002Ls7SMHDBgQY2/b\nOmrUqBizLSfb2QJ2fHrvvfdMji1Wr7nmmoI5vtb9ejx+ePvYe+65J8a+PlI9P18fJaTONTB37tx4\n/dx8880mx3OOvL0o25T7eSpsgX399dfH2H9n7L777jH+6it7qGyBffTRR5vcOuusE2OuK/+UlOeR\neKtztrbmsQSw73vffZcod/z3Fa/38MMPmxzXqX9yx/M9G4F6/SYohqZgIerPNc+79b/nmtofSup7\n9kcBSEfwgQBGluZwRBNCNSAA1YFQDQjVgFANNFuKsUe9G8CrANYJIXwWQvgtgMsA9A8hTAGwc+2y\naKaoBgSgOhCqAaEaEKqBpY2flB4lSXJogdROBf69IAsWLIgWc96GlB+Je1kSy3P8Y7xJkyblfS1v\nefbOO+/E2D9SZrlAKotJ4e6fLEPadNNN875uPlgW4W1g+VieeuqpGF9yySUF9+f3wZIl/2ixX79+\nAKw8p66UsgZatWoVO6V6ORRbiD7zzDMmx4/LuXsxYC3y2JbOd/TcaqutYuwfKQ8bNizG/jEhPwLm\nrq5sxQfYmttll11Mju0UuQM3YO0P2dLwpJNOMuuxNZ/X977++utobEpVByGEeM3598HXYpZMaMKE\nCWaZ7TAffPDBgtvVdyI1HwvLkLLm3HgZEls+ch0BwG9/+9sY83Xhz0GWtCmLVErVUAliqWqgffv2\n6NOnDwDgySefNLnbb789xjvvvLPJde/ePcZPPPGEybFt5vnnnx/jrbfe2qx30003xdjLhtg+0luz\ncodntuBlW2TAWngfe+yxJnfEEUfE2Nuqcjd6rm8vX1puueVi7GVJrPn23XxTuUNDLXJLVQMtW7aM\n78VL/fj9e6tifv9e4877+eijj2LsZZxsRctWqYC1aH7rrbdMjrsqszX7HXfcYdZjG1wvdWa8pJgl\nWLNnz46xt/Hl1/bdo1kW7TvAl2pOQCl/EzQX/NjMY63PNQUpFdO0jlYIIYQQQghRFnSjIIQQQggh\nhMhBNwpCCCGEEEKIHOprj1ovFi5ciJkzZwKwcwYAYOTIJRPkuX09YPX1Xj/OsHaf25j7nNfpsV7M\n24uy/Rxr0FnjCFhdaqq9TcnSWvOcC7ZU8/MQ2PYzyz7Wa77T/VSLJm7BggX47LPPANhzBth29t5C\nlDX4Xq/J2v1ll102xqk1bAqfJ54PAtjz/fTTT5scW52yxt3Ps/nrX/8aYz+Hhd/rtttua3Js88g2\nsDz3ArB62RtuuMHkvI1kNfPDDz/Euvc2uK+99lqMvc0wL3tNM19jPCfI76O+Gn+20dxkk01izJpl\nwH7O3nqT8XOt2B6V9c78XgB7/Fm59u3bm9xaa60FoNGsMRsEa/8Be26OO+44k2P7zyFDhpgcW6Ly\n+xw3bpxZj+dAeA33fffdF2P/HcKfO+vavR0xz7XyY9xZZ50VY56TANi5Slw7XqPPNszeWpZthsv/\npAAAGp5JREFUQHv27Jk398ILL6Aa6NChAzbbbDMAVo8PWAvULDbeeGOzzOeN56zcf//9Zj3+HeHP\nE39mzz//vMnx3DmeS8njNmBr2v8e4Ndmy3UAuPXWW2PMtcjvBQD22GOPGLPtN4B4ToHcuZo8/6K5\n0AjW7yWBf3PV5fdXNb6f6vj1KIQQQgghhKgqdKMghBBCCCGEyKGs0qMOHTrEx3qHHXaYyXXp0iXG\nvnvo8OHDY8yPlz0syWFbU7+8/fbbm1yWbKgQXtLAFmhsjQZYO8UsGdKee+4ZY7bkBKw0JquzNEuU\nAESZD3f6rCRJksRHr507dzY5thRlazvAPq71j5unTJkS41/+8pcx9vILPjde2sSyoSy4M7N/jMud\nGf2j4o022ijG6667rslx52c+B2ypCtgOol5W1ZRIkiTWrJcQZXUeZqkN2wMCttMxW2h6+Dry9cGf\nEXcJ9/A44+U/jB8jeOxiO1cA+Mc//pH3uLLGi6ycJ33fDbXGLBXt2rWLcijudgsAgwYNirHvqszf\nG94WkiWll19+eYx99+LtttsuxtylGbAyH29Rvfnmm8eYu/f67wyWBXqZD9t++vfN3Z65xvh4ATsO\ncBdeAOjWrVuM+/bta3Lp+JQkCZoS3oKY3yNblgNW0sxW4ankOYUlZ368YAnYhRdeWPC4+Fry4zFL\nj7wNLl/f/F4A24WaxzgvneJuzL7LL1uEe8mL7x4s8lMfqXa1yLsbg+b7zoQQQgghhBD1RjcKQggh\nhBBCiBzKKj1i2Yl/VMePCVmCA1gZDrva1Je6yIYK4R8p33333THmx4KAlTt41xWWD3BXaC+d4seQ\nvvsyP371bknp+a7GmfQedhlJJVMp3G33kEMOqdf++ZFslmwoC17Pu6IceeSRMfYdPYcOHRpj7/TT\nu3fvGLPjC8ucAHv83H2zqdG2bdvoTvKLX/zC5FhO4mU9WTKf+jz29e5hLHFguaCH5Sm+CzR3w/WO\nJuxQs+uuu5rciy++GGPvwsLweOGlR7z8xRdf5D1mL+usFAsXLoySIC+d4PHNu0O9/PLLMd57771N\njuWr3JmZ/x2wrkHeEYllrvvtt5/JDRgwIO92jzzyiFnv3nvvjfF//vMfk+PvDS9JYYkRyxF9J3qW\nJ7755psmx9893t0vdVmqRuerLPxvBb4+vJSOvzf97wimvrKhQnh3JHYs8lJhloR52RO/N/4+8eMF\nf85efsZSXC+zzhpbmirVIvmpy2+sajnmYmlaRyuEEEIIIYQoC7pREEIIIYQQQuSgGwUhhBBCCCFE\nDmWdozBv3rzYefXqq682Oe6+vM8++5gcz1HwtoheZ1wMn3zyiVlmHezo0aNNjjvFsrWd16fz3IMV\nV1zR5Hhdr5fl42ftqLeBZWtPr3nkffg5HGmH4GI7XZYT31WUz5O3L91www1jXF99X9b8E66BYcOG\nFVzvvffei7HXl5500kkxvvLKK03uj3/8Y4x5rgEA7L///jE+/fTTY3zttdea9dhO1uvTDzrooILH\nXG2EEOJ8A18DWZ2T+T17XXt9xoHJkyebZZ5vwB2AgcJdoblTN5DdHZk/5//93/81uZ/97Gd59+Ht\nY4vFn8d0P9UyV2nWrFl44IEHAAAHHHCAyfE8AbY7BoBvv/02xv4a4/GZO9f68ZJtT/l7B7Ddlz/+\n+GOTK3SN+e8k7rj85JNPmhzbJrMGHbBzD7gWV155ZbMed3334/r8+fNj7OddpddMtVjkMn7eIM8h\nu+aaawput/XWW5vl+sxh9PMLuDM61xuQO+ckxV/rWTa1PL/Af8/zPBiuD9/lnTuD+zkWEydOjLG3\n4G2qcxT8uFVqjX+1jIvVip4oCCGEEEIIIXLQjYIQQgghhBAih7JKj1q0aBHtO710h+VAbDEIFN81\ntxRkSYrYNs3bl/Ijct91mjtucodhDz8i91IKznnpFFuJTpo0Ke++vaSqUixevDhaT3rrw6OPPjrG\nvsM0W8B5O7j6wJ05ASsXy7LhTLvJArnSBLapHTNmjMlxN2aWGABW5rLBBhvE+MwzzzTrPf300zG+\n6qqrCu7j1FNPLXj81cCCBQviuWO7SyBbYsePh7OsQYtdz8sdWObjpTtcE7vttluMuWYB20nYy6O4\n3m+++WaTY/tcfu0si1h/jFmyp1Qe46U2laJVq1bRetJ/zizP9NIjvgZ8nbO1Lo/BF110kVmP7Ym9\nxSrb4vrzu+WWW8Y4S07Ky176ce6558bYyx3YDpm7U19//fVmPe7ee84555gcdxX2HY3Tc+KtvasB\nX6/Tpk2LsT9P/P4bgw4dOsTYy1W9FCnFjzP8/b3jjjua3MYbbxzj8847z+RGjBgRY7YyPvjgg816\nLMfmugSsbNfbgPvvrKZKU5MKNTU7VE/TPnohhBBCCCFEo6AbBSGEEEIIIUQOulEQQgghhBBC5FDW\nOQqtW7fGKqusAsDqAAFrMffhhx+anLcQa0zYeg6wWkC2bfXH73W2DGuTWXcIAP/+979jPHPmzBh3\n7drVrMf2iayFB+y5S+eApMyYMQNArl66UrA2+auvvjI51tR6S7lx48aV9Dj8+WB9OluZAtbCk+ca\nsJUpYPXq9913n8kdf/zxMfZzcKZPnx7j5557LsasNwasXtZretm2tdpJkiRqelkTDtjryOtQWQfs\nNcGF5iX49Vh3vtNOO5kc2196G0E+31wPfv8874FtEAFrCf3222+bHNd+1jwE/3rFku6zWqwx27dv\njz59+gAARo4caXJs+emtMdmC2FtVpuMKYLXf3l6U7Uu9teQrr7wS4zPOOMPkeL4BzxHyc+h4TL/l\nlltMjueX+XHm3nvvjfGbb74ZYz/njY/fzznZaKONYuz19aldapIkqDb8XBSGLWUBawFbCvw8oO23\n3z7G3sqaxyeer+Dt3rPsyFddddUYe2vdN954I8b8u8HPLdhqq63y7g+w14H/XqhGm/SlnaYw30JP\nFIQQQgghhBA56EZBCCGEEEIIkUPZpUephGaLLbYwOS+nYfhxu5f4FLIbrcsj+qxusAw/pvaPrFOJ\nD5BrX8p2dPxoEbAyK7bcq0unWT4nvit0KuGplkeOLVq0iDIO7kQNWItIbw3Kn6fv5Jp2ePU5L/9h\nizK/f7YgfOihh0yOHwFfcMEFMfbSI5ZL+Y6sbOXoHwdzF9YTTjghxv6RMsvWuEszYC14q51WrVrF\nOk3lJymFOhT7ZZ/jzzZrPZYZePlWlnXq448/HmPu3M3WxEB212a27/S2iIccckje48qSHmVZv3pb\nxHRMYmlNJWnbti3WWGMNALljKV+LXgL2l7/8JcZ33nmnybF0h22MvXSHa4ztVgE7fvj989jNMkOW\nQwHAE088EWNvw9yjR48YpzLcFB5n+H2n5ynlmWeeibHvKszjh5dmPfXUUwCqxyI3C5YiZcmSSoG/\njrgLsrdLZ1kPf0f7791iv28322yzgq/Nkiv+jgCsFNd/H+6+++4x5k7jAPDNN98UdVxCMHqiIIQQ\nQgghhMjhJ28UQgg9QwjPhRDeDSFMDCGcXPvvXUMIo0MIU2r/3+Wn9iWaJqoBoRoQqgGhGhCqgaWP\nYp4oLARwapIk6wPYEsAJIYT1AQwB8EySJGsBeKZ2WTRPVANCNSBUA0I1IFQDSxk/KYRPkmQ6gOm1\n8dwQwiQAPQDsA2DH2tWGAXgewBl5dsH7inMK/NyCWbNmFdyONfhZuly2mZw0aZJZj/WfWbBFKWDn\nG/B8Am/RljUnolOnTjH29qsnnnhijNmWzVu2TZgwIcasgQWAMWPGxNhrKlO71IbYIpayBhYtWhTn\nJvh5KjyXY6WVVjI51lwvu+yyJsefEdcVW5IC1o7Qa7hZU+rnTrDN4JFHHhnjZ5991qy32267xTjL\n+nWFFVYwObZXZNu7aqKUNRBCiNp7X+dZ8wt4meckZG3nr0vWdHv70mnTpsXYz1Hg+QZ8zFn6Zj9v\nhLXmrFUHrG6c7VeLnT/1U6TWmA2x4itlDSxcuDDaS3oNOltBnnXWWSZ32WWXxXjAgAEF98+W2vff\nf7/JTZw4McY777yzyfGcmeHDh5scW/nyPIftttvOrMfzHnbddVeTGzFiRIxfeuklk2M75LPPPjvG\nPAcLsOeE57YA9vvEz81I58j4eRl1oZQ1UEn4un3rrbdM7q677ooxj9sAsOOOO8Z44403jnGpLFt5\nrgPPUfPHyGOVnwfD8xL8PBg/rtWH5lIDonjqNEchhLA6gH4AxgLoVlswADADQLcC2xwXQngjhPBG\ntUyoFfWnoTXgJy6KpkdDa8BP8hVND9WAaGgNlLofgig/Da2BpjCxXtThRiGE0BHAAwAGJ0liPt2k\npoNL3i4uSZLclCTJpkmSbOr/EiyaFqWoAf/EQzQtSlED/Bdz0fRQDYhS1IBvWiqaFqWoAVZbiOql\nKA/OEEJr1BTEXUmSpD5dX4YQuidJMj2E0B3AV4X3UMP8+fOjxdztt99ucr4bcyH8o3i2KOOuxF6a\n4B8hMixtyuqwzD9y/Q/eLOkRP6b2nZP5ETl39PTwdn4f3D3a/5WmS5cuebepK6WqgSRJogzCW3zy\nMXrpGNvP8mNXwFoVsmXpo48+atbjz9Zbm3KHTP+4lq1UWV70zjvvmPW4Jrx86bHHHotx9+7dTc7X\naop/+sLL+++/v8l5OVpjUKoaWLhwYZT4+XPIEsEsmaG/Tgt1Zk5rLYXPobcg5rFlm222Mbk999wz\nxtz91kun+NG+lzuyhMTbWvK6bLHq4WPMkm1569fULtTXZV0pVQ3MmTMnWs7Onj3b5C666KIYX3jh\nhSZ3/vnn540BK91ha2RvLckduX0dsY3xxRdfbHIs6WBLSl9j3OE7HX9T2LLUd21me+Vjjjkmxn4c\n4M7Mn3/+ucnxcXEnaWCJ5KihnWBLVQOVhMcI/13OvxX8ky+2JWUK2bQ3BK4Hb5XNVr2jRo0yOT5G\nlkoBuZ2g60tzqAFRPMW4HgUAtwKYlCTJVZQaBWBgbTwQwMjSH56oBlQDQjUgVANCNSBUA0sfxTxR\n2AbA4QDeCSGMr/23swBcBuDeEMJvAXwC4KAC24umj2pAqAaEakCoBoRqYCmjGNejlwAUsszZqcC/\n56Vt27ZYc801AQCrrbaayfH8hfo+GmXHAO8qwnKSLDcVL0dgOQznsiRQPseuPNzhFbAOFPz40j8i\nZIcgL41JzykAvP/++yaXOkE1RHpUyhooFq9d5PfI7kUAcO2118aYJ0dxB1PAPmJOHVdSBg0aFGOW\nPgD23NW3AzLLB7wcgbtz8nXA7xkArrnmmhjvscce9TqO+lLKGkiSJD76ZycZILvzMMtE/BjB6/L1\nx84kgHW58dcYS3myZD1Z/87b+f0PHjw4xl4q9o9//CPGxXaVr0v3+RK5n5WsBlq1aoXlllsOQG63\naHaO8vIiliJlyZJYhuS/a7LGY5Z7sGQUAK677roYf/HFFzHedtttzXrcVdk7nLFMxMuGDj744Biz\nPJHrBrDOfLfddpvJ8bh5xx13mFw65nmpVF2oxHdBY8DX6QYbbGByfrkY6nIt1oeuXbuaZa4PL99k\nGbf/DvHjWn1oLjVQLXjpcUOlgY2BOjMLIYQQQgghctCNghBCCCGEECIH3SgIIYQQQgghcijKHrVk\nL9aqVdRne/0n63mz9H5ZuSxrU9auZ9kiegrNX8jS+vn9syadrTwBq41nHbu3OeWct2JjvX2h7apR\n9+bh9+itJblT6V577WVyQ4cOjXHafdT/O2A7IF9xxRUmx+fHW+J5q9YUb7nbunXrGPvup6x75e7O\ngO0EynNM2JYVsPp6tllsarRq1SrOJ+LPC7DjQrHXpV/mzyFrroEfS7gDt+92ypr0rKaBbGPINqoA\n0KtXrxgfccQRJnfUUUflPS7fSTVrDgcv8/ECS/TwY8eOLXjs5aRTp06xa/HUqVNN7k9/+lOMhwwZ\nYnLclfjyyy8vajt/nfL1xnaogJ3n1r9/f5M79NBDY/zqq6/GmK2PAWttuv3225sczxvwNs8nnHBC\njF988cUYX3XVVWY97kg9bNgwk+NzwjUFLOmA7TvKi8bF1x/PwWHbZcBaI6fzCwFrow4AP//5z/PG\ngJ2z8M0335ict+YWohj0REEIIYQQQgiRg24UhBBCCCGEEDmUVXrEnZm5syCQ3Zk5y9qzUM5LE7Ik\nS1ldFTnH0ia/P34935r+F7/4RYy9bIYlVw8//HCMR48ebdbL6ljLdOvWzSyn1pssvaoW2AoUsNaE\nbHkKAB999FGMd9hhB5NjyQhbjXr5zwsvvBBjfw5vvPHGGPtOnWyzyh1vuUMqYG0YvS0nP0b2+2f5\nDXeD9Z2l2SLX1wcf14gRI1DNhBDi9ZIlDaqv9GjWrFkx9haUX375ZYwbo5sqS5a8fKl3794x7tOn\nj8mxHIHJOgdZ+O1SSUO1SBAXLVoUxySua8BKcv785z+bHEuKzjjjDJNj2Q1v57vTso1q2rE6hS2I\n11hjDZNjGRHLpbwshK25va0l1+OUKVNMjiUqbMPMkjXAfleOGTPG5PiY/XtLbVGTJIEoH16qyJ+z\n77jM4yF/l3vJEI9dvpM7W6L6rudeiiTKgx93vSVqtdO0jlYIIYQQQghRFnSjIIQQQgghhMhBNwpC\nCCGEEEKIHMo6R6FFixZxToHX8bOFqIftRr22i3PelrQQ3kaV9d0eb0+Y4vXNrHn3uZEjR8b4gQce\nMDk+Zp5v0alTJ7OeXy6EP6/pPqtRE/fZZ5+ZZba19Dr+8ePHx9jru/ncsBbQ28a98sorMfba4Zkz\nZ8Z4hRVWMDnWFbN95/fff2/WW3nllWPMOnnAzoEYN26cyW299dYxZttFb4/K2mee8+CPK9UiF1qu\nNAsXLoxa2aVJM8u6dm8JWi6qpRbmzp0bbTo32WQTkzvllFNi7OcB8dyDwYMHm9yvf/3rGPM1y/OW\nADvO7LvvviaXZb86cODAGL/88ssx9mP6ueeeG2O2NAaAddddN8YHHnhgwdfecsstY7zHHnuY9Xj8\nuOeee0yuX79+Mb7++utNLp3bMHfuXIjS4n8nfPzxxzHmevb4eXqHHHJIjA8//PAY+7lc/BvGz1Hg\nY/nqq69Mzn+vispQLXPFiqX6fj0KIYQQQgghKo5uFIQQQgghhBA5lFV61KFDh/hI1Xcm9Y+fqx1v\nr8lyI2+H9sknn8TYy23YxpBtzXzH2mLxXYRTW9Xnn3++XvsrNYsWLcKcOXMA2MezALD33nvnjasV\nljAAVtbhpXRsg+dlT1tttVWM2VaVrTyBXJvEQrC8AQAmTpwIwMqahKgkLVu2ROfOnQEAb775ZsH1\nNt98c7PMEspRo0aZXNp5GLAyId/FnGVf3jqVu7KzXBCw0lCWA02YMMGs99prr8U4fY8pLDfylrgs\nr2Rpk5eL8Ng4efJkk+Nz6WVm6XeKt40WDcfLl1kO5C2aGf9bYcaMGTFmWVnHjh3NeixZWn755U2O\nLXJ79uxpcl4SW2lmzZoVpXveytV3ZRf14/XXXzfLbLVfLHqiIIQQQgghhMhBNwpCCCGEEEKIHHSj\nIIQQQgghhMihrHMUkiSJWv758+eb3Ndff13OQykrq622Wt44i/qeD695TJerxY6rTZs2Ua/vNf5N\njSyrybXXXjtzmeH5LaxHLnZOgufTTz81y+lxJklSr/0JUWratm2L3r17A1hi25nCcwi6d+9ucjyf\nh3X8AOLcJ8BeR94ism/fvjH2Gu7UshXIvb55bgCPXY8++qhZj8faww47zOR4joV/3y+99FLefRxw\nwAEF9/HEE0+YHM+d69+/f97tmvq4W43w/Jh8y4XwNTx27NgY89xGb/3OcyL85/nWW2/FeKONNjI5\nP3+t0ixYsCB+X/nr7cEHH6zEITU7/JyEYtsIMHqiIIQQQgghhMhBNwpCCCGEEEKIHEI55QghhJkA\nPgGwAoBq0BotTcexWpIkKzbya/wkqoGCqAYqx9J0HKqB/CxNx6EayM/SdhwVrwPVQEGqqgbKeqMQ\nXzSEN5Ik2bTsL6zjqBqq5T3rOCpHtbxnHUflqJb3rOOoHNXynnUclaNa3rOOIz+SHgkhhBBCCCFy\n0I2CEEIIIYQQIodK3SjcVKHX9eg4Kke1vGcdR+Wolves46gc1fKedRyVo1res46jclTLe9Zx5KEi\ncxSEEEIIIYQQ1Y2kR0IIIYQQQogcdKMghBBCCCGEyKGsNwohhF+FECaHEKaGEIaU8XVvCyF8FUKY\nQP/WNYQwOoQwpfb/XcpwHD1DCM+FEN4NIUwMIZxcqWOpFKoB1UClaqD2tSteB6oB1YBqQDWgGqhB\nvwmqvw7KdqMQQmgJ4DoAuwFYH8ChIYT1y/TyQwH8yv3bEADPJEmyFoBnapcbm4UATk2SZH0AWwI4\nofYcVOJYyo5qAIBqoJI1AFRHHagGVAOqAdXAUl0DQMXrYCgqXwNAU6iDJEnK8h+ArQA8SctnAjiz\njK+/OoAJtDwZQPfauDuAyeU6FjqGkQD6V8OxqAZUA0tDDVRjHagGVAOqAdXA0lYD1VAH1VYD1VoH\n5ZQe9QAwjZY/q/23StEtSZLptfEMAN3K+eIhhNUB9AMwttLHUkZUA4RqAEDlawCo4LlXDQBQDawO\n1YBqYOmrAaD66kC/CfKgycwAkppbtrL5xIYQOgJ4AMDgJEn+W8ljETWoBgRQ3nOvGqhOVANCNSD0\nm2AJ5bxR+BxAT1pepfbfKsWXIYTuAFD7/6/K8aIhhNaoKYa7kiR5sJLHUgFUA1ANoLpqAKjAuVcN\nqAZUA6qBpbwGgOqrA/0myEM5bxTGAVgrhNArhNAGwCEARpXx9T2jAAysjQeiRhfWqIQQAoBbAUxK\nkuSqSh5LhVANqAaqrQaAMp971YBqQDWgGlANAKi+OtBvgnyUeZLG7gDeB/ABgLPL+Lp3A5gO4EfU\naOB+C2B51MwknwLgaQBdy3Ac26Lm8dF/AIyv/W/3ShxLpf5TDagGKlUD1VIHqgHVgGpANaAaqGwd\nVEMNNJU6CLUHKoQQQgghhBARTWYWQgghhBBC5KAbBSGEEEIIIUQOulEQQgghhBBC5KAbBSGEEEII\nIUQOulEQQgghhBBC5KAbBSGEEEIIIUQOulEQQgghhBBC5PD/4g26FUIbsk8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x864 with 24 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ShAuv_gZwWN",
        "colab_type": "text"
      },
      "source": [
        "#Assignment 2 _session2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGpSlYlEZxJi",
        "colab_type": "text"
      },
      "source": [
        "##CONSTRAINTS:-\n",
        "\n",
        "define a new network such that:\n",
        "\n",
        "1.   it has less than 20000 parameters\n",
        "2.   it achieves validation accuracy of more than 99.4% (basically print(score) should be more than 0.994\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOrIcsgvlkN5",
        "colab_type": "text"
      },
      "source": [
        "##Experiment to design  Model which has less than 20000 parameters and accuracy 99.4%\n",
        "\n",
        "\n",
        "Following instructions told tried different architecture to design model which involves structure:\n",
        "\n",
        "--convl->convl->convl->Maxpool->convl->convl->convl-> flatten-> fully connected \n",
        "\n",
        "**Highest accuracy achieved is 99.05 %**\n",
        "\n",
        "\n",
        "Tried using different  number of kernels, 1X1 convolution layer, and got below insights:-\n",
        "\n",
        "\n",
        "\n",
        "1.   For mnist digit  classification - using 1X1 convolution layer achieves better accuracy.\n",
        "2.   using large  kernel size (16, 32, 64)  at last layer improves accuracy as whole image itself is digit. It helps in seeing whole digits features if large kernel size is used. thats  why the demo code is achieving 98 % accuracy but number of parameters are more.\n",
        "\n",
        "Improvements needed:-\n",
        "\n",
        "1. large Number of kernel size(128, 256, 512) and large kernel size(5, 7) may improve accuracy but increases number of parameters.\n",
        "1.   Increasing number of epochs may increase accuracy.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Note- every time model is run accuracy changes a bit.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW0BpHoGFpFQ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "###Experiment1\n",
        "1.   Total params: 19,052\n",
        "1.   Trainable params: 19,052\n",
        "2.  Non-trainable params: 0\n",
        "\n",
        "\n",
        " **Accuracy : 98.49 %**\n",
        "\n",
        "[0.050116387882188425, 0.9849]\n",
        "\n",
        "\n",
        "Each convolution layer with  64,, 32  (3X3) kernel followed by10 ( 1X1)  convolution layer with one maxpooling layer and last dense layer with kernel size of 12 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxOg1rRCl62W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "979de0bb-5c19-4dc1-b148-43ab3f4b5310"
      },
      "source": [
        "#given model architecture \n",
        "#rp- receptive field, channel size-cs\n",
        "\n",
        "from keras.layers import Activation\n",
        "model2 = Sequential()\n",
        "\n",
        " \n",
        "model2.add(Convolution2D(64, 3, 3, activation='relu', input_shape=(28,28,1))) #rp-3 , cs-26X26X64\n",
        "model2.add(Convolution2D(10, 1, activation='relu'))#3, 26x26x10\n",
        "\n",
        "model2.add(Convolution2D(32, 3,3, activation='relu'))#5, 24x24x32\n",
        "\n",
        "model2.add(Convolution2D(10, 1,1, activation='relu'))#5, 24x24x10\n",
        "\n",
        "model2.add(MaxPooling2D(pool_size=(2,2)))#10 , 12x12x10\n",
        "\n",
        "model2.add(Convolution2D(10, 1,1, activation='relu'))#10, 12X12X10\n",
        "\n",
        "\n",
        "model2.add(Convolution2D(10, 12))#28, 1X1X10\n",
        "\n",
        "\n",
        "\n",
        "model2.add(Flatten()) \n",
        "model2.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#provides model architecture information- input output shape,total parameters.\n",
        "model2.summary()\n",
        "\n",
        "#compile model which is defined in previous step.\n",
        "model2.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#train or fit model to our train dataset.\n",
        "model2.fit(X_train, Y_train, batch_size=32, nb_epoch=10, verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "#calculate score or evaluate model for test dataset- which is not included in training.\n",
        "score = model2.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print(score)\n",
        "\n",
        "\n",
        "y_pred = model2.predict(X_test)\n",
        "\n",
        "#compare predicted value of model and actual value of test data.\n",
        "print(y_pred[:9])\n",
        "print(\"\\n\")\n",
        "print(y_test[:9])\n",
        "\n"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_148 (Conv2D)          (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "conv2d_149 (Conv2D)          (None, 26, 26, 10)        650       \n",
            "_________________________________________________________________\n",
            "conv2d_150 (Conv2D)          (None, 24, 24, 32)        2912      \n",
            "_________________________________________________________________\n",
            "conv2d_151 (Conv2D)          (None, 24, 24, 10)        330       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling (None, 12, 12, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_152 (Conv2D)          (None, 12, 12, 10)        110       \n",
            "_________________________________________________________________\n",
            "conv2d_153 (Conv2D)          (None, 1, 1, 10)          14410     \n",
            "_________________________________________________________________\n",
            "flatten_26 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 19,052\n",
            "Trainable params: 19,052\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 24s 395us/step - loss: 0.1904 - acc: 0.9415\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 21s 351us/step - loss: 0.0722 - acc: 0.9785\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.0573 - acc: 0.9825\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 21s 351us/step - loss: 0.0478 - acc: 0.9850\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 21s 351us/step - loss: 0.0416 - acc: 0.9867\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 21s 351us/step - loss: 0.0349 - acc: 0.9887\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.0319 - acc: 0.9900\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 21s 350us/step - loss: 0.0280 - acc: 0.9914\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 21s 357us/step - loss: 0.0255 - acc: 0.9917\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 21s 351us/step - loss: 0.0233 - acc: 0.9922\n",
            "[0.050116387882188425, 0.9849]\n",
            "[[1.27161600e-13 3.49354931e-13 1.04200794e-10 7.90186004e-06\n",
            "  4.99870414e-18 1.96645196e-12 4.87164296e-22 9.99992132e-01\n",
            "  9.53183932e-09 1.80703541e-08]\n",
            " [7.15159757e-13 8.62372751e-09 1.00000000e+00 1.69638676e-13\n",
            "  8.02212114e-15 9.37404798e-16 1.28590846e-10 9.31133642e-13\n",
            "  1.67269865e-12 3.09130200e-17]\n",
            " [7.84600118e-09 9.99982476e-01 1.55958139e-06 2.98850750e-07\n",
            "  9.93925369e-06 1.96983308e-07 5.44612931e-07 2.09467703e-06\n",
            "  2.85524175e-06 1.51138444e-08]\n",
            " [9.99996901e-01 1.39649783e-12 2.46299745e-08 2.80362777e-09\n",
            "  6.16058246e-11 1.52468105e-09 3.39415720e-07 2.58427576e-06\n",
            "  9.82673409e-09 8.77076047e-08]\n",
            " [1.18170945e-10 4.75415985e-13 1.85968480e-11 1.58356439e-08\n",
            "  9.99994159e-01 3.05101194e-10 5.81354254e-09 2.32918012e-07\n",
            "  1.37521861e-08 5.55858287e-06]\n",
            " [7.11140613e-09 9.99708474e-01 1.16524370e-06 5.51417827e-07\n",
            "  2.87602616e-05 6.16483220e-08 2.02347692e-07 2.35595493e-04\n",
            "  2.49916829e-05 2.18832611e-07]\n",
            " [7.27774706e-14 2.76923117e-07 6.06832009e-06 1.76613160e-08\n",
            "  8.95735800e-01 1.63870609e-05 6.49221510e-10 4.01292164e-05\n",
            "  1.00950241e-01 3.25107109e-03]\n",
            " [7.06172154e-19 9.42468256e-11 2.47303740e-11 1.16152332e-08\n",
            "  1.01754942e-07 3.66343968e-08 1.71756479e-14 1.15135435e-09\n",
            "  1.83647853e-06 9.99998093e-01]\n",
            " [1.11169340e-09 4.14647030e-18 4.76515962e-12 6.27464877e-12\n",
            "  1.97376138e-09 7.50465870e-01 2.49205723e-01 2.41539712e-14\n",
            "  3.28378374e-04 8.02924305e-08]]\n",
            "\n",
            "\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTtqsBF2G6M-",
        "colab_type": "text"
      },
      "source": [
        "###Experiment2\n",
        "\n",
        "Total params: 19,532\n",
        "\n",
        "\n",
        "Trainable params: 19,532\n",
        "\n",
        "\n",
        "Non-trainable params: 0\n",
        "\n",
        "**Accuracy- 98.79 %**\n",
        "\n",
        "[0.05057578535213542, 0.9879] \n",
        "\n",
        "\n",
        "used 16 number of kernels for each 3X3 kernel convolution layer. used same design as experiment1 but added 10(1X1 ) convolutional after maxpooling Increased accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEqiIZcJHB6x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c86830fa-be31-49ef-a436-6d73584f5089"
      },
      "source": [
        "from keras.layers import Activation\n",
        "model2 = Sequential()\n",
        "\n",
        " \n",
        "model2.add(Convolution2D(16, 3, 3, activation='relu', input_shape=(28,28,1))) #rp-3 , cs-26X26X16\n",
        "model2.add(Convolution2D(10, 1, activation='relu'))#3, 26X26X10\n",
        "\n",
        "model2.add(Convolution2D(16, 3,3, activation='relu'))#5, 24X24X16\n",
        "\n",
        "model2.add(Convolution2D(10, 1,1, activation='relu'))#5, 24X24X10\n",
        "\n",
        "model2.add(MaxPooling2D(pool_size=(2,2)))#10 , 12X12X10\n",
        "\n",
        "model2.add(Convolution2D(10, 1,1, activation='relu'))#10, 12X12X10\n",
        "\n",
        "model2.add(Convolution2D(16, 3,3, activation='relu'))#12, 10X10X16\n",
        "\n",
        "\n",
        "model2.add(Convolution2D(10, 10))#28, 1X1X10\n",
        "\n",
        "\n",
        "\n",
        "model2.add(Flatten()) \n",
        "model2.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#provides model architecture information- input output shape,total parameters.\n",
        "model2.summary()\n",
        "\n",
        "#compile model which is defined in previous step.\n",
        "model2.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#train or fit model to our train dataset.\n",
        "model2.fit(X_train, Y_train, batch_size=32, nb_epoch=10, verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "#calculate score or evaluate model for test dataset- which is not included in training.\n",
        "score = model2.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print(\"\\n\",\"score:-\",score,\"\\n\")\n",
        "\n",
        "\n",
        "y_pred = model2.predict(X_test)\n",
        "\n",
        "#compare predicted value of model and actual value of test data.\n",
        "print(y_pred[:9])\n",
        "print(\"\\n\")\n",
        "print(y_test[:9])\n",
        "\n"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_141 (Conv2D)          (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_142 (Conv2D)          (None, 26, 26, 10)        170       \n",
            "_________________________________________________________________\n",
            "conv2d_143 (Conv2D)          (None, 24, 24, 16)        1456      \n",
            "_________________________________________________________________\n",
            "conv2d_144 (Conv2D)          (None, 24, 24, 10)        170       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 12, 12, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_145 (Conv2D)          (None, 12, 12, 10)        110       \n",
            "_________________________________________________________________\n",
            "conv2d_146 (Conv2D)          (None, 10, 10, 16)        1456      \n",
            "_________________________________________________________________\n",
            "conv2d_147 (Conv2D)          (None, 1, 1, 10)          16010     \n",
            "_________________________________________________________________\n",
            "flatten_25 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 19,532\n",
            "Trainable params: 19,532\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 24s 393us/step - loss: 0.1972 - acc: 0.9402\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 21s 355us/step - loss: 0.0663 - acc: 0.9802\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 21s 355us/step - loss: 0.0495 - acc: 0.9850\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.0389 - acc: 0.9881\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 22s 359us/step - loss: 0.0306 - acc: 0.9911\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 21s 355us/step - loss: 0.0260 - acc: 0.9917\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 21s 353us/step - loss: 0.0218 - acc: 0.9930\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 21s 357us/step - loss: 0.0182 - acc: 0.9940\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 21s 355us/step - loss: 0.0168 - acc: 0.9947\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 21s 355us/step - loss: 0.0134 - acc: 0.9959\n",
            "\n",
            " [0.05057578535213542, 0.9879] \n",
            "\n",
            "[[3.6690977e-15 1.2555914e-17 1.3503975e-09 1.2040600e-08 1.1121638e-16\n",
            "  8.4541979e-14 7.5054283e-27 1.0000000e+00 5.3721121e-11 6.4543784e-09]\n",
            " [4.7656903e-13 1.3961200e-10 1.0000000e+00 5.9779337e-14 2.6072761e-21\n",
            "  5.2300202e-15 3.4918331e-12 2.6101449e-19 2.5126290e-11 5.5351676e-20]\n",
            " [5.9992655e-10 9.9998569e-01 1.4631898e-06 7.5028460e-08 7.9031206e-06\n",
            "  3.4975860e-06 9.0149010e-10 9.2856754e-07 4.4977511e-07 1.9035587e-08]\n",
            " [9.9999917e-01 3.4382696e-13 2.7152953e-07 1.4943617e-13 7.9059920e-10\n",
            "  7.5667810e-09 3.3395008e-07 5.7213140e-12 1.8878627e-12 2.3493726e-07]\n",
            " [1.3015387e-12 2.0172199e-16 1.2856113e-10 2.0603843e-11 9.9999964e-01\n",
            "  2.7662427e-15 2.6335167e-12 7.5352034e-12 4.3698048e-10 2.9779466e-07]\n",
            " [8.9278342e-11 9.9994373e-01 6.1008905e-07 5.4208695e-09 2.0462356e-05\n",
            "  1.0648682e-09 2.4310642e-12 3.5010857e-05 2.4703954e-07 1.7951617e-08]\n",
            " [3.3100947e-20 3.8029848e-12 2.8114544e-09 5.2425416e-12 9.9885225e-01\n",
            "  5.4383625e-10 1.4223887e-13 5.7834292e-08 1.1470417e-03 7.4522529e-07]\n",
            " [1.3849595e-15 5.8659557e-12 7.7499770e-08 1.4406166e-07 1.9245639e-05\n",
            "  4.1103354e-07 1.1150853e-12 1.2400578e-10 8.0286391e-06 9.9997211e-01]\n",
            " [6.8091000e-09 1.1241985e-17 2.2227981e-12 5.4273884e-11 1.2681861e-14\n",
            "  9.9077517e-01 7.7269226e-03 8.8139366e-12 1.4956812e-03 2.1518970e-06]]\n",
            "\n",
            "\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8hZc_IwGnlk",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PARmzH0pP9g5",
        "colab_type": "text"
      },
      "source": [
        "###Eperiment3\n",
        "\n",
        "Total params: 18,412\n",
        "\n",
        "\n",
        "Trainable params: 18,412\n",
        "\n",
        "\n",
        "Non-trainable params: 0\n",
        "\n",
        "**Accuracy - 98.5 %**\n",
        "score:- [0.05180799219533437, 0.9855] \n",
        "\n",
        "used 32 number of kernels for 3X3 kernelsize accuracy decreased liitle bit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_jDdnC9QEyE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0aa8a5b6-a7a5-4108-c370-14f54b705dda"
      },
      "source": [
        "from keras.layers import Activation\n",
        "\n",
        "model2 = Sequential()\n",
        "\n",
        " \n",
        "model2.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(28,28,1))) #rp-3 , cs-26X26X32\n",
        "model2.add(Convolution2D(10, 1, activation='relu'))#3, 26X26X10\n",
        "\n",
        "model2.add(Convolution2D(32, 3,3, activation='relu'))#5, 24X24X32\n",
        "\n",
        "model2.add(Convolution2D(10, 1,1, activation='relu'))#5, 24X24X10\n",
        "\n",
        "model2.add(MaxPooling2D(pool_size=(2,2)))#10 , 12X12X10\n",
        "\n",
        "model2.add(Convolution2D(10, 1,1, activation='relu'))#10, 12X12X10\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model2.add(Convolution2D(10, 12))#28, 1X1X10\n",
        "\n",
        "\n",
        "\n",
        "model2.add(Flatten()) \n",
        "model2.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#provides model architecture information- input output shape,total parameters.\n",
        "model2.summary()\n",
        "\n",
        "#compile model which is defined in previous step.\n",
        "model2.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#train or fit model to our train dataset.\n",
        "model2.fit(X_train, Y_train, batch_size=32, nb_epoch=10, verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "#calculate score or evaluate model for test dataset- which is not included in training.\n",
        "score = model2.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print(\"\\n\",\"score:-\",score,\"\\n\")\n",
        "\n",
        "\n",
        "y_pred = model2.predict(X_test)\n",
        "\n",
        "#compare predicted value of model and actual value of test data.\n",
        "print(y_pred[:9])\n",
        "print(\"\\n\")\n",
        "print(y_test[:9])"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_154 (Conv2D)          (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_155 (Conv2D)          (None, 26, 26, 10)        330       \n",
            "_________________________________________________________________\n",
            "conv2d_156 (Conv2D)          (None, 24, 24, 32)        2912      \n",
            "_________________________________________________________________\n",
            "conv2d_157 (Conv2D)          (None, 24, 24, 10)        330       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_24 (MaxPooling (None, 12, 12, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_158 (Conv2D)          (None, 12, 12, 10)        110       \n",
            "_________________________________________________________________\n",
            "conv2d_159 (Conv2D)          (None, 1, 1, 10)          14410     \n",
            "_________________________________________________________________\n",
            "flatten_27 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 18,412\n",
            "Trainable params: 18,412\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 22s 372us/step - loss: 0.1862 - acc: 0.9428\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 20s 336us/step - loss: 0.0692 - acc: 0.9793\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 20s 335us/step - loss: 0.0528 - acc: 0.9831\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 20s 335us/step - loss: 0.0435 - acc: 0.9865\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 20s 335us/step - loss: 0.0389 - acc: 0.9873\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 20s 334us/step - loss: 0.0336 - acc: 0.9889\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 20s 333us/step - loss: 0.0294 - acc: 0.9909\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 20s 332us/step - loss: 0.0265 - acc: 0.9908\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0234 - acc: 0.9923\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 20s 336us/step - loss: 0.0210 - acc: 0.9932\n",
            "\n",
            " score:- [0.05180799219533437, 0.9855] \n",
            "\n",
            "[[8.77518457e-16 1.83687607e-15 1.31484768e-09 1.51902398e-07\n",
            "  1.68474919e-17 7.06491601e-11 8.14861807e-27 9.99999881e-01\n",
            "  1.97940917e-11 1.50071544e-09]\n",
            " [2.27997384e-06 2.99031093e-08 9.99996901e-01 4.63647512e-13\n",
            "  4.64670078e-13 9.63070769e-14 8.77940010e-07 8.69305364e-18\n",
            "  3.00632785e-09 2.86760633e-14]\n",
            " [1.84733256e-10 9.99863863e-01 1.11514134e-06 6.44431424e-08\n",
            "  6.08105511e-05 2.67009705e-06 1.67047475e-07 2.81193315e-05\n",
            "  4.31058434e-05 1.27019577e-08]\n",
            " [1.00000000e+00 6.83737701e-17 3.40617987e-08 2.44283302e-14\n",
            "  2.27958906e-12 1.28497243e-13 1.14219665e-08 1.65908235e-13\n",
            "  7.09129630e-11 3.66000494e-11]\n",
            " [2.51085722e-14 2.74998428e-18 9.76600697e-15 6.11395632e-14\n",
            "  1.00000000e+00 5.05661348e-15 2.67940746e-15 8.65690505e-12\n",
            "  9.74107080e-13 4.13325069e-10]\n",
            " [1.14805755e-10 9.99847174e-01 4.81932183e-08 1.36632909e-08\n",
            "  4.02433543e-05 5.86451598e-09 1.40633435e-10 1.06677864e-04\n",
            "  5.63480398e-06 3.46354909e-08]\n",
            " [8.76958621e-16 6.01522165e-11 9.61885016e-10 3.65941956e-12\n",
            "  9.99987364e-01 3.83491994e-09 6.23457701e-16 8.98073349e-09\n",
            "  1.24845374e-05 1.96740388e-07]\n",
            " [4.97254507e-19 7.47905626e-11 1.75973278e-10 1.71676522e-08\n",
            "  1.14505389e-03 3.44108628e-07 3.42638092e-12 1.94626815e-10\n",
            "  1.10150431e-05 9.98843431e-01]\n",
            " [7.03990821e-09 4.82188489e-22 4.13653503e-17 3.58477693e-14\n",
            "  1.08348983e-08 9.91728485e-01 7.50508066e-03 7.51918360e-17\n",
            "  7.66449200e-04 9.00635677e-10]]\n",
            "\n",
            "\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpZ7pO2FXKUN",
        "colab_type": "text"
      },
      "source": [
        "###Experiment4\n",
        "\n",
        "Total params: 17,218\n",
        "\n",
        "Trainable params: 17,218\n",
        "\n",
        "Non-trainable params: 0\n",
        "\n",
        "**Accuracy  99.05%**\n",
        "\n",
        "score:- [0.03604102025316315, 0.9905] \n",
        "\n",
        "used 3 conv-> maxpool->3 convl with increasing number of kernels (8, 16, 32) and no 1X1 conv layer. accuracy increased to 99%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_x_3PAeYQF4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f7b5d993-5148-4a24-b76d-d1b5c9d5594c"
      },
      "source": [
        "from keras.layers import Activation\n",
        "\n",
        "model2 = Sequential()\n",
        "\n",
        " \n",
        "model2.add(Convolution2D(8, 3, 3, activation='relu', input_shape=(28,28,1))) #3, 26X26X8\n",
        "model2.add(Convolution2D(16, 3,3, activation='relu'))# 5, 24X24X16\n",
        "model2.add(Convolution2D(32, 3,3, activation='relu'))#7,  22X22X32\n",
        "\n",
        "\n",
        "model2.add(MaxPooling2D(pool_size=(2,2)))#14,  11X11X32\n",
        "\n",
        "model2.add(Convolution2D(8, 3,3, activation='relu'))#16,  9X9X8\n",
        "model2.add(Convolution2D(16, 3,3, activation='relu'))#18 7X7X16\n",
        "model2.add(Convolution2D(10, 7 ))#28,  1X1X32\n",
        "\n",
        "\n",
        "\n",
        "model2.add(Flatten()) \n",
        "model2.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#provides model architecture information- input output shape,total parameters.\n",
        "model2.summary()\n",
        "\n",
        "#compile model which is defined in previous step.\n",
        "model2.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#train or fit model to our train dataset.\n",
        "model2.fit(X_train, Y_train, batch_size=32, nb_epoch=10, verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "#calculate score or evaluate model for test dataset- which is not included in training.\n",
        "score = model2.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print(\"\\n\",\"score:-\",score,\"\\n\")\n",
        "\n",
        "\n",
        "y_pred = model2.predict(X_test)\n",
        "\n",
        "#compare predicted value of model and actual value of test data.\n",
        "print(y_pred[:9])\n",
        "print(\"\\n\")\n",
        "print(y_test[:9])"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\")`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_216 (Conv2D)          (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "conv2d_217 (Conv2D)          (None, 24, 24, 16)        1168      \n",
            "_________________________________________________________________\n",
            "conv2d_218 (Conv2D)          (None, 22, 22, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_34 (MaxPooling (None, 11, 11, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_219 (Conv2D)          (None, 9, 9, 8)           2312      \n",
            "_________________________________________________________________\n",
            "conv2d_220 (Conv2D)          (None, 7, 7, 16)          1168      \n",
            "_________________________________________________________________\n",
            "conv2d_221 (Conv2D)          (None, 1, 1, 10)          7850      \n",
            "_________________________________________________________________\n",
            "flatten_38 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 17,218\n",
            "Trainable params: 17,218\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 24s 402us/step - loss: 0.1708 - acc: 0.9471\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 21s 347us/step - loss: 0.0580 - acc: 0.9826\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 21s 343us/step - loss: 0.0427 - acc: 0.9868\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.0345 - acc: 0.9889\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.0292 - acc: 0.9909\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 21s 345us/step - loss: 0.0247 - acc: 0.9918\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 21s 345us/step - loss: 0.0200 - acc: 0.9934\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.0194 - acc: 0.9938\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0171 - acc: 0.9943\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 21s 346us/step - loss: 0.0147 - acc: 0.9952\n",
            "\n",
            " score:- [0.03604102025316315, 0.9905] \n",
            "\n",
            "[[1.58861035e-10 4.48981672e-13 1.05613162e-06 3.46682434e-08\n",
            "  1.45411641e-18 5.31181273e-13 1.54543860e-18 9.99998927e-01\n",
            "  2.53798205e-10 1.08850007e-09]\n",
            " [1.80379584e-15 6.21569094e-13 1.00000000e+00 8.80766217e-13\n",
            "  2.37153112e-17 1.65698390e-19 3.78351066e-11 9.76612454e-14\n",
            "  2.74529016e-13 4.11693912e-18]\n",
            " [2.70184153e-10 9.99939322e-01 1.91751878e-05 5.47273268e-11\n",
            "  3.21010411e-05 2.49517811e-08 3.73931002e-08 5.49323931e-06\n",
            "  5.84533382e-07 3.20345998e-06]\n",
            " [9.99624848e-01 3.66259627e-12 3.00835404e-06 6.83146428e-09\n",
            "  3.91975037e-12 4.96783343e-08 3.72003298e-04 1.95656540e-08\n",
            "  5.49712809e-08 7.10264061e-08]\n",
            " [2.70721466e-11 1.61473496e-13 6.46297577e-13 9.64228087e-14\n",
            "  1.00000000e+00 3.56499812e-12 6.27882607e-11 4.58878352e-11\n",
            "  2.36944041e-11 9.61022906e-09]\n",
            " [6.21958263e-10 9.99781311e-01 3.36053417e-05 5.19747613e-11\n",
            "  1.00223268e-04 6.06969142e-10 3.63667829e-09 5.89612791e-05\n",
            "  3.07364928e-07 2.55992891e-05]\n",
            " [6.84749107e-19 2.11467976e-09 9.90608359e-11 6.08455794e-16\n",
            "  9.99999046e-01 6.08366899e-11 2.32413383e-17 6.54195101e-12\n",
            "  9.56081180e-07 2.31468622e-08]\n",
            " [1.58568228e-10 7.54010465e-10 2.42398812e-08 2.41813042e-10\n",
            "  1.86422578e-04 3.47778001e-10 1.16534361e-14 1.02437803e-09\n",
            "  1.26285158e-07 9.99813497e-01]\n",
            " [1.11488973e-07 3.58552461e-16 3.12363968e-10 1.97187533e-09\n",
            "  2.34796449e-14 8.90695274e-01 7.78500438e-02 2.77704614e-14\n",
            "  3.14545520e-02 1.02765085e-07]]\n",
            "\n",
            "\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6SgpOTgg2lZ",
        "colab_type": "text"
      },
      "source": [
        "###Experiment5\n",
        "\n",
        "Total params: 18,382\n",
        "\n",
        "Trainable params: 18,382\n",
        "\n",
        "Non-trainable params: 0\n",
        "\n",
        "**Accuracy - 99.05 %**\n",
        "score:- [0.037659333861383495, 0.9905] \n",
        "\n",
        "added 1X1 convl  layer  and used (16, 32 number of kernels)  not much difference in accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX5Eg9Yxg5lW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "64565f9b-db18-4768-9ca3-752ec3de66d5"
      },
      "source": [
        "from keras.layers import Activation\n",
        "\n",
        "model2 = Sequential()\n",
        "\n",
        " \n",
        "model2.add(Convolution2D(16, 3, 3, activation='relu', input_shape=(28,28,1))) #3, 26X26X16\n",
        "model2.add(Convolution2D(32, 3,3, activation='relu'))#5,  24X24X32\n",
        "model2.add(Convolution2D(10, 1,1, activation='relu'))#5,  24X24X10\n",
        "\n",
        "\n",
        "model2.add(MaxPooling2D(pool_size=(2,2)))#10,  12X12X10\n",
        "\n",
        "model2.add(Convolution2D(32, 3,3, activation='relu'))#12,  10X10X32\n",
        "model2.add(Convolution2D(10, 1,1, activation='relu'))#12,  10X10X10\n",
        "model2.add(Convolution2D(10, 10,10 ))#28,  1X1X10\n",
        "\n",
        "\n",
        "\n",
        "model2.add(Flatten()) \n",
        "model2.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#provides model architecture information- input output shape,total parameters.\n",
        "model2.summary()\n",
        "\n",
        "#compile model which is defined in previous step.\n",
        "model2.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#train or fit model to our train dataset.\n",
        "model2.fit(X_train, Y_train, batch_size=32, nb_epoch=10, verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "#calculate score or evaluate model for test dataset- which is not included in training.\n",
        "score = model2.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print(\"\\n\",\"score:-\",score,\"\\n\")\n",
        "\n",
        "\n",
        "y_pred = model2.predict(X_test)\n",
        "\n",
        "#compare predicted value of model and actual value of test data.\n",
        "print(y_pred[:9])\n",
        "print(\"\\n\")\n",
        "print(y_test[:9])"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (10, 10))`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_222 (Conv2D)          (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_223 (Conv2D)          (None, 24, 24, 32)        4640      \n",
            "_________________________________________________________________\n",
            "conv2d_224 (Conv2D)          (None, 24, 24, 10)        330       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_35 (MaxPooling (None, 12, 12, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_225 (Conv2D)          (None, 10, 10, 32)        2912      \n",
            "_________________________________________________________________\n",
            "conv2d_226 (Conv2D)          (None, 10, 10, 10)        330       \n",
            "_________________________________________________________________\n",
            "conv2d_227 (Conv2D)          (None, 1, 1, 10)          10010     \n",
            "_________________________________________________________________\n",
            "flatten_39 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 18,382\n",
            "Trainable params: 18,382\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 24s 398us/step - loss: 0.1772 - acc: 0.9454\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 20s 334us/step - loss: 0.0598 - acc: 0.9818\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0439 - acc: 0.9861\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.0353 - acc: 0.9893\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 20s 336us/step - loss: 0.0285 - acc: 0.9912\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 20s 335us/step - loss: 0.0234 - acc: 0.9924\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0194 - acc: 0.9933\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.0172 - acc: 0.9942\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 20s 335us/step - loss: 0.0150 - acc: 0.9950\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 20s 334us/step - loss: 0.0132 - acc: 0.9956\n",
            "\n",
            " score:- [0.037659333861383495, 0.9905] \n",
            "\n",
            "[[9.0044603e-14 2.1292745e-11 3.6951997e-09 7.4608475e-10 1.0825237e-17\n",
            "  1.5386132e-15 4.8686718e-22 1.0000000e+00 2.2734677e-11 1.4745158e-09]\n",
            " [4.0523669e-12 1.2284685e-12 1.0000000e+00 1.9863481e-16 5.6452953e-17\n",
            "  3.0089280e-18 2.0634783e-09 7.9937384e-16 2.4529625e-12 8.8258487e-15]\n",
            " [8.4112543e-12 9.9999523e-01 5.2107541e-09 3.0560119e-14 9.6527481e-07\n",
            "  7.4653990e-09 1.9861991e-08 3.6348893e-06 1.1251167e-07 2.1798592e-10]\n",
            " [9.9996316e-01 2.6787073e-14 1.1650315e-10 2.8589376e-16 5.4631508e-12\n",
            "  5.3352207e-09 3.6796020e-05 1.4876883e-12 1.1265369e-10 1.4546174e-09]\n",
            " [5.2429187e-13 5.0673133e-14 1.9030462e-14 1.6383621e-17 9.9999988e-01\n",
            "  1.2259161e-17 5.8763697e-14 3.1329182e-13 7.4320422e-10 6.8459684e-08]\n",
            " [2.0972805e-13 9.9999416e-01 1.2334099e-09 6.3604488e-16 2.8816939e-07\n",
            "  1.6528703e-12 3.9276381e-11 5.4983939e-06 3.8316563e-09 7.0294799e-12]\n",
            " [1.1426258e-19 6.3779884e-09 7.2545070e-10 1.0006893e-17 9.9816650e-01\n",
            "  3.0298988e-12 2.8764688e-16 1.9107658e-12 1.8334471e-03 6.2273585e-08]\n",
            " [6.4062599e-18 9.9050134e-12 1.8528057e-12 3.0089382e-08 6.4939786e-06\n",
            "  3.4895539e-10 8.5526367e-13 7.3576964e-11 5.4190576e-05 9.9993920e-01]\n",
            " [4.7568921e-10 3.5671644e-19 7.0562474e-15 4.3756912e-13 2.7569698e-15\n",
            "  9.7843742e-01 2.1560103e-02 6.7006401e-19 1.2047076e-06 1.1866525e-06]]\n",
            "\n",
            "\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1LMk-F83SDH",
        "colab_type": "text"
      },
      "source": [
        "###Experiment6\n",
        "\n",
        "Total params: 42,228\n",
        "\n",
        "Trainable params: 42,228\n",
        "\n",
        "Non-trainable params: 0\n",
        "\n",
        "**Accuracy : 98.95 %**  \n",
        "\n",
        "score:- [0.036342939333917273, 0.9895] "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib1gV6Ej3Yw2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ba53bc8f-1bdd-4127-eae3-a5a45f3a773b"
      },
      "source": [
        "from keras.layers import Activation\n",
        "\n",
        "model2 = Sequential()\n",
        "\n",
        " \n",
        "model2.add(Convolution2D(16, 3, 3, activation='relu', input_shape=(28,28,1))) #3, 26X26X16\n",
        "model2.add(Convolution2D(10, 1,1, activation='relu'))#3, 26X26X10\n",
        "model2.add(Convolution2D(32, 3,3, activation='relu'))#5,  24X24X32\n",
        "\n",
        "\n",
        "model2.add(MaxPooling2D(pool_size=(2,2)))#10,  12X12X32\n",
        "\n",
        "model2.add(Convolution2D(32, 3,3, activation='relu'))#12, 10X10X32\n",
        "\n",
        "model2.add(Convolution2D(32, 3,3, activation='relu'))#14, 8X8X32\n",
        "model2.add(Convolution2D(10, 8 ))#28, 1X1X10\n",
        "\n",
        "\n",
        "\n",
        "model2.add(Flatten()) \n",
        "model2.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#provides model architecture information- input output shape,total parameters.\n",
        "model2.summary()\n",
        "\n",
        "#compile model which is defined in previous step.\n",
        "model2.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#train or fit model to our train dataset.\n",
        "model2.fit(X_train, Y_train, batch_size=32, nb_epoch=10, verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "#calculate score or evaluate model for test dataset- which is not included in training.\n",
        "score = model2.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print(\"\\n\",\"score:-\",score,\"\\n\")\n",
        "\n",
        "\n",
        "y_pred = model2.predict(X_test)\n",
        "\n",
        "#compare predicted value of model and actual value of test data.\n",
        "print(y_pred[:9])\n",
        "print(\"\\n\")\n",
        "print(y_test[:9])"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_210 (Conv2D)          (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_211 (Conv2D)          (None, 26, 26, 10)        170       \n",
            "_________________________________________________________________\n",
            "conv2d_212 (Conv2D)          (None, 24, 24, 32)        2912      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_33 (MaxPooling (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_213 (Conv2D)          (None, 10, 10, 32)        9248      \n",
            "_________________________________________________________________\n",
            "conv2d_214 (Conv2D)          (None, 8, 8, 32)          9248      \n",
            "_________________________________________________________________\n",
            "conv2d_215 (Conv2D)          (None, 1, 1, 10)          20490     \n",
            "_________________________________________________________________\n",
            "flatten_37 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_37 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 42,228\n",
            "Trainable params: 42,228\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 25s 422us/step - loss: 0.1496 - acc: 0.9535\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.0471 - acc: 0.9859\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.0327 - acc: 0.9898\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.0244 - acc: 0.9921\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.0200 - acc: 0.9939\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.0165 - acc: 0.9947\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0132 - acc: 0.9959\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 21s 356us/step - loss: 0.0118 - acc: 0.9962\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 21s 353us/step - loss: 0.0099 - acc: 0.9969\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 21s 353us/step - loss: 0.0085 - acc: 0.9973\n",
            "\n",
            " score:- [0.036342939333917273, 0.9895] \n",
            "\n",
            "[[1.56153329e-13 2.39931535e-14 2.26826913e-08 1.54431277e-07\n",
            "  1.80397371e-16 3.43007767e-13 4.35328883e-23 9.99999881e-01\n",
            "  3.95073134e-16 1.55262658e-09]\n",
            " [1.81112102e-12 6.89767471e-12 1.00000000e+00 1.73249314e-13\n",
            "  4.62542306e-20 6.83954541e-16 3.53997859e-10 2.11231108e-17\n",
            "  4.92091990e-13 3.69082833e-15]\n",
            " [2.34547733e-13 9.99998331e-01 4.67586014e-09 1.88852822e-11\n",
            "  2.42282301e-08 1.36451331e-06 5.37836442e-12 3.16015758e-07\n",
            "  4.49158755e-09 9.27945290e-11]\n",
            " [9.99953151e-01 4.28085600e-13 2.72152162e-11 1.46816348e-09\n",
            "  4.64250220e-13 8.84076456e-08 4.14396818e-05 3.03378600e-10\n",
            "  2.37719000e-09 5.30903662e-06]\n",
            " [1.10475651e-09 4.21996856e-12 5.61462099e-10 1.05196364e-11\n",
            "  9.98946965e-01 1.13092417e-14 1.44170838e-11 6.09746758e-12\n",
            "  3.55613849e-09 1.05309172e-03]\n",
            " [3.13087203e-13 9.99997020e-01 2.66058677e-08 3.94931960e-12\n",
            "  1.95060679e-07 2.01476968e-08 4.73703921e-13 2.72270699e-06\n",
            "  1.97621386e-09 3.29331770e-11]\n",
            " [6.50358845e-17 1.17271185e-07 2.62720476e-07 4.06750473e-14\n",
            "  9.99998808e-01 2.82315751e-11 2.26422558e-15 1.69790337e-09\n",
            "  1.63105696e-07 6.93710717e-07]\n",
            " [2.54980078e-19 1.46714312e-08 3.85759824e-09 1.64069220e-08\n",
            "  2.49794471e-06 9.21637788e-09 2.34184064e-13 2.25078445e-11\n",
            "  1.25897532e-08 9.99997497e-01]\n",
            " [1.13289132e-14 4.86554583e-20 1.28925445e-15 3.57188897e-17\n",
            "  4.23246240e-24 9.99993801e-01 6.21066192e-06 8.27433895e-18\n",
            "  5.02416775e-10 3.26037418e-12]]\n",
            "\n",
            "\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrl6d5VaPJPy",
        "colab_type": "text"
      },
      "source": [
        "###Experiment7\n",
        "\n",
        "\n",
        "*  Total params: 38,814\n",
        "\n",
        "*  Trainable params: 38,814\n",
        "\n",
        "*  Non-trainable params: 0\n",
        "\n",
        "****Accuracy score is 98.85% ****\n",
        "\n",
        "[0.05012940304659878, 0.9885]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nir4XXIjJaC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "909b6ecb-3e84-4f18-90f4-a0910da44991"
      },
      "source": [
        "#given model architecture \n",
        "#rp- receptive field, channel size-cs\n",
        "\n",
        "from keras.layers import Activation\n",
        "model1 = Sequential()\n",
        "\n",
        " \n",
        "model1.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(28,28,1))) #rp-3 , cs-26X26X32\n",
        "model1.add(Convolution2D(10, 1, activation='relu'))#3, 26X26X10\n",
        "\n",
        "model1.add(Convolution2D(32, 3,3, activation='relu'))#5, 24X24X32\n",
        "\n",
        "model1.add(MaxPooling2D(pool_size=(2,2)))#5, 12X12X32\n",
        "\n",
        "model1.add(Convolution2D(10, 1,1, activation='relu'))#10, 12X12X10\n",
        "\n",
        "model1.add(Convolution2D(32, 3,3, activation='relu'))#12, 10X10X32\n",
        "\n",
        "\n",
        "model1.add(Convolution2D(10, 10))#28, 1X1X10\n",
        "\n",
        "\n",
        "\n",
        "model1.add(Flatten()) \n",
        "model1.add(Activation('softmax'))\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "  if __name__ == '__main__':\n",
            "W0816 06:59:47.202515 139814578218880 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tvptcn8dxvp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "26cb17bb-01b9-45e1-d2f7-10de0974f039"
      },
      "source": [
        "#provides model architecture information- input output shape,total parameters.\n",
        "model1.summary()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_34 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 26, 26, 10)        330       \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 24, 24, 32)        2912      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 12, 12, 10)        330       \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 10, 10, 32)        2912      \n",
            "_________________________________________________________________\n",
            "conv2d_39 (Conv2D)           (None, 1, 1, 10)          32010     \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 38,814\n",
            "Trainable params: 38,814\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3ICn4sPaoi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#compile model which is defined in previous step.\n",
        "model1.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk7LyuktavYD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "3ef68744-01e9-4d37-dec6-a60b4263c0e5"
      },
      "source": [
        "\n",
        "#train or fit model to our train dataset.\n",
        "model1.fit(X_train, Y_train, batch_size=32, nb_epoch=10, verbose=1)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 11s 183us/step - loss: 0.1493 - acc: 0.9547\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.0484 - acc: 0.9853\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0344 - acc: 0.9890\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0252 - acc: 0.9920\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0208 - acc: 0.9929\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.0155 - acc: 0.9947\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.0131 - acc: 0.9956\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0110 - acc: 0.9963\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 10s 168us/step - loss: 0.0086 - acc: 0.9973\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 10s 168us/step - loss: 0.0092 - acc: 0.9967\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f28c0131b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ey_V_Dla4AU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculate score or evaluate model for test dataset- which is not included in training.\n",
        "score = model1.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GySTvjM4a_-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c60a1394-489f-47d6-b3a9-1021aa6b7c86"
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.05012940304659878, 0.9885]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sHNb5iVa-kb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpIGynetbCKI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "0bd990e0-c9c0-43b9-cf93-f76e07f04b17"
      },
      "source": [
        "#compare predicted value of model and actual value of test data.\n",
        "print(y_pred[:9])\n",
        "print(\"\\n\")\n",
        "print(y_test[:9])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.10493474 0.10161652 0.0954626  0.10033016 0.10162459 0.09758297\n",
            "  0.10224651 0.09449761 0.10311491 0.0985894 ]\n",
            " [0.09773988 0.09438017 0.09649892 0.10334422 0.09998719 0.10401086\n",
            "  0.10180815 0.10126363 0.09922975 0.10173725]\n",
            " [0.10311636 0.10550562 0.09732316 0.10152198 0.09591486 0.1001429\n",
            "  0.09637088 0.09977473 0.10396071 0.09636884]\n",
            " [0.10557345 0.09860246 0.09388263 0.10887089 0.08811744 0.10318051\n",
            "  0.09971385 0.09575008 0.10097131 0.10533746]\n",
            " [0.10115144 0.11407564 0.09346947 0.10007957 0.09665465 0.09803565\n",
            "  0.0990995  0.09858677 0.09917868 0.0996686 ]\n",
            " [0.1032745  0.10722182 0.09790049 0.10100112 0.09795441 0.09907487\n",
            "  0.09647804 0.09715941 0.10599656 0.09393874]\n",
            " [0.09980239 0.1037842  0.09979733 0.1045879  0.10284805 0.10005597\n",
            "  0.09589493 0.09179944 0.10186139 0.09956834]\n",
            " [0.10587639 0.10283493 0.09797458 0.09868401 0.09520015 0.10066752\n",
            "  0.10272906 0.09694833 0.09672572 0.10235931]\n",
            " [0.09691148 0.114339   0.0964378  0.09850784 0.10260563 0.10011236\n",
            "  0.09351663 0.09292795 0.10305043 0.10159093]]\n",
            "\n",
            "\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph2gB8C_cF3N",
        "colab_type": "text"
      },
      "source": [
        "###Queries\n",
        "bit confused about calculating global receptive field when maxpooling and large kernel size is used. kindly let me know if i have written GRF wrongly next to each layer in comment.\n",
        "\n",
        "***I am not able to get 99.4% accuracy. max accuracy got was 99.05%.  tried different architectures and submitted  few models which gave good accuracy. using relu activation in last layer before flattening layer will drastically reduce accuracy to 38 % , 65 % with same model if i remove relu activation function for conv layer before flaten layer accuracy will go above 97 % with no change in any other layers.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlLTvGlUgkh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}